# DocuChat â€” RAG-Based Question Answering System

DocuChat is a **production-grade Retrieval-Augmented Generation (RAG) system** that lets users upload documents and ask contextual questions. It combines embeddings, FAISS similarity search, background ingestion, and an LLM-powered answer generator to deliver fast, accurate responses.

This project is designed to **demonstrate applied AI system design**â€”beyond simple prompt engineering.

---

## ğŸš€ Objective

Provide an API-driven system that:
- Accepts documents
- Indexes them using embeddings
- Retrieves relevant content via similarity search
- Generates accurate answers using an LLM
- Handles ingestion asynchronously
- Applies basic rate limiting
- Tracks meaningful metrics (e.g., latency)

The system is designed with production constraints such as latency, reliability, and observability in mind.

---

## ğŸ§  Key Features

- ğŸ“ Document upload support (`.pdf`, `.txt`)
- âœ‚ï¸ Intelligent chunking strategy
- ğŸ”¢ Embedding generation using SentenceTransformers
- ğŸ“¦ Vector storage using **FAISS** (local)
- ğŸ” Similarity-based retrieval (Topâ€‘K)
- ğŸ¤– LLM-powered answer generation (Groq / OpenAI-compatible)
- ğŸ§µ Background ingestion jobs
- ğŸš¦ API rate limiting
- ğŸ“Š Metric logging (latency, similarity distance)
- ğŸŒ Simple web UI (HTML + CSS)
- ğŸ§© Session-based FAISS indexing (isolated per user session)

---

## ğŸ› ï¸ Tech Stack

| Layer            | Technology                                   |
|------------------|----------------------------------------------|
| API              | FastAPI                                      |
| Embeddings       | sentence-transformers (MiniLM-L6-v2)         |
| Vector Store     | FAISS (local)                                |
| LLM              | Groq / OpenAI-compatible                     |
| Background Jobs  | FastAPI `BackgroundTasks`                    |
| Validation       | Pydantic                                     |
| Rate Limiting    | SlowAPI                                      |
| UI               | HTML + CSS                                   |
| Logging          | Custom structured logger                     |

---

## ğŸ“ Project Structure

```text
.
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py                  # FastAPI application
â”œâ”€â”€ src/
â”‚   â””â”€â”€ DocumentChat/
â”‚       â”œâ”€â”€ ingestion.py         # Document ingestion & FAISS indexing
â”‚       â””â”€â”€ retrieval.py         # Retrieval + answer generation
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ document_ops.py          # File abstraction & parsing
â”‚   â”œâ”€â”€ model_loader.py          # Embeddings + LLM loader
â”‚   â””â”€â”€ config_loader.py         # YAML config loader
â”œâ”€â”€ prompt/
â”‚   â””â”€â”€ prompt_library.py        # Centralized prompt templates
â”œâ”€â”€ models/
â”‚   â””â”€â”€ models.py                # Pydantic request/response models
â”œâ”€â”€ exception/
â”‚   â””â”€â”€ custom_exception.py      # Custom exception handling
â”œâ”€â”€ logger/
â”‚   â””â”€â”€ custom_logger.py         # Structured logging
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml              # Chunking, model, retrieval configs
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html               # Web UI
â”œâ”€â”€ static/
â”‚   â””â”€â”€ style.css                # UI styling
â”œâ”€â”€ faiss_index/                 # Generated FAISS indexes (runtime)
â”œâ”€â”€ data/                        # Uploaded files (runtime)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ README.md
```

---

## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/your-username/docuchat.git
cd docuchat
```

### 2ï¸âƒ£ Create and activate a virtual environment

```bash
python -m venv venv
source venv/bin/activate
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Configure environment variables

Create a `.env` file or export variables in your shell:

```bash
export GROQ_API_KEY=your_key_here
export GOOGLE_API_KEY=optional
export HF_TOKEN=optional
```

### 5ï¸âƒ£ Run the application

```bash
uvicorn api.main:app --reload
```

The application will be available at:

- Local: `http://127.0.0.1:8000`

---

## ğŸ—ï¸ System Architecture (High Level)

```text
Web UI (HTML / JS)
        â†“
FastAPI Backend
        â†“
Background Ingestion Engine
        â†“
Chunking & Embeddings
        â†“
FAISS Vector Store
        â†“
Retriever (Top-K)
        â†“
Prompt Construction
        â†“
LLM (Groq / Gemini)
```

---

## ğŸ§© Detailed Architecture Diagrams

### 1ï¸âƒ£ Document Ingestion Flow

```mermaid
flowchart TD
    A[User uploads files] --> B[FastAPI /chat/index]
    B --> C[Background task]
    C --> D[Document parsing]
    D --> E[Chunking + overlap]
    E --> F[Embedding generation]
    F --> G[FAISS index per session]
```

### 2ï¸âƒ£ RAG Question Answering Flow

```mermaid
flowchart TD
    A[User question] --> B[FastAPI /chat/query]
    B --> C[Query embedding]
    C --> D[FAISS similarity search]
    D --> E[Top-K chunks]
    E --> F[Prompt builder]
    F --> G[LLM answer]
```

---

## ğŸ”Œ API Endpoints

| Endpoint       | Method | Purpose                  |
|---------------|--------|--------------------------|
| /chat/index   | POST   | Index uploaded documents |
| /chat/query   | POST   | Ask questions            |
| /health       | GET    | Service health check     |

---

## ğŸ§  Model Selection Justification

### ğŸ”¢ Embedding Model

- **Model:** `sentence-transformers/all-MiniLM-L6-v2`
- **Reasons for choice:**
  - Fast embedding generation
  - Low memory footprint (384-dimensional vectors)
  - Strong semantic similarity performance
  - Well-suited for FAISS-based local retrieval

### ğŸ¤– Language Model (LLM)

- **Primary LLM:** Groq (OSS LLaMA-family)
- **Secondary / fallback:** Google Gemini

**Rationale:**
- Groq offers very low latency, ideal for interactive APIs.
- Gemini provides stable reasoning and structured output.
- The abstraction layer allows easy future model swapping.

---

## ğŸ“ Design Decisions

### âœ… Chunk Size Choice

- **Chunk size:** ~500 characters
- **Overlap:** ~100 characters

**Reasoning:**
- Preserves semantic meaning across sentence boundaries
- Prevents loss of context during retrieval
- Balances recall vs. precision
- Avoids overly large chunks that degrade embedding quality

This configuration performed best during testing for both retrieval accuracy and latency.

### âŒ Observed Retrieval Failure Case

**Scenario:**
- User asked a question that was **not present** in the uploaded documents.

**Observed behavior:**
- FAISS returned low-similarity chunks.
- Prompt logic correctly responded with: _"I donâ€™t know."_

**Why this is acceptable:**
- Prevents hallucinations
- Improves trustworthiness
- Aligns with enterprise RAG best practices

---

## ğŸ“Š Metrics Tracked

- **Primary metric:** End-to-end query latency (ms)

Latency is logged at:
- Retrieval stage
- LLM invocation
- Full request lifecycle

**Why it matters:**
- Directly impacts user experience
- Helps identify bottlenecks (embedding vs. LLM)
- Informs future optimization and scaling efforts

---


## ğŸ§‘â€ğŸ’» Author

**Niraj Kumar**  
Aspiring GenAI Engineer

**GitHub:** https://github.com/nirajj12

â­ If you found this project useful, consider starring the repository!
