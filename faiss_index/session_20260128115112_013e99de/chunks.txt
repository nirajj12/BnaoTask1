GENERATIVE  ARTIFICIAL  
INTELLIGENCE
 
 
 
 
 
 
 
 
 
 
Mohammad  Arbaaz  Khan  
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   1 
 
         
  Artiﬁcial  Neural  Network(ANN) 
           

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   2 
 
Q1.  What  is  an  Artiﬁcial  Neural  Network,  and  how  does  it  work?
 
An
 
Artiﬁcial
 
Neural

---

d  how  does  it  work?
 
An
 
Artiﬁcial
 
Neural
 
Network
 
is
 
a
 
computing
 
system
 
inspired
 
by
 
biological
 
neural
 
networks
 
in
 
animal
 
brains.
 
Think
 
of
 
it
 
as
 
a
 
network
 
of
 
interconnected
 
nodes
 
(artiﬁcial
 
neurons)
 
that
 
work
 
together
 
to
 
recognize
 
patterns
 
in
 
data.
 
 
Here's
 
how
 
it
 
works
 
at
 
a
 
basic
 
level:
 
❖
 
Structure
 
➢
 
Input
 
Layer:
 
Receives
 
initial
 
data
 
(like:
 
pixels
 
of
 
an
 
image)
 
➢
 
Hidden
 
Layers:

---

pixels
 
of
 
an
 
image)
 
➢
 
Hidden
 
Layers:
 
Processes
 
the
 
information
 
➢
 
Output
 
Layer:
 
Produces
 
the
 
ﬁnal
 
result
 
(like:
 
"this
 
is
 
a
 
cat")
 
 
❖
 
Basic
 
Operation
 
➢
 
Each
 
neuron
 
receives
 
inputs
 
from
 
other
 
neurons
 
➢
 
These
 
inputs
 
are
 
multiplied
 
by
 
weights
 
(indicating
 
their
 
importance)
 
➢
 
The
 
weighted
 
sum
 
is
 
passed
 
through
 
an
 
activation
 
function
 
➢
 
The
 
result
 
is
 
sent
 
to
 
the
 
next
 
layer
 
 
 
 
❖

---

lt
 
is
 
sent
 
to
 
the
 
next
 
layer
 
 
 
 
❖
 
Learning
 
Process
 
➢
 
The
 
network
 
starts
 
with
 
random
 
weights
 
➢
 
During
 
training,
 
it:
 
➢
 
Makes
 
predictions
 
on
 
training
 
data
 
➢
 
Compares
 
predictions
 
to
 
actual
 
answers
 
➢
 
 
Adjusts
 
weights
 
to
 
reduce
 
errors
 
(backpropagation)
 
➢
 
This
 
process
 
repeats
 
until
 
performance
 
is
 
satisfactory
 
 

MD  ARBAAZ  KHAN                                                                Generative  A

---

Generative  AI  Interview  Questions   3 
 
 
Q2.  What  are  activation  functions,  tell  me  the  type  of  the  activation  
functions
 
and
 
why
 
are
 
they
 
used
 
in
 
neural
 
networks?
 
In
 
neural
 
networks,
 
activation
 
functions
 
are
 
essential
 
components
 
that
 
introduce
 
non-linear
 
transformations
 
to
 
the
 
data
 
ﬂowing
 
through
 
the
 
network.
 
They
 
determine
 
how
 
the
 
weighted
 
sum
 
of
 
the
 
input
 
is
 
transf

---

weighted
 
sum
 
of
 
the
 
input
 
is
 
transformed
 
into
 
the
 
output
 
of
 
a
 
node
 
(or
 
neuron)
 
in
 
a
 
layer.
 
Without
 
activation
 
functions,
 
neural
 
networks
 
would
 
simply
 
compute
 
linear
 
transformations,
 
which
 
would
 
limit
 
their
 
ability
 
to
 
model
 
complex
 
relationships
 
and
 
solve
 
intricate
 
problems.
 
 
Why  Activation  Functions  Are  Used:
 
➔
 
Non-linearity
:
 
Activation
 
functions
 
introduce
 
non-linear
 
transformations
 
to
 
th

---

introduce
 
non-linear
 
transformations
 
to
 
the
 
data,
 
allowing
 
neural
 
networks
 
to
 
learn
 
from
 
complex
 
data
 
patterns
 
and
 
solve
 
non-linear
 
problems,
 
like
 
image
 
and
 
speech
 
recognition.
 
 
➔
 
Signal
 
transformation
:
 
They
 
transform
 
the
 
inputs
 
into
 
output
 
values
 
that
 
range
 
within
 
a
 
speciﬁc
 
interval,
 
which
 
helps
 
in
 
stabilizing
 
and
 
normalizing
 
the
 
data
 
passed
 
through
 
each
 
layer.
 
This
 
is
 
critical
 
in
 
p

---

gh
 
each
 
layer.
 
This
 
is
 
critical
 
in
 
preventing
 
exploding
 
or
 
vanishing
 
gradients.
 
 
➔
 
Enable
 
backpropagation
:
 
Activation
 
functions
 
allow
 
for
 
diﬀerentiation.
 
Since
 
neural
 
networks
 
are
 
trained
 
with
 
backpropagation,
 
the
 
gradient
 
must
 
be
 
computable.
 
Non-linear
 
activation
 
functions
 
ensure
 
that
 
each
 
layer
 
can
 
have
 
its
 
gradients
 
computed
 
during
 
training.
 
 
Types  of  Activation  Functions
 
There
 
are
 
several

---

of  Activation  Functions
 
There
 
are
 
several
 
types
 
of
 
activation
 
functions,
 
each
 
with
 
distinct
 
properties
 
that
 
make
 
them
 
suitable
 
for
 
speciﬁc
 
types
 
of
 
tasks.
 
 
1.
 
Step
 
Function
 
A
 
simple
 
binary
 
threshold
 
function
 
that
 
outputs
 
1
 
if
 
the
 
input
 
is
 
above
 
a
 
certain
 
threshold
 
and
 
0
 
otherwise.
 
   
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   4

---

Generative  AI  Interview  Questions   4 
 
 
●
 
Pros:
 
Conceptually
 
simple.
 
●
 
Cons:
 
Not
 
diﬀerentiable,
 
so
 
it’s
 
rarely
 
used
 
in
 
modern
 
neural
 
networks.
 
●
 
Use:
 
Early
 
models
 
and
 
basic
 
binary
 
classiﬁers.
 
 
2.
 
Sigmoid
 
(Logistic)
 
Function
 
   
 
Formula:
  
 
 
●
 
Range:
 
0
 
to
 
1
 
●
 
Pros:
 
Smooth
 
gradient,
 
output
 
range
 
between
 
0
 
and
 
1
 
makes
 
it
 
interpretable
 
as
 
a
 
probability.
 
●
 
Cons:
 
Vanishing
 
gradie

---

a
 
probability.
 
●
 
Cons:
 
Vanishing
 
gradient
 
problem
 
(gradients
 
get
 
smaller
 
as
 
the
 
activation
 
saturates
 
at
 
0
 
or
 
1),
 
leading
 
to
 
slow
 
convergence.
 
●
 
Use:
 
Often
 
used
 
in
 
the
 
output
 
layer
 
for
 
binary
 
classiﬁcation
 
tasks.
 
 
3.
 
Hyperbolic
 
Tangent
 
(tanh)
 
  
 
Formula:
 
 
   
 
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   5 
 
 
●
 
Range:
 
-1
 
to
 
1

---

ew  Questions   5 
 
 
●
 
Range:
 
-1
 
to
 
1
 
●
 
Pros:
 
Centers
 
output
 
around
 
zero,
 
which
 
often
 
leads
 
to
 
faster
 
convergence
 
than
 
sigmoid.
 
●
 
Cons:
 
Suﬀers
 
from
 
the
 
vanishing
 
gradient
 
problem,
 
though
 
slightly
 
less
 
than
 
sigmoid.
 
●
 
Use:
 
Commonly
 
used
 
in
 
hidden
 
layers,
 
especially
 
in
 
older
 
neural
 
networks.
 
 
4.
 
ReLU
 
(Rectiﬁed
 
Linear
 
Unit)
 
   
 
Formula:
 
 
   
 
 
●
 
Range:
 
0
 
to
 
inﬁnity
 
●
 
Pros:
 
Simpl

---

●
 
Range:
 
0
 
to
 
inﬁnity
 
●
 
Pros:
 
Simple,
 
computationally
 
eﬃcient,
 
reduces
 
the
 
likelihood
 
of
 
vanishing
 
gradient
 
issues,
 
and
 
helps
 
in
 
training
 
deep
 
networks
 
●
 
Cons:
 
Can
 
lead
 
to
 
"dead
 
neurons"
 
if
 
too
 
many
 
inputs
 
fall
 
below
 
zero
 
(output
 
remains
 
zero);
 
only
 
positive
 
gradients
 
are
 
allowed.
 
●
 
Use:
 
Widely
 
used
 
in
 
hidden
 
layers
 
of
 
modern
 
deep
 
neural
 
networks.
 
 
5.
 
Leaky
 
ReLU
 
 
Formula:

---

networks.
 
 
5.
 
Leaky
 
ReLU
 
 
Formula:
 
 
 
 
●
 
Range:
 
Negative
 
inﬁnity
 
to
 
inﬁnity
 
●
 
Pros:
 
Allows
 
a
 
small
 
gradient
 
when
 
\(
 
x
 
<
 
0
 
\),
 
reducing
 
the
 
problem
 
of
 
dead
 
neurons
 
in
 
ReLU.
 
●
 
Cons:
 
The
 
"leakiness"
 
(slope
 
for
 
negative
 
values)
 
needs
 
to
 
be
 
manually
 
set.
 
●
 
Use:
 
Often
 
used
 
as
 
an
 
alternative
 
to
 
ReLU
 
in
 
deep
 
networks
 
to
 
mitigate
 
dead
 
neuron
 
problems.
 
 
 

MD  ARBAAZ  KHAN

---

neuron
 
problems.
 
 
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   6 
 
6.
 
Parametric
 
ReLU
 
(PReLU)
 
 
●
 
Range:
 
Negative
 
inﬁnity
 
to
 
inﬁnity
 
●
 
Pros:
 
Learns
 
the
 
"leakiness"
 
parameter
 
during
 
training,
 
making
 
it
 
more
 
adaptable
 
than
 
Leaky
 
ReLU.
 
●
 
Cons:
 
Adds
 
extra
 
parameters
 
to
 
the
 
network.
 
●
 
Use:
 
In
 
advanced
 
architectures
 
where
 
ﬁne-tuning
 
is

---

anced
 
architectures
 
where
 
ﬁne-tuning
 
is
 
needed.
 
 
7.
 
Softmax
 
 
Formula:
 
 
 
 
●
 
Range:
 
0
 
to
 
1
 
for
 
each
 
output,
 
and
 
the
 
outputs
 
sum
 
to
 
1.
 
●
 
Pros:
 
Useful
 
for
 
multi-class
 
classiﬁcation
 
since
 
it
 
represents
 
the
 
probabilities
 
of
 
each
 
class.
 
●
 
Cons:
 
Computationally
 
intensive,
 
often
 
used
 
only
 
in
 
the
 
output
 
layer.
 
●
 
Use:
 
In
 
the
 
output
 
layer
 
for
 
multi-class
 
classiﬁcation.
 
 
Choosing  the  Righ

---

lti-class
 
classiﬁcation.
 
 
Choosing  the  Right  Activation  Function
 
❖
 
Hidden
 
Layers:
 
ReLU
 
and
 
its
 
variants
 
(Leaky
 
ReLU,
 
PReLU)
 
are
 
popular
 
choices
 
due
 
to
 
their
 
eﬃciency
 
and
 
ability
 
to
 
mitigate
 
the
 
vanishing
 
gradient
 
problem.
 
ELU
 
and
 
Swish
 
are
 
also
 
eﬀective
 
in
 
some
 
deep
 
learning
 
applications.
 
❖
 
Output
 
Layers:
 
➢
 
For
 
binary
 
classiﬁcation
,
 
sigmoid
 
is
 
typically
 
used.
 
➢
 
For
 
multi-class
 
classiﬁc

---

pically
 
used.
 
➢
 
For
 
multi-class
 
classiﬁcation
,
 
softmax
 
is
 
preferred.
 
➢
 
For
 
regression
 
tasks
,
 
a
 
linear
 
activation
 
function
 
(i.e.,
 
no
 
activation)
 
is
 
often
 
used.
 
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   7 
 
By
 
selecting
 
appropriate
 
activation
 
functions,
 
neural
 
networks
 
can
 
eﬀectively
 
learn
 
complex
 
patterns,
 
enabling
 
them
 
to
 
perform
 
a
 
wid

---

tterns,
 
enabling
 
them
 
to
 
perform
 
a
 
wide
 
range
 
of
 
tasks
 
across
 
diﬀerent
 
domains.consequat,
 
vel
 
illum
 
dolore
 
eu
 
feugiat
 
nulla
 
facilisis
 
at
 
vero
 
eros
 
et
 
accumsan.
 
 Q3.  What  is  backpropagation,  and  how  does  it  work  in  training  neural  
networks? 
Backpropagation
 
is
 
a
 
key
 
algorithm
 
used
 
for
 
training
 
neural
 
networks.
 
It
 
calculates
 
the
 
gradient
 
of
 
the
 
loss
 
function
 
with
 
respect
 
to
 
each
 
weight
 
by

---

ction
 
with
 
respect
 
to
 
each
 
weight
 
by
 
applying
 
the
 
chain
 
rule,
 
propagating
 
error
 
signals
 
backward
 
through
 
the
 
network.
 
This
 
allows
 
the
 
neural
 
network
 
to
 
adjust
 
its
 
weights
 
in
 
a
 
way
 
that
 
minimizes
 
the
 
error,
 
thereby
 
improving
 
performance
 
on
 
the
 
task
 
it
 
is
 
learning.
 
 
How  Backpropagation  Works
 
The
 
backpropagation
 
algorithm
 
involves
 
two
 
main
 
steps:
 
the
 
forward
 
pass
 
and
 
the
 
backward
 
pas

---

the
 
forward
 
pass
 
and
 
the
 
backward
 
pass.
 
 
1.
 
Forward
 
Pass
 
●
 
The
 
input
 
data
 
is
 
passed
 
through
 
the
 
network
 
layer
 
by
 
layer,
 
and
 
each
 
neuron
 
applies
 
a
 
weight
 
and
 
an
 
activation
 
function
 
to
 
compute
 
its
 
output.
 
●
 
This
 
process
 
continues
 
until
 
the
 
ﬁnal
 
output
 
layer
 
produces
 
predictions.
 
●
 
The
 
network
 
then
 
compares
 
the
 
predicted
 
output
 
with
 
the
 
actual
 
(target)
 
output
 
using
 
a
 
loss

---

actual
 
(target)
 
output
 
using
 
a
 
loss
 
function
 
(such
 
as
 
Mean
 
Squared
 
Error
 
for
 
regression
 
tasks
 
or
 
CrossEntropy
 
for
 
classiﬁcation).
 
●
 
The
 
diﬀerence
 
between
 
the
 
predicted
 
and
 
actual
 
values
 
is
 
the
 
error
 
or
 
loss,
 
which
 
represents
 
how
 
far
 
oﬀ
 
the
 
network's
 
predictions
 
are.
 
 
2.
 
Backward
 
Pass
 
(Backpropagation)
 
●
 
The
 
backward
 
pass
 
starts
 
from
 
the
 
output
 
layer
 
and
 
propagates
 
the
 
error
 
ba

---

tput
 
layer
 
and
 
propagates
 
the
 
error
 
backward
 
through
 
the
 
network
 
to
 
update
 
the
 
weights.
 
●
 
It
 
computes
 
the
 
gradient
 
of
 
the
 
loss
 
function
 
with
 
respect
 
to
 
each
 
weight
 
by
 
using
 
the
 
chain
 
rule.
 
This
 
chain
 
rule
 
allows
 
us
 
to
 
express
 
the
 
derivative
 
of
 
a
 
function
 
in
 
terms
 
of
 
the
 
derivatives
 
of
 
its
 
composite
 
parts.
 
 
Steps  in  Backpropagation
 

MD  ARBAAZ  KHAN

---

ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   8 
 
❖
 
Calculate
 
the
 
error
 
at
 
the
 
output
 
layer
 
❖
 
Compute
 
gradients
 
layer
 
by
 
layer
 
❖
 
Update
 
the
 
weights
 
❖
 
Repeat
 
the
 
process
 
 
Key  Components  in  Backpropagation
 
❖
 
 
Learning
 
Rate:
 
 
➢
 
This
 
parameter
 
controls
 
how
 
much
 
the
 
weights
 
are
 
adjusted
 
with
 
respect
 
to
 
the
 
gradient.
 
 
➢
 
A
 
small
 
learning

---

o
 
the
 
gradient.
 
 
➢
 
A
 
small
 
learning
 
rate
 
might
 
lead
 
to
 
a
 
long
 
training
 
time,
 
while
 
a
 
large
 
learning
 
rate
 
might
 
overshoot
 
the
 
optimal
 
solution.
 
 
❖
 
Loss
 
Function:
 
 
➢
 
The
 
loss
 
function
 
quantiﬁes
 
the
 
diﬀerence
 
between
 
the
 
predicted
 
outputs
 
and
 
actual
 
target
 
values.
 
 
➢
 
Common
 
choices
 
are
 
Mean
 
Squared
 
Error
 
(MSE)
 
for
 
regression
 
and
 
CrossEntropy
 
for
 
classiﬁcation.
 
 
❖
 
Activation
 
Fun

---

py
 
for
 
classiﬁcation.
 
 
❖
 
Activation
 
Functions:
 
 
➢
 
Activation
 
functions
 
introduce
 
nonlinearity
 
and
 
inﬂuence
 
the
 
gradients
 
that
 
propagate
 
backward.
 
 
➢
 
Functions
 
like
 
ReLU
 
are
 
often
 
used
 
because
 
they
 
reduce
 
the
 
likelihood
 
of
 
vanishing
 
gradients.
 
 
 
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   9 
 
Q4.  What  is  the  vanishing  gradient  and  exploding

---

at  is  the  vanishing  gradient  and  exploding  gradient  problem,  and  
how
 
can
 
it
 
affect
 
neural
 
network
 
training?
 
The
 
vanishing
 
gradient
 
and
 
exploding
 
gradient
 
problems
 
are
 
issues
 
that
 
can
 
arise
 
during
 
the
 
training
 
of
 
deep
 
neural
 
networks,
 
particularly
 
when
 
using
 
gradient-based
 
optimization
 
methods
 
like
 
backpropagation.
 
Both
 
problems
 
can
 
signiﬁcantly
 
aﬀect
 
the
 
performance
 
and
 
convergence
 
of
 
neural
 
netw

---

rformance
 
and
 
convergence
 
of
 
neural
 
networks.
 
 
Vanishing  Gradient  Problem
 
The
 
vanishing
 
gradient
 
problem
 
occurs
 
when
 
the
 
gradients
 
of
 
the
 
loss
 
function
 
become
 
very
 
small
 
as
 
they
 
are
 
propagated
 
back
 
through
 
the
 
layers
 
of
 
the
 
network.
 
This
 
typically
 
happens
 
in
 
deep
 
networks
 
with
 
many
 
layers,
 
especially
 
when
 
using
 
activation
 
functions
 
like
 
sigmoid
 
or
 
tanh,
 
which
 
can
 
squash
 
input
 
values

---

tanh,
 
which
 
can
 
squash
 
input
 
values
 
into
 
a
 
small
 
range
 
(0
 
to
 
1
 
or
 
-1
 
to
 
1).
 
 
Eﬀects:
 
●
 
When
 
gradients
 
are
 
very
 
small,
 
the
 
updates
 
to
 
the
 
weights
 
become
 
negligible
,
 
leading
 
to
 
slow
 
or
 
stalled
 
training.
 
As
 
a
 
result,
 
earlier
 
layers
 
in
 
the
 
network
 
learn
 
very
 
little,
 
if
 
at
 
all,causing
 
the
 
model
 
to
 
fail
 
to
 
capture
 
complex
 
patterns
 
in
 
the
 
data.
 
Then
 
the
 
network
 
may
 
no

---

in
 
the
 
data.
 
Then
 
the
 
network
 
may
 
not
 
converge
 
to
 
a
 
useful
 
solution,
 
particularly
 
in
 
tasks
 
requiring
 
deep
 
architectures.
 
 
Exploding  Gradient  Problem
 
The
 
exploding
 
gradient
 
problem
 
occurs
 
when
 
the
 
gradients
 
grow
 
exponentially
 
large
 
during
 
backpropagation.
 
This
 
can
 
happen
 
in
 
deep
 
networks
 
or
 
in
 
certain
 
recurrent
 
architectures
 
where
 
repeated
 
multiplications
 
of
 
large
 
weights
 
can
 
lead
 
to
 
incre

---

s
 
of
 
large
 
weights
 
can
 
lead
 
to
 
increasingly
 
larger
 
gradients.
 
 
Eﬀects:
 
●
 
When
 
gradients
 
are
 
excessively
 
large,
 
the
 
weight
 
updates
 
become
 
too
 
large,
 
causing
 
the
 
model
 
parameters
 
to
 
diverge.
 
This
 
can
 
lead
 
to
 
numerical
 
instability,
 
resulting
 
in
 
NaN
 
(notanumber)
 
values
 
or
 
overﬂow
 
errors
 
during
 
training.
 
Then
 
the
 
training
 
process
 
can
 
oscillate
 
or
 
fail
 
completely,
 
making
 
it
 
diﬃcult
 
to
 
a

---

ail
 
completely,
 
making
 
it
 
diﬃcult
 
to
 
achieve
 
a
 
stable
 
convergence.
 
 
 
Q5.  How  do  you  prevent  overﬁtting  in  neural  networks? 
Preventing
 
overﬁtting
 
in
 
neural
 
networks
 
is
 
crucial
 
for
 
ensuring
 
that
 
the
 
model
 
generalizes
 
well
 
to
 
unseen
 
data
 
rather
 
than
 
simply
 
memorizing
 
the
 
training
 
data.
 
Here
 
are
 
several
 
eﬀective
 
strategies
 
to
 
mitigate
 
overﬁtting:
 
 
 
 

MD  ARBAAZ  KHAN

---

ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   10 
 
1.
 
Train
 
with
 
More
 
Data
 
Increase
 
the
 
Dataset
 
Size:
 
The
 
simplest
 
way
 
to
 
combat
 
overﬁtting
 
is
 
to
 
train
 
the
 
model
 
on
 
a
 
larger
 
dataset.
 
More
 
data
 
can
 
help
 
the
 
model
 
learn
 
more
 
general
 
patterns
 
and
 
reduce
 
the
 
chances
 
of
 
memorizing
 
speciﬁc
 
examples.
 
 
2.
 
Data
 
Augmentation
 
Transform
 
the
 
Tra

---

2.
 
Data
 
Augmentation
 
Transform
 
the
 
Training
 
Data:
 
Apply
 
transformations
 
to
 
the
 
training
 
data,
 
such
 
as
 
rotations,
 
translations,
 
zooms,
 
ﬂips,
 
and
 
color
 
changes.
 
This
 
increases
 
the
 
diversity
 
of
 
the
 
training
 
set
 
and
 
helps
 
the
 
model
 
become
 
more
 
robust.
 
 
3.
 
Regularization
 
Techniques
 
➢
 
L1
 
and
 
L2
 
Regularization:
 
Add
 
a
 
penalty
 
term
 
to
 
the
 
loss
 
function
 
based
 
on
 
the
 
magnitude
 
of
 
the
 
wei

---

on
 
based
 
on
 
the
 
magnitude
 
of
 
the
 
weights.
 
L2
 
regularization
 
(also
 
known
 
as
 
weight
 
decay)
 
is
 
commonly
 
used
 
and
 
encourages
 
smaller
 
weights,
 
reducing
 
model
 
complexity.
 
 
➢
 
Dropout:
 
Randomly
 
drop
 
a
 
fraction
 
of
 
the
 
neurons
 
during
 
training.
 
This
 
prevents
 
the
 
model
 
from
 
relying
 
too
 
heavily
 
on
 
speciﬁc
 
neurons,
 
forcing
 
it
 
to
 
learn
 
more
 
robust
 
features.
 
 
4.
 
Early
 
Stopping
 
Monitor
 
Validation

---

.
 
 
4.
 
Early
 
Stopping
 
Monitor
 
Validation
 
Loss:
 
Track
 
the
 
model
 
s
 
performance
 
on
 
a
 
validation
 
set
 
during
 
training.
 
Stop
 
training
 
when
 
the
 
validation
 
loss
 
begins
 
to
 
increase,
 
indicating
 
that
 
the
 
model
 
has
 
started
 
to
 
overﬁt
 
the
 
training
 
data.
 
 
5.
 
Reduce
 
Model
 
Complexity
 
Simpler
 
Architectures:
 
Use
 
a
 
less
 
complex
 
model
 
with
 
fewer
 
layers
 
or
 
neurons.
 
A
 
simpler
 
model
 
is
 
less
 
likely
 
to

---

s.
 
A
 
simpler
 
model
 
is
 
less
 
likely
 
to
 
overﬁt
 
the
 
training
 
data,
 
as
 
it
 
has
 
fewer
 
parameters
 
to
 
learn
 
from.
 
 
6.
 
CrossValidation
 
K=Fold
 
CrossValidation:
 
Use
 
cross-validation
 
techniques
 
to
 
ensure
 
the
 
model
 
is
 
evaluated
 
on
 
multiple
 
subsets
 
of
 
the
 
data.
 
This
 
can
 
provide
 
a
 
more
 
reliable
 
estimate
 
of
 
the
 
model's
 
performance
 
and
 
help
 
identify
 
overﬁtting.
 
 
7.
 
Ensemble
 
Methods
 
Combine
 
Multipl

---

g.
 
 
7.
 
Ensemble
 
Methods
 
Combine
 
Multiple
 
Models:
 
Use
 
techniques
 
like
 
bagging
 
or
 
boosting
 
to
 
combine
 
the
 
predictions
 
of
 
multiple
 
models.
 
This
 
can
 
help
 
reduce
 
variance
 
and
 
improve
 
generalization.
 
 
8.
 
Batch
 
Normalization
 
Normalize
 
Layer
 
Outputs:
 
Apply
 
batch
 
normalization
 
to
 
stabilize
 
and
 
accelerate
 
training.
 
It
 
can
 
also
 
act
 
as
 
a
 
form
 
of
 
regularization
 
by
 
adding
 
noise
 
to
 
the
 
inputs
 
of

---

on
 
by
 
adding
 
noise
 
to
 
the
 
inputs
 
of
 
layers
 
during
 
training.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   11 
 
 
9.
 
Using
 
Pretrained
 
Models
 
Transfer
 
Learning:
 
Start
 
with
 
a
 
pretrained
 
model
 
on
 
a
 
similar
 
task
 
and
 
ﬁnetune
 
it
 
on
 
your
 
speciﬁc
 
dataset.
 
This
 
leverages
 
learned
 
features,
 
which
 
can
 
reduce
 
the
 
risk
 
of
 
overﬁtting,
 
especially
 
wit

---

the
 
risk
 
of
 
overﬁtting,
 
especially
 
with
 
limited
 
data.
 
 
By
 
implementing
 
these
 
strategies,
 
you
 
can
 
signiﬁcantly
 
reduce
 
the
 
risk
 
of
 
overﬁtting
 
in
 
neural
 
networks,
 
leading
 
to
 
models
 
that
 
generalize
 
better
 
to
 
new,
 
unseen
 
data.
 
 
 
Q6.  What  is  dropout,  and  how  does  it  help  in  training  neural  networks? 
Dropout
 
is
 
a
 
regularization
 
technique
 
used
 
in
 
neural
 
networks
 
to
 
prevent
 
overﬁtting.
 
It
 
works

---

tworks
 
to
 
prevent
 
overﬁtting.
 
It
 
works
 
by
 
randomly
 
"dropping
 
out"
 
(setting
 
to
 
zero)
 
a
 
fraction
 
of
 
the
 
neurons
 
during
 
training,
 
which
 
helps
 
the
 
model
 
generalize
 
better
 
to
 
unseen
 
data.
 
 
How  Dropout  Works
 
During
 
each
 
training
 
iteration,
 
dropout
 
randomly
 
selects
 
a
 
subset
 
of
 
neurons
 
(along
 
with
 
their
 
connections)
 
to
 
be
 
ignored.
 
This
 
means
 
that
 
these
 
neurons
 
do
 
not
 
contribute
 
to
 
the
 
f

---

e
 
neurons
 
do
 
not
 
contribute
 
to
 
the
 
forward
 
pass
 
and
 
do
 
not
 
receive
 
weight
 
updates
 
during
 
backpropagation.
 
For
 
example,
 
if
 
a
 
dropout
 
rate
 
of
 
0.5
 
is
 
used,
 
half
 
of
 
the
 
neurons
 
in
 
a
 
layer
 
might
 
be
 
dropped
 
out
 
at
 
each
 
training
 
step.
 
 
 Key  Beneﬁts  of  Dropout
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   12 
 
1.
 
Reduces
 
Overﬁtting:
 
By

---

Questions   12 
 
1.
 
Reduces
 
Overﬁtting:
 
By
 
preventing
 
the
 
network
 
from
 
relying
 
too
 
heavily
 
on
 
any
 
speciﬁc
 
neurons,
 
dropout
 
encourages
 
it
 
to
 
learn
 
more
 
robust
 
and
 
generalizable
 
features.
 
This
 
reduces
 
the
 
risk
 
of
 
the
 
model
 
memorizing
 
the
 
training
 
data
 
instead
 
of
 
learning
 
useful
 
patterns.
 
 
2.
 
Promotes
 
Redundancy:
 
Dropout
 
forces
 
the
 
network
 
to
 
develop
 
multiple
 
independent
 
representations
 
of

---

multiple
 
independent
 
representations
 
of
 
the
 
data,
 
as
 
diﬀerent
 
subsets
 
of
 
neurons
 
are
 
activated
 
during
 
each
 
training
 
pass.
 
This
 
redundancy
 
helps
 
the
 
model
 
maintain
 
performance
 
even
 
when
 
some
 
neurons
 
are
 
inactive.
 
 
3.
 
Acts
 
as
 
an
 
Ensemble
 
Method:
 
Dropout
 
can
 
be
 
seen
 
as
 
training
 
a
 
large
 
number
 
of
 
diﬀerent
 
sub-networks
 
within
 
the
 
same
 
model.
 
At
 
test
 
time,
 
when
 
all
 
neurons
 
are
 
used

---

test
 
time,
 
when
 
all
 
neurons
 
are
 
used,
 
the
 
model
 
eﬀectively
 
behaves
 
like
 
an
 
ensemble
 
of
 
many
 
models,
 
which
 
can
 
improve
 
predictive
 
performance.
 
 
Implementation  
➢
 
Dropout
 
Rate:
 
The
 
proportion
 
of
 
neurons
 
to
 
drop
 
can
 
be
 
controlled
 
by
 
a
 
parameter
 
known
 
as
 
the
 
dropout
 
rate,
 
typically
 
set
 
between
 
0.2
 
and
 
0.5
 
for
 
hidden
 
layers.
 
Higher
 
dropout
 
rates
 
may
 
lead
 
to
 
underﬁtting,
 
while
 
lowe

---

es
 
may
 
lead
 
to
 
underﬁtting,
 
while
 
lower
 
rates
 
may
 
not
 
eﬀectively
 
prevent
 
overﬁtting.
 
 
➢
 
Application:
 
Dropout
 
is
 
typically
 
applied
 
to
 
hidden
 
layers
 
of
 
the
 
network
 
during
 
training,
 
while
 
during
 
testing
 
(inference),
 
all
 
neurons
 
are
 
used
 
with
 
their
 
full
 
weights
 
to
 
make
 
predictions.
 
 
Q7.  How  do  you  choose  the  number  of  layers  and  neurons  for  a  neural  
network?
 
Choosing
 
the
 
number
 
of
 
layers

---

etwork?
 
Choosing
 
the
 
number
 
of
 
layers
 
and
 
neurons
 
in
 
a
 
neural
 
network
 
is
 
crucial
 
for
 
its
 
performance
 
and
 
depends
 
on
 
several
 
factors:
 
 
1.
 
Problem
 
Complexity:
 
For
 
simple
 
tasks,
 
a
 
smaller
 
network
 
with
 
one
 
or
 
two
 
hidden
 
layers
 
may
 
suﬃce,
 
while
 
complex
 
tasks
 
(e.g.,
 
image
 
recognition)
 
typically
 
require
 
deeper
 
networks.
 
 
2.
 
Dataset
 
Size:
 
Use
 
simpler
 
models
 
for
 
small
 
datasets
 
to
 
avoid

---

er
 
models
 
for
 
small
 
datasets
 
to
 
avoid
 
overﬁtting,
 
whereas
 
larger
 
datasets
 
can
 
support
 
deeper
 
networks
 
that
 
better
 
capture
 
intricate
 
patterns.
 
 
3.
 
Heuristics:
 
Start
 
with
 
a
 
number
 
of
 
neurons
 
between
 
the
 
input
 
and
 
output
 
layer
 
sizes.
 
For
 
example,
 
if
 
you
 
have
 
100
 
input
 
features
 
and
 
10
 
output
 
classes,
 
consider
 
hidden
 
layers
 
with
 
50
 
to
 
100
 
neurons.
 
Begin
 
with
 
23
 
hidden
 
layers
 
and

---

ns.
 
Begin
 
with
 
23
 
hidden
 
layers
 
and
 
adjust
 
based
 
on
 
performance.
 
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   13 
 
4.
 
Experimentation:
 
Use
 
grid
 
search
 
or
 
random
 
search
 
to
 
test
 
diﬀerent
 
architectures.
 
Tools
 
like
 
Keras
 
Tuner
 
can
 
facilitate
 
hyperparameter
 
tuning.
 
 
5.
 
Monitoring:
 
Regularly
 
evaluate
 
training
 
and
 
validation
 
performance.
 
If
 
under

---

ing
 
and
 
validation
 
performance.
 
If
 
underﬁtting
 
occurs,
 
increase
 
complexity;
 
if
 
overﬁtting
 
is
 
detected,
 
simplify
 
the
 
model
 
or
 
apply
 
regularization
 
techniques.
 
 
6.
 
Transfer
 
Learning:
 
For
 
tasks
 
like
 
image
 
or
 
language
 
processing,
 
leverage
 
pretrained
 
models
 
to
 
jumpstart
 
performance.
 
 
Q8.  What  is  transfer  learning,  and  when  is  it  useful? 
Transfer
 
learning
 
is
 
a
 
machine
 
learning
 
technique
 
where
 
a
 
model

---

achine
 
learning
 
technique
 
where
 
a
 
model
 
developed
 
for
 
one
 
task
 
is
 
reused
 
as
 
the
 
starting
 
point
 
for
 
a
 
model
 
on
 
a
 
second,
 
related
 
task.
 
 
 
Instead
 
of
 
training
 
a
 
neural
 
network
 
from
 
scratch,
 
transfer
 
learning
 
allows
 
you
 
to
 
leverage
 
pretrained
 
models
 
that
 
have
 
already
 
learned
 
useful
 
features
 
from
 
a
 
large
 
dataset.
 
This
 
approach
 
can
 
save
 
time
 
and
 
resources,
 
often
 
leading
 
to
 
better

---

and
 
resources,
 
often
 
leading
 
to
 
better
 
performance,
 
especially
 
when
 
labeled
 
data
 
is
 
scarce
 
for
 
the
 
new
 
task.
 
 
How  Transfer  Learning  Works
 
1.
 
Pre-training:
 
A
 
model
 
is
 
ﬁrst
 
trained
 
on
 
a
 
large
 
dataset
 
(e.g.,
 
ImageNet
 
for
 
image
 
classiﬁcation)
 
to
 
learn
 
general
 
features.
 
 
2.
 
Fine-tuning:
 
The
 
pretrained
 
model
 
is
 
then
 
adapted
 
to
 
a
 
new,
 
often
 
smaller
 
dataset
 
related
 
to
 
a
 
speciﬁc
 
task.
 
Th

---

dataset
 
related
 
to
 
a
 
speciﬁc
 
task.
 
This
 
can
 
involve:
 
➢
 
Freezing
 
Layers:
 
Keeping
 
the
 
initial
 
layers
 
of
 
the
 
pretrained
 
model
 
ﬁxed
 
while
 
only
 
training
 
the
 
later
 
layers.
 
➢
 
Unfreezing
 
Layers:
 
Optionally
 
allowing
 
some
 
of
 
the
 
earlier
 
layers
 
to
 
be
 
trained
 
along
 
with
 
the
 
later
 
ones.
 
 
When  Transfer  Learning  is  Useful
 
1.
 
Limited
 
Data:
 
When
 
you
 
have
 
a
 
small
 
dataset
 
for
 
the
 
new
 
task,
 
tr

---

a
 
small
 
dataset
 
for
 
the
 
new
 
task,
 
transfer
 
learning
 
can
 
help
 
by
 
using
 
knowledge
 
from
 
the
 
larger
 
dataset
 
to
 
improve
 
performance.
 
2.
 
Similar
 
Domains:
 
It’s
 
particularly
 
eﬀective
 
when
 
the
 
source
 
and
 
target
 
tasks
 
are
 
related,
 
such
 
as
 
using
 
a
 
model
 
trained
 
on
 
cats
 
and
 
dogs
 
to
 
classify
 
diﬀerent
 
breeds
 
of
 
dogs.
 
3.
 
Time
 
and
 
Resource
 
Constraints:
 
Training
 
deep
 
neural
 
networks
 
can
 
be

---

Training
 
deep
 
neural
 
networks
 
can
 
be
 
computationally
 
expensive.
 
Transfer
 
learning
 
reduces
 
training
 
time
 
since
 
you
 
start
 
with
 
a
 
model
 
that
 
already
 
has
 
learned
 
features.
 
4.
 
Improving
 
Performance:
 
Transfer
 
learning
 
often
 
leads
 
to
 
improved
 
accuracy
 
and
 
robustness,
 
as
 
the
 
model
 
beneﬁts
 
from
 
the
 
rich
 
features
 
learned
 
from
 
the
 
larger
 
dataset.
 
 

MD  ARBAAZ  KHAN

---

KHAN                                                                Generative  AI  Interview  Questions   14 
 
In
 
summary,
 
transfer
 
learning
 
is
 
a
 
powerful
 
technique
 
that
 
helps
 
eﬃciently
 
solve
 
machine
 
learning
 
problems,
 
especially
 
when
 
data
 
is
 
limited,
 
by
 
utilizing
 
existing
 
knowledge
 
from
 
related
 
tasks.
 
 
Q9.  What  is  a  loss  function,  and  how  do  you  choose  the  appropriate  one  
for
 
your
 
model?
 
 
 
A
 
loss
 
function
 
is

---

for
 
your
 
model?
 
 
 
A
 
loss
 
function
 
is
 
a
 
mathematical
 
tool
 
that
 
quantiﬁes
 
the
 
diﬀerence
 
between
 
a
 
model's
 
predictions
 
and
 
the
 
actual
 
target
 
values
 
in
 
a
 
dataset.
 
It
 
guides
 
the
 
optimization
 
process
 
during
 
training,
 
helping
 
minimize
 
prediction
 
errors
 
and
 
improve
 
model
 
performance.
 
 
Types  of  Loss  Functions
 
●
 
Regression
 
Tasks:
 
○
 
Mean
 
Squared
 
Error
 
(MSE):
 
Calculates
 
the
 
average
 
squared
 
diﬀer

---

E):
 
Calculates
 
the
 
average
 
squared
 
diﬀerences
 
between
 
predicted
 
and
 
actual
 
values,
 
sensitive
 
to
 
outliers.
 
○
 
Mean
 
Absolute
 
Error
 
(MAE):
 
Measures
 
average
 
absolute
 
diﬀerences,
 
less
 
sensitive
 
to
 
outliers
 
than
 
MSE.
 
 
●
 
Classiﬁcation
 
Tasks:
 
○
 
Binary
 
CrossEntropy:
 
Used
 
for
 
binary
 
classiﬁcation,
 
measuring
 
the
 
diﬀerence
 
between
 
predicted
 
probabilities
 
and
 
actual
 
binary
 
outcomes.
 
○
 
Categorical
 
CrossEntrop

---

binary
 
outcomes.
 
○
 
Categorical
 
CrossEntropy:
 
Suitable
 
for
 
multiclass
 
classiﬁcation,
 
comparing
 
predicted
 
probability
 
distributions
 
against
 
true
 
one-hot
 
encoded
 
labels.
 
 
Choosing  the  Appropriate  Loss  Function
 
1.
 
Task
 
Nature:
 
Identify
 
if
 
the
 
task
 
is
 
regression
 
or
 
classiﬁcation.
 
2.
 
Data
 
Characteristics:
 
Consider
 
data
 
properties,
 
like
 
outliers,
 
when
 
selecting
 
loss
 
functions
 
(e.g.,
 
MAE
 
for
 
regression).
 
3.

---

unctions
 
(e.g.,
 
MAE
 
for
 
regression).
 
3.
 
Experimentation:
 
Test
 
diﬀerent
 
loss
 
functions
 
to
 
determine
 
which
 
performs
 
best
 
for
 
your
 
model.
 
4.
 
Domain
 
Knowledge:
 
Utilize
 
domainspeciﬁc
 
insights
 
to
 
inform
 
your
 
choice.
 
 
In
 
summary,
 
the
 
right
 
loss
 
function
 
is
 
essential
 
for
 
eﬀective
 
model
 
training,
 
and
 
its
 
selection
 
should
 
align
 
with
 
the
 
speciﬁc
 
task
 
and
 
data
 
characteristics.
 
 
Q10.  Explain  the  con

---

ata
 
characteristics.
 
 
Q10.  Explain  the  concept  of  gradient  descent  and  its  variations  like  
stochastic
 
gradient
 
descent
 
(SGD)
 
and
 
mini-batch
 
gradient
 
descent. 
Gradient
 
Descent
 
is
 
an
 
optimization
 
algorithm
 
used
 
to
 
minimize
 
the
 
loss
 
function
 
in
 
machine
 
learning
 
and
 
neural
 
networks.
 
The
 
process
 
involves
 
iteratively
 
updating
 
model
 

MD  ARBAAZ  KHAN                                                                Generative

---

Generative  AI  Interview  Questions   15 
 
parameters
 
by
 
calculating
 
the
 
gradient
 
of
 
the
 
loss
 
function
 
and
 
moving
 
in
 
the
 
opposite
 
direction.
 
The
 
basic
 
steps
 
are:
 
 
1.
 
Initialization:
 
Start
 
with
 
random
 
parameter
 
values.
 
2.
 
Compute
 
Gradient:
 
Calculate
 
the
 
gradient
 
of
 
the
 
loss
 
function.
 
3.
 
Update
 
Parameters:
 
Adjust
 
the
 
parameter:
 
 
 
 
 
 
 
 
4.
 
Iterate:
 
Repeat
 
until

---

:
 
 
 
 
 
 
 
 
4.
 
Iterate:
 
Repeat
 
until
 
convergence.
 
 
 Variations  of  Gradient  Descent
 
➢
 
Stochastic
 
Gradient
 
Descent
 
(SGD):
 
○
 
Updates
 
parameters
 
using
 
one
 
training
 
example
 
at
 
a
 
time.
 
○
 
This
 
introduces
 
variability,
 
which
 
can
 
help
 
escape
 
local
 
minima
 
but
 
may
 
lead
 
to
 
noisy
 
convergence.
 
 
➢
 
MiniBatch
 
Gradient
 
Descent:
 
○
 
Uses
 
a
 
small
 
batch
 
of
 
training
 
examples
 
for
 
each
 
update.
 
○
 
This
 
bala

---

examples
 
for
 
each
 
update.
 
○
 
This
 
balances
 
the
 
stability
 
of
 
the
 
updates
 
and
 
computational
 
eﬃciency,
 
leading
 
to
 
more
 
stable
 
convergence.
 
 
In
 
summary,
 
gradient
 
descent
 
is
 
essential
 
for
 
optimizing
 
model
 
parameters,
 
with
 
SGD
 
and
 
mini-batch
 
gradient
 
descent
 
providing
 
variations
 
that
 
enhance
 
convergence
 
speed
 
and
 
stability.
 
 
 

MD  ARBAAZ  KHAN                                                                Genera

---

Generative  AI  Interview  Questions   16 
 
Q11.  What  is  the  role  of  a  learning  rate  in  neural  network  training,  and  
how
 
do
 
you
 
optimize
 
it?
 
The
 
learning
 
rate
 
is
 
a
 
crucial
 
hyperparameter
 
in
 
neural
 
network
 
training
 
that
 
controls
 
the
 
step
 
size
 
of
 
each
 
update
 
in
 
the
 
gradient
 
descent
 
process.
 
It
 
determines
 
how
 
much
 
the
 
model’s
 
weights
 
are
 
adjusted
 
with
 
respect
 
t

---

’s
 
weights
 
are
 
adjusted
 
with
 
respect
 
to
 
the
 
calculated
 
gradient.
 
A
 
well-chosen
 
learning
 
rate
 
can
 
signiﬁcantly
 
impact
 
the
 
speed
 
and
 
quality
 
of
 
training.
 
 
Role  of  the  Learning  Rate  
 
1.
 
Controls
 
Convergence
 
Speed:
 
A
 
higher
 
learning
 
rate
 
speeds
 
up
 
training
 
but
 
risks
 
overshooting
 
the
 
minimum
 
of
 
the
 
loss
 
function,
 
leading
 
to
 
oscillations
 
or
 
divergence.
 
A
 
lower
 
learning
 
rate
 
results
 
in
 
sl

---

A
 
lower
 
learning
 
rate
 
results
 
in
 
slower
 
but
 
more
 
stable
 
convergence,
 
though
 
it
 
might
 
get
 
stuck
 
in
 
local
 
minima
 
or
 
take
 
too
 
long
 
to
 
reach
 
an
 
optimal
 
solution.
 
 
2.
 
Prevents
 
Underﬁtting
 
or
 
Overﬁtting:
 
A
 
very
 
low
 
learning
 
rate
 
may
 
cause
 
the
 
model
 
to
 
underﬁt
 
because
 
it
 
may
 
not
 
reach
 
the
 
loss
 
function
 
s
 
minimum
 
within
 
reasonable
 
time.
 
An
 
excessively
 
high
 
learning
 
rate
 
can
 
l

---

n
 
excessively
 
high
 
learning
 
rate
 
can
 
lead
 
to
 
instability
 
and
 
poor
 
generalization,
 
eﬀectively
 
overﬁtting.
 
 
Optimizing  the  Learning  Rate
 
●
 
Learning
 
Rate
 
Schedulers:
 
○
 
Decay
 
Schedules:
 
Gradually
 
reduce
 
the
 
learning
 
rate
 
over
 
epochs.
 
Common
 
schedules
 
include
 
exponential
 
decay,
 
step
 
decay,
 
and
 
cosine
 
annealing.
 
○
 
Adaptive
 
Schedulers:
 
Adjust
 
the
 
learning
 
rate
 
based
 
on
 
training
 
performance,
 
such
 
as

---

based
 
on
 
training
 
performance,
 
such
 
as
 
reducing
 
it
 
if
 
validation
 
loss
 
stops
 
improving.
 
 
●
 
Cyclic
 
Learning
 
Rates:
 
○
 
Alternate
 
between
 
a
 
low
 
and
 
high
 
learning
 
rate,
 
which
 
can
 
help
 
the
 
model
 
escape
 
local
 
minima.
 
Techniques
 
like
 
the
  
One
 
Cycle
 
Policy
  
are
 
eﬀective
 
in
 
some
 
cases.
 
 
●
 
Learning
 
Rate
 
Finder:
 
○
 
Train
 
the
 
model
 
with
 
an
 
increasing
 
learning
 
rate
 
and
 
plot
 
the
 
loss.
 
T

---

g
 
learning
 
rate
 
and
 
plot
 
the
 
loss.
 
The
 
optimal
 
learning
 
rate
 
is
 
often
 
where
 
the
 
loss
 
decreases
 
most
 
sharply
 
before
 
rising.
 
 
●
 
 
Trial
 
and
 
Error:
 
○
 
Start
 
with
 
a
 
moderate
 
learning
 
rate
 
(e.g.,
 
0.001),
 
then
 
adjust
 
based
 
on
 
initial
 
model
 
performance.
 
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   17 
 
In
 
summary,
 
the
 
learning
 
rate
 
con

---

17 
 
In
 
summary,
 
the
 
learning
 
rate
 
controls
 
training
 
stability
 
and
 
speed.
 
Optimizing
 
it,
 
often
 
with
 
schedulers
 
or
 
adaptive
 
methods,
 
can
 
lead
 
to
 
faster,
 
more
 
robust
 
training
 
and
 
better
 
model
 
performance.
 
 
Q12.  What  are  some  common  neural  network  based  architectures,  and  
when
 
would
 
you
 
use
 
them? 
Here
 
are
 
some
 
common
 
neural
 
networ-kbased
 
architectures
 
used
 
in
 
Artiﬁcial
 
Neural
 
Networks
 
(ANNs)
 
a

---

in
 
Artiﬁcial
 
Neural
 
Networks
 
(ANNs)
 
and
 
their
 
applications:
 
 
1.
 
Feedforward
 
Neural
 
Networks
 
(FNNs)
 
➢
 
Structure:
 
Consist
 
of
 
input,
 
hidden,
 
and
 
output
 
layers,
 
with
 
each
 
layer
 
fully
 
connected
 
to
 
the
 
next.
 
Information
 
ﬂows
 
in
 
one
 
direction,
 
from
 
input
 
to
 
output.
 
➢
 
Use:
 
Suitable
 
for
 
basic
 
tasks
 
like
 
simple
 
classiﬁcation
 
and
 
regression.
 
They
 
are
 
widely
 
used
 
for
 
structured
 
data
 
tasks,

---

widely
 
used
 
for
 
structured
 
data
 
tasks,
 
such
 
as
 
predicting
 
customer
 
behavior
 
or
 
forecasting
 
sales.
 
 
2.
 
Convolutional
 
Neural
 
Networks
 
(CNNs)
 
➢
 
Structure:
 
Use
 
convolutional
 
layers
 
to
 
process
 
spatial
 
information,
 
followed
 
by
 
pooling
 
layers
 
to
 
downsample
 
features.
 
➢
 
Use:
 
Primarily
 
applied
 
in
 
computer
 
vision
 
tasks
 
(e.g.,
 
image
 
classiﬁcation,
 
object
 
detection)
 
due
 
to
 
their
 
ability
 
to
 
capture
 
loc

---

due
 
to
 
their
 
ability
 
to
 
capture
 
local
 
spatial
 
features.
 
They
 
are
 
also
 
used
 
for
 
tasks
 
like
 
medical
 
image
 
analysis
 
and
 
video
 
recognition.
 
 
3.
 
Recurrent
 
Neural
 
Networks
 
(RNNs)
 
➢
 
Structure:
 
Have
 
connections
 
that
 
form
 
loops,
 
allowing
 
information
 
to
 
persist,
 
making
 
them
 
suitable
 
for
 
sequential
 
data.
 
➢
 
Use:
 
Best
 
for
 
time
 
series
 
and
 
natural
 
language
 
processing
 
(NLP)
 
tasks,
 
such
 
as
 
lang

---

e
 
processing
 
(NLP)
 
tasks,
 
such
 
as
 
language
 
modeling,
 
text
 
generation,
 
and
 
stock
 
price
 
prediction.
 
Variants
 
like
 
Long
 
ShortTerm
 
Memory
 
(LSTM)
 
and
 
Gated
 
Recurrent
 
Units
 
(GRU)
 
handle
 
long-term
 
dependencies
 
better.
 
 
4.
 
Transformers
 
➢
 
Structure:
 
Use
 
self-attention
 
mechanisms
 
to
 
capture
 
relationships
 
between
 
distant
 
words
 
or
 
symbols
 
in
 
sequences.
 
➢
 
Use:
 
Essential
 
for
 
NLP
 
tasks
 
(e.g.,
 
language
 
t

---

ential
 
for
 
NLP
 
tasks
 
(e.g.,
 
language
 
translation,
 
text
 
summarization)
 
and,
 
more
 
recently,
 
image
 
processing.
 
Transformers
 
handle
 
longer
 
context
 
and
 
are
 
state-of-the-art
 
in
 
many
 
NLP
 
applications.
 
 
Each
 
architecture
 
excels
 
in
 
speciﬁc
 
tasks,
 
and
 
choosing
 
the
 
right
 
one
 
depends
 
on
 
the
 
data
 
type
 
(e.g.,
 
sequential,
 
spatial,
 
structured)
 
and
 
the
 
complexity
 
of
 
the
 
problem.
 
 
 

MD  ARBAAZ  KHAN

---

the
 
problem.
 
 
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   18 
 
Q13.  What  is  a  convolutional  neural  network  (CNN),  and  how  does  it  
differ
 
from
 
an
 
artiﬁcial
 
neural
 
network? 
A
 
Convolutional
 
Neural
 
Network
 
(CNN)
 
is
 
a
 
type
 
of
 
neural
 
network
 
speciﬁcally
 
designed
 
to
 
process
 
grid-like
 
data,
 
such
 
as
 
images.
 
Unlike
 
standard
 
Artiﬁcial
 
Neural
 
Networks

---

nlike
 
standard
 
Artiﬁcial
 
Neural
 
Networks
 
(ANNs),
 
which
 
rely
 
on
 
fully
 
connected
 
layers,
 
CNNs
 
use
 
specialized
 
layers
 
to
 
capture
 
spatial
 
hierarchies
 
in
 
data
 
more
 
eﬀectively.
 
 
Key
 
Diﬀerences
 
Between
 
CNNs
 
and
 
ANNs
 
 
Artiﬁcial
 
Neural
 
Networks
 
(ANNs)
 
Convolutional
 
Neural
 
Network
 
(CNN)
 
1.
 
Layer
 
Types
 
Consists
 
of
 
fully
 
connected
 
layers,
 
where
 
each
 
neuron
 
is
 
connected
 
to
 
every
 
neuron
 
in
 
the
 
nex

---

connected
 
to
 
every
 
neuron
 
in
 
the
 
next
 
layer.
 
This
 
works
 
well
 
for
 
structured
 
data
 
but
 
struggles
 
with
 
high-dimensional
 
data
 
like
 
images.
 
Uses
 
convolutional
 
layers
 
that
 
apply
 
ﬁlters
 
(kernels)
 
across
 
the
 
input
 
to
 
detect
 
features
 
(e.g.,
 
edges,
 
textures).
 
It
 
also
 
includes
 
pooling
 
layers
 
to
 
downsample
 
feature
 
maps,
 
reducing
 
computation
 
and
 
focusing
 
on
 
important
 
features.
 
2.
 
Spatial
 
Informatio

---

important
 
features.
 
2.
 
Spatial
 
Information
 
Treats
 
all
 
input
 
features
 
independently,
 
ignoring
 
spatial
 
relationships,
 
which
 
can
 
lead
 
to
 
information
 
loss
 
in
 
tasks
 
requiring
 
pattern
 
recognition.
 
Uses
 
convolutional
 
layers
 
that
 
apply
 
ﬁlters
 
(kernels)
 
across
 
the
 
input
 
to
 
detect
 
features
 
(e.g.,
 
edges,
 
textures).
 
It
 
also
 
includes
 
pooling
 
layers
 
to
 
downsample
 
feature
 
maps,
 
reducing
 
computation
 
and
 
focu

---

ture
 
maps,
 
reducing
 
computation
 
and
 
focusing
 
on
 
important
 
features.
 
3.
 
Parameter
 
Eﬃciency
 
Fully
 
connected
 
layers
 
result
 
in
 
a
 
large
 
number
 
of
 
parameters,
 
especially
 
with
 
high-dimensional
 
inputs,
 
which
 
can
 
lead
 
to
 
overﬁtting.
 
Convolutional
 
layers
 
share
 
weights
 
across
 
diﬀerent
 
regions,
 
signiﬁcantly
 
reducing
 
the
 
parameter
 
count,
 
making
 
CNNs
 
more
 
computationally
 
eﬃcient.
 

MD  ARBAAZ  KHAN

---

y
 
eﬃcient.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   19 
 
 
 
 
CNNs
 
are
 
designed
 
to
 
recognize
 
patterns
 
in
 
grid-structured
 
data
 
by
 
preserving
 
spatial
 
structure,
 
making
 
them
 
well-suited
 
for
 
tasks
 
like
 
image
 
classiﬁcation
 
and
 
object
 
detection.
 
Their
 
architecture,
 
with
 
convolutional
 
and
 
pooling
 
layers,
 
allows
 
them
 
to
 
handle
 
high-dimensional
 
data

---

ws
 
them
 
to
 
handle
 
high-dimensional
 
data
 
more
 
eﬃciently
 
than
 
standard
 
ANNs.
 
 
Q14.How  does  a  recurrent  neural  network  (RNN)  work,  and  what  are  its  
limitations? 
A
 
Recurrent
 
Neural
 
Network
 
(RNN)
 
is
 
a
 
type
 
of
 
neural
 
network
 
designed
 
for
 
processing
 
sequential
 
data.
 
Unlike
 
traditional
 
feedforward
 
networks,
 
RNNs
 
have
 
connections
 
that
 
loop
 
back
 
on
 
themselves,
 
allowing
 
them
 
to
 
maintain
 
a
 
hidden
 
state

---

wing
 
them
 
to
 
maintain
 
a
 
hidden
 
state
 
that
 
can
 
capture
 
information
 
about
 
previous
 
inputs
 
in
 
the
 
sequence.
 
This
 
characteristic
 
makes
 
RNNs
 
particularly
 
eﬀective
 
for
 
tasks
 
such
 
as
 
time
 
series
 
prediction,
 
natural
 
language
 
processing,
 
and
 
speech
 
recognition.
 
 
How  RNNs  Work
 
1.
 
Sequential
 
Processing:
 
RNNs
 
process
 
data
 
one
 
element
 
at
 
a
 
time
 
while
 
maintaining
 
a
 
hidden
 
state
 
that
 
gets
 
updated

---

ng
 
a
 
hidden
 
state
 
that
 
gets
 
updated
 
with
 
each
 
input.
 
 
At
 
each
 
time
 
step
 
,
 
the
 
RNN
 
takes
 
the
 
current
 
input
 
 
and
 
the
 
previous
 
hidden
 
state
 
 
to
 
compute
 
the
 
new
 
hidden
 
state
 
:
 
 
   
 
   
 
where
 
 
and
 
 
are
 
weight
 
matrices,
 
 
is
 
a
 
bias
 
vector,
 
 
and
 
 
is
 
an
 
activation
 
function
 
(often
 
tanh
 
or
 
ReLU).
 

MD  ARBAAZ  KHAN                                                                Generative  AI  I

---

Generative  AI  Interview  Questions   20 
 
2.
 
Output
 
Generation:
 
The
 
output
 
at
 
each
 
time
 
step
 
can
 
be
 
computed
 
based
 
on
 
the
 
hidden
 
state:
 
   
 
   
where
 
 
is
 
the
 
weight
 
matrix
 
for
 
the
 
output
 
and
 
 
is
 
the
 
corresponding
 
bias.
 
3.
 
Backpropagation
 
Through
 
Time
 
(BPTT):
 
To
 
train
 
RNNs,
 
backpropagation
 
is
 
extended
 
through
 
the
 
time
 
steps,
 
allowing
 
the
 
model
 
to
 
learn
 
from

---

ps,
 
allowing
 
the
 
model
 
to
 
learn
 
from
 
the
 
sequence
 
of
 
inputs.
 
 Limitations  of  RNNs
 
1.
 
Vanishing
 
and
 
Exploding
 
Gradients:
 
During
 
training,
 
gradients
 
can
 
become
 
very
 
small
 
(vanishing)
 
or
 
very
 
large
 
(exploding),
 
making
 
it
 
diﬃcult
 
for
 
RNNs
 
to
 
learn
 
long-range
 
dependencies
 
eﬀectively.
 
This
 
issue
 
is
 
particularly
 
pronounced
 
with
 
long
 
sequences.
 
 
2.
 
Short-term
 
Memory:
 
RNNs
 
struggle
 
to
 
remember
 
i

---

rm
 
Memory:
 
RNNs
 
struggle
 
to
 
remember
 
information
 
from
 
distant
 
past
 
inputs
 
due
 
to
 
their
 
reliance
 
on
 
hidden
 
states.
 
While
 
they
 
can
 
capture
 
some
 
context,
 
their
 
memory
 
is
 
limited
 
compared
 
to
 
other
 
architectures
 
like
 
Long
 
ShortTerm
 
Memory
 
(LSTM)
 
networks
 
or
 
Gated
 
Recurrent
 
Units
 
(GRU),
 
which
 
are
 
speciﬁcally
 
designed
 
to
 
mitigate
 
this
 
limitation.
 
 
3.
 
Computational
 
Ineﬃciency:
 
Training
 
RNNs
 
c

---

Computational
 
Ineﬃciency:
 
Training
 
RNNs
 
can
 
be
 
slower
 
compared
 
to
 
feedforward
 
networks
 
due
 
to
 
the
 
sequential
 
nature
 
of
 
data
 
processing.
 
Each
 
time
 
step
 
depends
 
on
 
the
 
previous
 
one,
 
preventing
 
parallelization
 
of
 
computations.
 
 
4.
 
Diﬃculty
 
in
 
Handling
 
Long
 
Sequences:
 
While
 
RNNs
 
can
 
work
 
with
 
variable-length
 
sequences,
 
they
 
tend
 
to
 
perform
 
poorly
 
on
 
very
 
long
 
sequences
 
due
 
to
 
the
 
limitat

---

very
 
long
 
sequences
 
due
 
to
 
the
 
limitations
 
of
 
their
 
hidden
 
state.
 
 
RNNs
 
are
 
powerful
 
for
 
sequential
 
data
 
tasks
 
due
 
to
 
their
 
ability
 
to
 
maintain
 
a
 
memory
 
of
 
previous
 
inputs.
 
However,
 
they
 
face
 
challenges
 
like
 
vanishing
 
gradients
 
and
 
ineﬃciencies
 
in
 
handling
 
long
 
sequences.
 
Advanced
 
variants
 
like
 
LSTMs
 
and
 
GRUs
 
address
 
some
 
of
 
these
 
limitations
 
by
 
incorporating
 
mechanisms
 
to
 
better

---

by
 
incorporating
 
mechanisms
 
to
 
better
 
manage
 
memory
 
and
 
gradients.
 
 
 
 
 
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   21 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
        
Classical  Natural  Language  Processing          

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   22 
 
Q1.  What  is  tokenization?  Give  me  a  difference

---

What  is  tokenization?  Give  me  a  difference  between  lemmatization  
and
 
stemming?
 
Tokenization
 
is
 
the
 
process
 
of
 
breaking
 
a
 
sequence
 
of
 
text
 
into
 
smaller
 
pieces
 
called
 
tokens,
 
which
 
can
 
be
 
words,
 
phrases,
 
or
 
sentences.
 
Tokenization
 
helps
 
in
 
processing
 
text
 
by
 
dividing
 
it
 
into
 
manageable
 
units,
 
discarding
 
irrelevant
 
characters
 
like
 
punctuation.
 
 
Difference  Between  Stemming  and  Lemmatization
 
 
Stemming:

---

ween  Stemming  and  Lemmatization
 
 
Stemming:
 
Stemming
 
reduces
 
words
 
to
 
their
 
root
 
form,
 
often
 
by
 
removing
 
suﬃxes
 
or
 
preﬁxes.
 
The
 
resulting
 
"stem"
 
may
 
not
 
be
 
a
 
real
 
word
 
but
 
represents
 
a
 
base
 
form.
 
For
 
example,
 
"running,"
 
"runner,"
 
and
 
"runs"
 
could
 
all
 
stem
 
to
 
"run,"
 
even
 
though
 
"run"
 
might
 
not
 
ﬁt
 
all
 
contexts
 
in
 
a
 
sentence.
 
 
Lemmatization:
 
Lemmatization
 
also
 
reduces
 
words
 
to
 
a
 
b

---

emmatization
 
also
 
reduces
 
words
 
to
 
a
 
base
 
form,
 
but
 
it
 
considers
 
context
 
and
 
the
 
word's
 
meaning.
 
The
 
result,
 
called
 
a
 
lemma,
 
is
 
a
 
real
 
word
 
in
 
the
 
language.
 
For
 
instance,
 
"running"
 
and
 
"ran"
 
would
 
both
 
be
 
lemmatized
 
to
 
"run,"
 
taking
 
into
 
account
 
their
 
meanings
 
and
 
grammar.
 
This
 
makes
 
lemmatization
 
more
 
accurate
 
than
 
stemming,
 
but
 
it's
 
typically
 
slower
 
due
 
to
 
the
 
need
 
for
 
a

---

ically
 
slower
 
due
 
to
 
the
 
need
 
for
 
a
 
vocabulary
 
lookup.
 
 
 
Q2.  Explain  the  concept  of  Bag  of  Words  (BoW)  and  its  limitations.  
The
 
Bag
 
of
 
Words
 
(BoW)
 
model
 
is
 
a
 
common
 
technique
 
in
 
natural
 
language
 
processing
 
for
 
representing
 
text.
 
It
 
transforms
 
a
 
text
 
document
 
into
 
a
 
ﬁxed-length
 
vector
 
based
 
on
 
word
 
occurrences,
 
ignoring
 
grammar,
 
word
 
order,
 
and
 
semantics.
 
Each
 
unique
 
word
 
in
 
the
 
te

---

semantics.
 
Each
 
unique
 
word
 
in
 
the
 
text
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   23 
 
corpus
 
becomes
 
a
 
feature,
 
and
 
its
 
value
 
in
 
a
 
document's
 
vector
 
represents
 
the
 
frequency
 
of
 
that
 
word.
 
 
How  BoW  Works
 
1.
 
Tokenization:
 
The
 
text
 
is
 
split
 
into
 
tokens
 
(usually
 
words).
 
2.
 
Vocabulary
 
Creation:
 
All
 
unique
 
words
 
across
 
the
 
corpus
 
ar

---

All
 
unique
 
words
 
across
 
the
 
corpus
 
are
 
gathered
 
to
 
form
 
a
 
vocabulary.
 
3.
 
Vector
 
Representation:
 
Each
 
document
 
is
 
represented
 
by
 
a
 
vector
 
where
 
each
 
element
 
corresponds
 
to
 
a
 
word
 
in
 
the
 
vocabulary.
 
The
 
value
 
can
 
be:
 
●
 
The
 
word
 
s
 
frequency
 
in
 
the
 
document
 
(term
 
frequency).
 
●
 
A
 
binary
 
presence
 
(1
 
if
 
the
 
word
 
appears,
 
0
 
otherwise).
 
 
For
 
example,
 
with
 
a
 
corpus
 
containing
  
I

---

example,
 
with
 
a
 
corpus
 
containing
  
I
 
like
 
NLP
  
and
  
I
 
like
 
machine
 
learning,
  
the
 
BoW
 
representation
 
might
 
look
 
like
 
this:
 
 
 
Document
 
I
 
like
 
NLP
 
machine
 
learning
 
“I
 
like
 
NLP”
 
1
 
1
 
1
 
0
 
0
 
“I
 
like
 
machine
 
learning”
 
1
 
1
 
0
 
1
 
1
 
 
 
Limitations  of  Bag  of  Words
 
●
 
Ignores
 
Word
 
Order:
 
BoW
 
does
 
not
 
capture
 
the
 
sequence
 
of
 
words,
 
so
  
dog
 
bites
 
man
  
and
  
man
 
bites
 
dog
  
are

---

bites
 
man
  
and
  
man
 
bites
 
dog
  
are
 
represented
 
identically.
 
 
●
 
High
 
Dimensionality:
 
With
 
large
 
vocabularies,
 
the
 
BoW
 
vector
 
becomes
 
high-dimensional,
 
increasing
 
computation
 
and
 
memory
 
requirements.
 
 
●
 
No
 
Semantic
 
Understanding:
 
The
 
model
 
doesn’t
 
understand
 
context
 
or
 
relationships
 
between
 
words;
 
synonyms
 
or
 
related
 
words
 
are
 
treated
 
independently.
 
 
●
 
Sparse
 
Representation:
 
Many
 
elements
 
in

---

Sparse
 
Representation:
 
Many
 
elements
 
in
 
the
 
vector
 
are
 
often
 
zero
 
due
 
to
 
a
 
large
 
vocabulary
 
and
 
limited
 
document
 
length,
 
leading
 
to
 
sparsity
 
and
 
ineﬃciency.
 
 
Despite
 
these
 
limitations,
 
BoW
 
can
 
be
 
useful
 
for
 
simple
 
text
 
classiﬁcation
 
tasks
 
but
 
is
 
often
 
outperformed
 
by
 
advanced
 
models
 
like
 
Word2Vec,
 
TFIDF,
 
and
 
deep
 
learning
 
methods.
 
 

MD  ARBAAZ  KHAN

---

HAN                                                                Generative  AI  Interview  Questions   24 
 
Q3.  How  does  TF-IDF  work,  and  how  is  it  different  from  simple  word  
frequency?
 
TF-IDF
 
(Term
 
FrequencyInverse
 
Document
 
Frequency)
 
is
 
a
 
statistical
 
measure
 
used
 
in
 
text
 
analysis
 
to
 
evaluate
 
the
 
importance
 
of
 
a
 
word
 
in
 
a
 
document
 
relative
 
to
 
a
 
collection
 
of
 
documents
 
(the
 
corpus).
 
Unlike
 
simple
 
word
 
frequen

---

(the
 
corpus).
 
Unlike
 
simple
 
word
 
frequency,
 
which
 
counts
 
the
 
number
 
of
 
times
 
a
 
word
 
appears,
 
TFIDF
 
adjusts
 
the
 
frequency
 
by
 
giving
 
less
 
weight
 
to
 
common
 
words
 
and
 
more
 
to
 
rare
 
but
 
meaningful
 
words.
 
 
 How  TF-IDF  Works  
 
TFIDF
 
consists
 
of
 
two
 
main
 
components:
 
 
1.
 
Term
 
Frequency
 
(TF):
 
Measures
 
how
 
frequently
 
a
 
word
 
appears
 
in
 
a
 
document.
 
It’s
 
calculated
 
as:
 
   
 
 
2.
 
Inverse
 
Docu

---

s
 
calculated
 
as:
 
   
 
 
2.
 
Inverse
 
Document
 
Frequency
 
(IDF):
 
Measures
 
the
 
importance
 
of
 
the
 
word
 
across
 
the
 
entire
 
corpus.
 
Words
 
that
 
appear
 
in
 
many
 
documents
 
(e.g.,
 
"the,"
 
"is")
 
have
 
a
 
low
 
IDF
 
score,
 
while
 
rare
 
words
 
have
 
a
 
high
 
IDF.
 
It
 
is
 
calculated
 
as:
 
   
 
 
The
 
TF-IDF
 
score
 
for
 
each
 
word
 
in
 
a
 
document
 
is
 
the
 
product
 
of
 
its
 
TF
 
and
 
IDF
 
values:
 
 
 
Difference  from  Simpl

---

and
 
IDF
 
values:
 
 
 
Difference  from  Simple  Word  Frequency
 
●
 
Importance
 
Weighting:
 
Simple
 
word
 
frequency
 
counts
 
how
 
often
 
a
 
word
 
appears
 
without
 
adjusting
 
for
 
relevance.
 
TFIDF,
 
however,
 
reduces
 
the
 
weight
 
of
 
commonly
 
occurring
 
words
 
and
 
ampliﬁes
 
less
 
common,
 
meaningful
 
words.
 
 
●
 
Context
 
Sensitivity:
 
TFIDF
 
considers
 
how
 
unique
 
a
 
word
 
is
 
to
 
a
 
particular
 
document
 
within
 
a
 
corpus,
 
making
 
it

---

ar
 
document
 
within
 
a
 
corpus,
 
making
 
it
 
more
 
eﬀective
 
for
 
identifying
 
keywords
 
than
 
word
 
frequency,
 
which
 
treats
 
all
 
words
 
equally
 
across
 
documents.
 
 
Q4.  What  is  word  embedding,  and  why  is  it  useful  in  NLP? 
Word
 
embedding
 
is
 
a
 
technique
 
in
 
Natural
 
Language
 
Processing
 
(NLP)
 
where
 
words
 
or
 
phrases
 
from
 
a
 
vocabulary
 
are
 
mapped
 
to
 
vectors
 
of
 
real
 
numbers.
 
This
 
process
 
creates
 
a
 
dense,
 
co

---

bers.
 
This
 
process
 
creates
 
a
 
dense,
 
continuous
 
vector
 
representation
 
of
 
words
 
that
 
captures
 
their
 
meanings,
 
syntactic
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   25 
 
roles,
 
and
 
relationships
 
to
 
other
 
words
 
based
 
on
 
their
 
contexts.
 
Word
 
embeddings
 
are
 
typically
 
trained
 
on
 
large
 
corpora
 
and
 
use
 
models
 
like
 
Word2Vec,
 
GloVe,
 
or
 
FastText.

---

like
 
Word2Vec,
 
GloVe,
 
or
 
FastText.
 
 
How  Word  Embeddings  Work
 
➢
 
Each
 
word
 
is
 
represented
 
as
 
a
 
vector
 
in
 
a
 
high-dimensional
 
space.
 
 
➢
 
Words
 
that
 
appear
 
in
 
similar
 
contexts
 
or
 
have
 
similar
 
meanings
 
are
 
positioned
 
closer
 
together
 
in
 
this
 
space.
 
 
➢
 
For
 
example,
 
in
 
a
 
word
 
embedding
 
model,
 
vectors
 
for
 
"king"
 
and
 
"queen"
 
or
 
"happy"
 
and
 
"joyful"
 
would
 
be
 
located
 
near
 
each
 
other.

---

"
 
would
 
be
 
located
 
near
 
each
 
other.
 
 
 
These
 
embeddings
 
are
 
trained
 
using
 
unsupervised
 
learning
 
on
 
a
 
large
 
text
 
corpus.
 
Methods
 
like
 
Word2Vec
 
use
 
neural
 
networks
 
to
 
learn
 
word
 
embeddings
 
by
 
predicting
 
a
 
word
 
from
 
its
 
context
 
(skip-gram)
 
or
 
by
 
predicting
 
the
 
context
 
from
 
a
 
word
 
(CBOW,
 
or
 
continuous
 
bag
 
of
 
words).
 
 
 
Why  Word  Embeddings  are  Useful  in  NLP
 
1.
 
Capturing
 
Semantic
 
Meani

---

seful  in  NLP
 
1.
 
Capturing
 
Semantic
 
Meaning:
 
Word
 
embeddings
 
preserve
 
semantic
 
relationships,
 
enabling
 
NLP
 
models
 
to
 
understand
 
similarity,
 
analogy,
 
and
 
context.
 
For
 
instance,
 
the
 
embeddings
 
can
 
capture
 
analogies
 
like
 
"king
  
man
 
+
 
woman
 
≈
 
queen."
 
 
2.
 
Dimensionality
 
Reduction:
 
Unlike
 
the
 
sparse
 
vectors
 
in
 
traditional
 
Bag
 
of
 
Words
 
or
 
TFIDF
 
representations,
 
word
 
embeddings
 
are
 
dense
 
and
 
lower

---

s,
 
word
 
embeddings
 
are
 
dense
 
and
 
lower-dimensional,
 
making
 
them
 
computationally
 
eﬃcient
 
and
 
less
 
memory-intensive.
 
 
3.
 
Improved
 
Model
 
Performance:
 
Word
 
embeddings
 
enable
 
NLP
 
models
 
to
 
generalize
 
better
 
by
 
understanding
 
words
 
in
 
context.
 
This
 
improves
 
performance
 
in
 
tasks
 
like
 
text
 
classiﬁcation,
 
sentiment
 
analysis,
 
machine
 
translation,
 
and
 
more.
 
 
4.
 
Transferable
 
Knowledge:
 
Pre-trained
 
word
 
embed

---

sferable
 
Knowledge:
 
Pre-trained
 
word
 
embeddings
 
on
 
large
 
corpora
 
(e.g.,
 
Google
 
News,
 
Wikipedia)
 
can
 
be
 
used
 
for
 
various
 
NLP
 
tasks
 
without
 
retraining,
 
saving
 
time
 
and
 
resources.
 
 
Word
 
embeddings
 
have
 
become
 
foundational
 
in
 
NLP
 
for
 
their
 
ability
 
to
 
enrich
 
models
 
with
 
contextual
 
understanding,
 
boosting
 
the
 
performance
 
of
 
applications
 
across
 
many
 
domains.
 
 
Q5.  What  are  some  common  applications  o

---

Q5.  What  are  some  common  applications  of  NLP  in  real-world  systems?  
Natural
 
Language
 
Processing
 
(NLP)
 
powers
 
numerous
 
real-world
 
applications
 
that
 
make
 
technology
 
more
 
intuitive
 
and
 
accessible.
 
Here
 
are
 
some
 
common
 
applications:
 
 
1.
 
Chatbots
 
and
 
Virtual
 
Assistants:
 
NLP
 
enables
 
chatbots
 
(e.g.,
 
on
 
websites
 
or
 
customer
 
service
 
platforms)
 
and
 
virtual
 
assistants
 
(like
 
Siri,
 
Alexa,
 
or
 
Google
 

MD  AR

---

s
 
(like
 
Siri,
 
Alexa,
 
or
 
Google
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   26 
 
Assistant)
 
to
 
understand
 
and
 
respond
 
to
 
human
 
language,
 
helping
 
users
 
with
 
tasks,
 
questions,
 
or
 
even
 
purchasing.
 
 
2.
 
Sentiment
 
Analysis:
 
NLP
 
analyzes
 
opinions
 
or
 
emotions
 
expressed
 
in
 
text,
 
such
 
as
 
social
 
media
 
posts,
 
product
 
reviews,
 
or
 
customer
 
feedback.

---

product
 
reviews,
 
or
 
customer
 
feedback.
 
Sentiment
 
analysis
 
helps
 
companies
 
gauge
 
public
 
opinion,
 
improve
 
customer
 
experience,
 
and
 
manage
 
brand
 
reputation.
 
 
3.
 
Machine
 
Translation:
 
Services
 
like
 
Google
 
Translate
 
use
 
NLP
 
to
 
translate
 
text
 
from
 
one
 
language
 
to
 
another
 
by
 
learning
 
contextual
 
nuances,
 
enabling
 
multilingual
 
communication
 
and
 
breaking
 
language
 
barriers.
 
 
4.
 
Spam
 
Filtering:
 
NLP
 
ﬁlter

---

barriers.
 
 
4.
 
Spam
 
Filtering:
 
NLP
 
ﬁlters
 
out
 
spam
 
or
 
malicious
 
messages
 
in
 
emails
 
by
 
analyzing
 
text
 
patterns,
 
keywords,
 
and
 
even
 
tone
 
to
 
distinguish
 
spam
 
from
 
legitimate
 
messages,
 
helping
 
to
 
reduce
 
phishing
 
and
 
unwanted
 
content.
 
 
5.
 
Speech
 
Recognition:
 
Voice-to-text
 
systems
 
like
 
those
 
in
 
call
 
centers
 
or
 
virtual
 
assistants
 
transcribe
 
spoken
 
language
 
into
 
written
 
text,
 
enabling
 
voice
 
com

---

into
 
written
 
text,
 
enabling
 
voice
 
commands,
 
automated
 
transcription,
 
and
 
accessibility
 
for
 
users
 
with
 
disabilities.
 
 
6.
 
Text
 
Summarization:
 
NLP
 
condenses
 
long
 
articles,
 
news
 
reports,
 
or
 
documents
 
into
 
brief
 
summaries,
 
providing
 
users
 
with
 
key
 
information
 
quickly.
 
This
 
is
 
helpful
 
in
 
news
 
aggregation,
 
research,
 
and
 
document
 
analysis.
 
 
7.
 
Information
 
Retrieval
 
and
 
Search:
 
Search
 
engines
 
and

---

rieval
 
and
 
Search:
 
Search
 
engines
 
and
 
knowledge
 
management
 
systems
 
use
 
NLP
 
to
 
understand
 
user
 
queries,
 
match
 
them
 
with
 
relevant
 
documents,
 
and
 
rank
 
results,
 
making
 
information
 
retrieval
 
more
 
eﬀective.
 
 
8.
 
Document
 
Classiﬁcation
 
and
 
Processing:
 
NLP
 
categorizes
 
and
 
processes
 
documents
 
(e.g.,
 
emails,
 
legal
 
ﬁles,
 
or
 
resumes)
 
by
 
recognizing
 
content
 
patterns,
 
enabling
 
automation
 
in
 
document
 
handlin

---

enabling
 
automation
 
in
 
document
 
handling,
 
fraud
 
detection,
 
and
 
workﬂow
 
management.
 
 
9.
 
Named
 
Entity
 
Recognition
 
(NER):
 
NER
 
identiﬁes
 
entities
 
like
 
names,
 
dates,
 
locations,
 
and
 
organizations
 
within
 
text,
 
assisting
 
in
 
extracting
 
structured
 
information
 
from
 
unstructured
 
data
 
for
 
tasks
 
in
 
ﬁelds
 
like
 
legal,
 
medical,
 
and
 
ﬁnance.
 
 
10.
 
Recommendation
 
Systems:
 
By
 
analyzing
 
user
 
reviews,
 
preferences,

---

By
 
analyzing
 
user
 
reviews,
 
preferences,
 
and
 
search
 
patterns,
 
NLP
 
helps
 
in
 
making
 
personalized
 
recommendations
 
for
 
products,
 
movies,
 
news
 
articles,
 
or
 
content.
 
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   27 
 
11.
 
Healthcare
 
Applications
:
 
NLP
 
is
 
used
 
in
 
healthcare
 
for
 
analyzing
 
clinical
 
notes,
 
extracting
 
relevant
 
information
 
from
 
patient
 
recor

---

relevant
 
information
 
from
 
patient
 
records,
 
and
 
even
 
predicting
 
patient
 
outcomes
 
based
 
on
 
textual
 
data.
 
 
These
 
applications
 
leverage
 
NLP
 
to
 
make
 
interactions
 
with
 
technology
 
more
 
intuitive,
 
automate
 
repetitive
 
tasks,
 
and
 
extract
 
insights
 
from
 
vast
 
amounts
 
of
 
unstructured
 
data,
 
beneﬁting
 
industries
 
from
 
retail
 
and
 
healthcare
 
to
 
ﬁnance
 
and
 
entertainment.
 
 Q6.  What  is  Named  Entity  Recognition  (NER

---

Q6.  What  is  Named  Entity  Recognition  (NER),  and  where  is  it  applied? Named  Entity  Recognition  (NER)  is  an  NLP  technique  for  identifying  and  
categorizing
 
speciﬁc
 
entities,
 
such
 
as
 
names,
 
locations,
 
organizations,
 
dates,
 
and
 
quantities,
 
in
 
a
 
text.
 
NER
 
structures
 
unstructured
 
text
 
by
 
tagging
 
these
 
entities,
 
enabling
 
easier
 
analysis
 
and
 
information
 
extraction.
  Applications  of  NER  1.  Information  Extraction:  Extrac

---

ons  of  NER  1.  Information  Extraction:  Extracts  relevant  data  from  large  text  corpora,  such  
as
 
people
 
and
 
places
 
in
 
news.
 2.  Search  Engines:  Improves  search  relevance  by  understanding  user  queries'  
entities.
 3.  Customer  Support:  Helps  chatbots  recognize  and  respond  to  key  entities  like  
names
 
and
 
dates
 
in
 
queries.
 4.  Healthcare:  Extracts  medical  entities  like  symptoms  and  drugs  from  patient  
notes.
 5.  Finance:  Identiﬁes  com

---

om  patient  
notes.
 5.  Finance:  Identiﬁes  companies,  terms,  and  monetary  values  in  ﬁnancial  
reports.
 6.  Legal:  Aids  in  extracting  names,  dates,  and  legal  terms  from  legal  documents.   
Beneﬁts  of  NER  NER  structures  data,  making  it  more  searchable  and  useful  for  tasks  like  
information
 
retrieval,
 
sentiment
 
analysis,
 
and
 
automation
 
across
 
various
 
ﬁelds.
 
It
 
helps
 
systems
 
process
 
critical
 
details,
 
enabling
 
better
 
decision-mak

---

ical
 
details,
 
enabling
 
better
 
decision-making
 
and
 
efﬁcient
 
data
 
handling.
  
Q7.  How  does  Latent  Dirichlet  Allocation  (LDA)  work  for  topic  modeling?  Latent  Dirichlet  Allocation  (LDA)  is  a  popular  algorithm  used  for  topic  modeling,  
which
 
aims
 
to
 
discover
 
hidden
 
topics
 
in
 
a
 
collection
 
of
 
documents.
 
 LDA  assumes  that  documents  are  mixtures  of  various  topics  and  that  each  topic  is  a  
mixture
 
of
 
words
 
with
 
certain

---

is  a  
mixture
 
of
 
words
 
with
 
certain
 
probabilities.
 
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   28 
 
It's  a  generative  probabilistic  model,  meaning  it  attempts  to  explain  observed  data  
(words
 
in
 
documents)
 
based
 
on
 
assumed
 
underlying
 
distributions
 
of
 
topics
 
and
 
words.
  
How  LDA  Works ●  Assumptions:  LDA  assumes:  ○  Each  document  consists  of  multiple  topics.

---

Each  document  consists  of  multiple  topics.  ○  Each  topic  is  a  distribution  over  words.   ●  Process:  LDA  uses  a  probabilistic  approach  to  assign  words  to  topics  within  
each
 
document:
 ○  For  each  document,  LDA  randomly  assigns  each  word  to  a  topic.  ○  Then,  it  iteratively  reﬁnes  these  assignments  based  on  two  main  
probabilities:
 ■  DocumentTopic  Distribution:  The  probability  that  a  given  topic  
is
 
in
 
the
 
document.
 ■  TopicWord  D

---

topic  
is
 
in
 
the
 
document.
 ■  TopicWord  Distribution:  The  probability  of  a  word  appearing  in  
a
 
speciﬁc
 
topic.
  ●  Iterative  Reﬁnement:  LDA  repeatedly  adjusts  wordtopic  assignments  to  
maximize
 
these
 
probabilities,
 
estimating
 
each
 
word's
 
afﬁliation
 
with
 
a
 
topic.
 ●   ●  Output:  The  ﬁnal  result  is  a  set  of  topics  (groups  of  related  words)  and  a  
distribution
 
of
 
topics
 
within
 
each
 
document.
  
Why  LDA  is  Useful LDA  provid

---

ch
 
document.
  
Why  LDA  is  Useful LDA  provides  insight  into  the  structure  of  a  document  corpus  by  identifying  
common
 
themes
 
or
 
topics
 
across
 
the
 
texts.
 
This
 
is
 
useful
 
for
 
applications
 
like
 
document
 
classiﬁcation,
 
information
 
retrieval,
 
and
 
discovering
 
trends
 
in
 
large
 
text
 
datasets.
  
Q8.  What  are  transformers  in  NLP,  and  how  have  they  impacted  the  ﬁeld? Transformers  are  a  type  of  neural  network  architecture  intr

---

a  type  of  neural  network  architecture  introduced  in  the  paper  
"Attention
 
is
 
All
 
You
 
Need"
 
by
 
Vaswani
 
et
 
al.
 
in
 
2017.
 
 They  have  become  the  foundation  of  many  state-of-the-art  models  in  Natural  
Language
 
Processing
 
(NLP)
 
due
 
to
 
their
 
ability
 
to
 
handle
 
sequential
 
data
 
efﬁciently
 
and
 
effectively.
 
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   29 
 
Tra

---

Generative  AI  Interview  Questions   29 
 
Transformers  leverage  a  mechanism  called  self-attention,  which  allows  the  model  
to
 
weigh
 
the
 
importance
 
of
 
different
 
words
 
in
 
a
 
sentence
 
relative
 
to
 
each
 
other,
 
regardless
 
of
 
their
 
position.
  
Key  Features  of  Transformers 1.  Self-Attention  Mechanism:  This  allows  
the
 
model
 
to
 
consider
 
the
 
context
 
of
 
all
 
words
 
in
 
a
 
sentence
 
simultaneously,
 
enabling
 
it
 
to
 
capture
 
l

---

simultaneously,
 
enabling
 
it
 
to
 
capture
 
long-range
 
dependencies
 
and
 
relationships
 
effectively.
  2.  Positional  Encoding:  Since  transformers  
do
 
not
 
inherently
 
understand
 
the
 
order
 
of
 
input
 
data
 
(unlike
 
recurrent
 
networks),
 
they
 
use
 
positional
 
encodings
 
to
 
retain
 
information
 
about
 
the
 
position
 
of
 
each
 
word
 
in
 
the
 
sequence.
  3.  Parallelization:  Unlike  recurrent  neural  
networks
 
(RNNs),
 
transformers
 
can
 
proces

---

networks
 
(RNNs),
 
transformers
 
can
 
process
 
words
 
in
 
a
 
sequence
 
simultaneously,
 
leading
 
to
 
signiﬁcantly
 
faster
 
training
 
times.
  4.  Encoder-Decoder  Architecture:  The  
transformer
 
model
 
typically
 
consists
 
of
 
an
 
encoder
 
that
 
processes
 
the
 
input
 
text
 
and
 
a
 
decoder
 
that
 
generates
 
the
 
output,
 
although
 
many
 
modern
 
applications
 
use
 
just
 
the
 
encoder
 
(e.g.,
 
BERT)
 
or
 
just
 
the
 
decoder
 
(e.g.,
 
GPT).
  
Impac

---

r
 
just
 
the
 
decoder
 
(e.g.,
 
GPT).
  
Impact  on  NLP  1.  Performance  Improvements:  Transformers  have  led  to  signiﬁcant  improvements  
in
 
various
 
NLP
 
tasks,
 
such
 
as
 
machine
 
translation,
 
text
 
classiﬁcation,
 
question
 
answering,
 
and
 
sentiment
 
analysis.
 
Models
 
like
 
BERT,
 
GPT2,
 
and
 
GPT3
 
have
 
set
 
new
 
benchmarks
 
for
 
accuracy.
  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Q

---

Generative  AI  Interview  Questions   30 
 
2.  Transfer  Learning:  The  transformer  architecture  enables  pretraining  on  large  
corpora
 
and
 
ﬁne-tuning
 
on
 
speciﬁc
 
tasks,
 
allowing
 
models
 
to
 
leverage
 
learned
 
knowledge
 
effectively.
 
This
 
has
 
made
 
it
 
easier
 
to
 
apply
 
NLP
 
models
 
across
 
different
 
domains
 
with
 
less
 
task-speciﬁc
 
data.
  3.  Innovation  in  Model  Design:  The  introduction  of  transformers  has  sparked

---

The  introduction  of  transformers  has  sparked  
further
 
research
 
and
 
innovation
 
in
 
the
 
ﬁeld,
 
leading
 
to
 
numerous
 
variations
 
and
 
improvements,
 
such
 
as
 
RoBERTa,
 
T5,
 
and
 
DistilBERT,
 
each
 
designed
 
to
 
enhance
 
performance
 
or
 
efﬁciency.
  4.  Accessibility:  Transformers  have  made  powerful  NLP  tools  more  accessible,  with  
numerous
 
pretrained
 
models
 
available
 
in
 
libraries
 
like
 
Hugging
 
Face's
 
Transformers,
 
allowing
 
devel

---

ugging
 
Face's
 
Transformers,
 
allowing
 
developers
 
to
 
implement
 
state-of-the-art
 
solutions
 
with
 
ease.
  In  summary,  transformers  have  revolutionized  NLP  by  providing  a  robust,  efﬁcient,  
and
 
ﬂexible
 
architecture
 
that
 
has
 
improved
 
performance
 
across
 
a
 
wide
 
range
 
of
 
tasks
 
and
 
has
 
fostered
 
ongoing
 
innovation
 
in
 
the
 
ﬁeld.
  
Q9.  What  is  transfer  learning,  and  how  is  it  applied  in  NLP? Transfer  learning  is  a  machine  l

---

ed  in  NLP? Transfer  learning  is  a  machine  learning  technique  where  a  model  developed  for  a  
speciﬁc
 
task
 
is
 
reused
 
as
 
the
 
starting
 
point
 
for
 
a
 
model
 
on
 
a
 
different
 
but
 
related
 
task.
 
 This  approach  leverages  knowledge  gained  from  one  task  (source  domain)  to  
improve
 
learning
 
in
 
another
 
task
 
(target
 
domain).
 
 Transfer  learning  is  especially  beneﬁcial  when  the  target  task  has  limited  data,  as  it  
allows
 
for

---

ask  has  limited  data,  as  it  
allows
 
for
 
better
 
generalization
 
and
 
faster
 
convergence.
  
Application  of  Transfer  Learning  in  NLP  1.  Pretrained  Models:  In  NLP,  transfer  learning  often  involves  using  pretrained  
models
 
that
 
have
 
been
 
trained
 
on
 
large
 
corpora.
 
For
 
example,
 
models
 
like
 
BERT,
 
GPT,
 
and
 
RoBERTa
 
are
 
trained
 
on
 
extensive
 
datasets
 
and
 
can
 
capture
 
rich
 
linguistic
 
features
 
and
 
contextual
 
relationshi

---

uistic
 
features
 
and
 
contextual
 
relationships.
 
These
 
models
 
can
 
then
 
be
 
ﬁnetuned
 
on
 
smaller,
 
speciﬁc
 
datasets
 
to
 
adapt
 
them
 
to
 
particular
 
tasks.
  2.  Fine-tuning:  Fine-tuning  involves  training  the  pre-trained  model  on  a  
smaller,
 
task-speciﬁc
 
dataset
 
while
 
retaining
 
the
 
knowledge
 
gained
 
during
 
the
 
pre-training
 
phase.
 
This
 
typically
 
involves
 
adjusting
 
the
 
ﬁnal
 
layers
 
of
 
the
 

MD  ARBAAZ  KHAN

---

ers
 
of
 
the
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   31 
 
model  to  align  with  the  speciﬁc  task,  such  as  sentiment  analysis,  named  
entity
 
recognition,
 
or
 
text
 
classiﬁcation.
  3.  Task  Adaptation:  Transfer  learning  in  NLP  allows  models  to  be  easily  adapted  
for
 
various
 
tasks
 
without
 
starting
 
from
 
scratch.
 
For
 
instance,
 
a
 
model
 
trained
 
for
 
sentiment
 
anal

---

ce,
 
a
 
model
 
trained
 
for
 
sentiment
 
analysis
 
can
 
be
 
ﬁnetuned
 
for
 
topic
 
classiﬁcation
 
with
 
minimal
 
additional
 
training.
  4.  Data  Efﬁciency:  Since  many  NLP  tasks  can  be  datahungry,  transfer  learning  
reduces
 
the
 
need
 
for
 
extensive
 
labeled
 
datasets.
 
By
 
utilizing
 
pretrained
 
models,
 
organizations
 
can
 
achieve
 
competitive
 
performance
 
even
 
with
 
limited
 
annotated
 
data.
  5.  Improved  Performance:  Transfer  learning  has

---

Improved  Performance:  Transfer  learning  has  led  to  signiﬁcant  
improvements
 
in
 
various
 
NLP
 
tasks,
 
allowing
 
models
 
to
 
achieve
 
state-of-the-art
 
results
 
in
 
areas
 
like
 
machine
 
translation,
 
text
 
summarization,
 
and
 
question
 
answering.
  
Examples  of 
Transfer  Learning  in  NLP ➔  BERT  (Bidirectional  Encoder  Representations  from  Transformers):  
Pretrained
 
on
 
vast
 
amounts
 
of
 
text,
 
BERT
 
can
 
be
 
ﬁnetuned
 
for
 
tasks
 
like
 
sent

---

can
 
be
 
ﬁnetuned
 
for
 
tasks
 
like
 
sentiment
 
analysis
 
or
 
named
 
entity
 
recognition.
  ➔  GPT  (Generative  Pretrained  Transformer):  Primarily  used  for  
text-generation
 
tasks,
 
GPT
 
models
 
can
 
also
 
be
 
ﬁnetuned
 
for
 
speciﬁc
 
applications
 
like
 
chatbots
 
or
 
summarization.
  ➔  T5  (TexttoText  Transfer  Transformer):  Treats  all  NLP  tasks  as  text-to-text  
problems,
 
allowing
 
a
 
single
 
model
 
to
 
be
 
ﬁnetuned
 
for
 
various
 
tasks
 
suc

---

to
 
be
 
ﬁnetuned
 
for
 
various
 
tasks
 
such
 
as
 
translation,
 
summarization,
 
and
 
classiﬁcation.
  In  summary,  transfer  learning  has  transformed  NLP  by  enabling  the  efﬁcient  use  of  
large
 
pretrained
 
models,
 
signiﬁcantly
 
improving
 
performance
 
across
 
a
 
wide
 
range
 
of
 
applications
 
while
 
requiring
 
less
 
task-speciﬁc
 
data.
     

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Question

---

Generative  AI  Interview  Questions   32 
 
Q10.  How  do  you  handle  out-of-vocabulary  (OOV)  words  in  NLP  models?   Handling  OutofVocabulary  (OOV)  Words  in  NLP  Models   Out-of-vocabulary  (OOV)  words  are  those  not  present  in  the  vocabulary  used  to  
train
 
an
 
NLP
 
model,
 
which
 
can
 
lead
 
to
 
challenges
 
during
 
inference.
 
Here
 
are
 
common
 
techniques
 
to
 
handle
 
OOV
 
words:
  1.  Subword  Tokenization:  ●  Breaking  words  into  sma

---

word  Tokenization:  ●  Breaking  words  into  smaller  units:  This  approach  involves  segmenting  words  
into
 
smaller
 
components
 
like
 
preﬁxes,
 
sufﬁxes,
 
and
 
stems.
 ●  Handling  OOV  words:  It  enables  the  model  to  manage  OOV  words  by  
representing
 
them
 
as
 
combinations
 
of
 
known
 
subwords.
  2.  Character-Level  Models:  ●  Processing  text  at  the  character  level:  These  models  analyze  text  character  
by
 
character
 
instead
 
of
 
word
 
by
 
word.

---

by
 
character
 
instead
 
of
 
word
 
by
 
word.
 ●  Handling  OOV  words:  This  method  effectively  addresses  OOV  words,  as  it  
does
 
not
 
depend
 
on
 
a
 
ﬁxed
 
vocabulary.
  3.  Special  Token:  ●  Assigning  a  unique  token:  A  special  token,  such  as  `<UNK>`,  is  designated  for  
OOV
 
words.
 ●  Training  the  model  to  recognize  this  token:  The  model  learns  to  treat  this  
token
 
as
 
a
 
placeholder
 
for
 
unknown
 
words.
  4.  Word  Embeddings:  ●   Seman

---

nknown
 
words.
  4.  Word  Embeddings:  ●   Semantic  similarity:  Word  embeddings  represent  words  as  dense  vectors  in  a  
high-dimensional
 
space.
 ●  Handling  OOV  words:  Techniques  like  subword  or  character-based  
embeddings
 
can
 
be
 
utilized
 
to
 
generate
 
embeddings
 
for
 
OOV
 
words
 
based
 
on
 
their
 
similarity
 
to
 
known
 
words.
  5.  Vocabulary  Expansion:  ●  Dynamically  adding  new  words:  New  words  can  be  incorporated  into  the  
vocabulary
 
d

---

can  be  incorporated  into  the  
vocabulary
 
during
 
training
 
or
 
inference.
 ●  Handling  OOV  words:  This  can  help  decrease  the  number  of  OOV  words,  
although
 
it
 
may
 
require
 
retraining
 
the
 
model.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   33 
 
 6.  Contextual  Embeddings:  ●  Contextual  understanding:  Models  like  BERT  produce  contextual  embeddings  
that
 
capture
 
word
 
meani

---

extual  embeddings  
that
 
capture
 
word
 
meanings
 
based
 
on
 
context.
 ●  Handling  OOV  words:  These  models  often  manage  OOV  words  more  
effectively
 
than
 
traditional
 
word
 
embeddings.
  The  best  method  for  handling  OOV  words  varies  based  on  the  speciﬁc  NLP  task  and  
data
 
characteristics.
 
Experimenting
 
with
 
different
 
techniques
 
is
 
often
 
recommended
 
to
 
ﬁnd
 
the
 
most
 
effective
 
solution
 
for
 
a
 
given
 
application.
  
Q11.  Explai

---

n
 
for
 
a
 
given
 
application.
  
Q11.  Explain  the  concept  of  attention  mechanisms  and  their  role  in  
sequence-to-sequence
 
tasks. In  the  context  of  attention  mechanisms,  particularly  within  transformer  
architectures,
 
the
 
terms
 
Q
 
(Query),
 
K
 
(Key),
 
and
 
V
 
(Value)
 
refer
 
to
 
the
 
three
 
fundamental
 
components
 
used
 
to
 
compute
 
attention
 
scores.
 
Here's
 
a
 
breakdown
 
of
 
each:
  1.  Query  (Q)  ➢  Deﬁnition:  The  query  represents  t

---

ery  (Q)  ➢  Deﬁnition:  The  query  represents  the  current  input  or  state  for  which  we  
want
 
to
 
ﬁnd
 
relevant
 
information
 
from
 
the
 
input
 
sequence.
 
 ➢  Role:  Each  query  is  used  to  compare  against  the  keys  to  determine  the  
attention
 
weights,
 
which
 
dictate
 
how
 
much
 
focus
 
should
 
be
 
placed
 
on
 
different
 
parts
 
of
 
the
 
input.
  2.  Key  (K)  ➢  Deﬁnition:  Keys  are  representations  of  the  input  elements,  essentially  serving  
a

---

f  the  input  elements,  essentially  serving  
as
 
the
 
identiﬁers
 
for
 
each
 
element
 
in
 
the
 
input
 
sequence.
 ➢  Role:  The  keys  are  compared  with  the  queries  to  calculate  the  attention  
scores.
 
Each
 
input
 
element
 
has
 
a
 
corresponding
 
key
 
that
 
indicates
 
its
 
signiﬁcance
 
when
 
matched
 
with
 
the
 
query.
  3.  Value  (V)  ➢  Deﬁnition:  Values  are  the  actual  content  or  information  associated  with  each  
input
 
element
 
that
 
will
 
b

---

ed  with  each  
input
 
element
 
that
 
will
 
be
 
weighted
 
and
 
used
 
to
 
compute
 
the
 
output
 
of
 
the
 
attention
 
mechanism.
 ➢  Role:  Once  the  attention  scores  are  computed  (based  on  the  comparison  
between
 
queries
 
and
 
keys),
 
these
 
scores
 
are
 
used
 
to
 
weight
 
the
 
values.
 
The
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   34 
 
weighted  sum  of  the  values  produces  the

---

4 
 
weighted  sum  of  the  values  produces  the  ﬁnal  output  of  the  attention  
mechanism.
  
Attention  Mechanism  Process 1.  Input  Representation:  Each  input  element  in  the  sequence  is  transformed  into  a  
query,
 
key,
 
and
 
value
 
using
 
learned
 
linear
 
transformations
 
(typically
 
achieved
 
through
 
weight
 
matrices).
    2.  Score  Calculation:  The  attention  scores  are  computed  by  taking  the  dot  product  
of
 
the
 
query
 
vector
 
with
 
all
 
the

---

uct  
of
 
the
 
query
 
vector
 
with
 
all
 
the
 
key
 
vectors,
 
followed
 
by
 
a
 
softmax
 
function
 
to
 
normalize
 
the
 
scores.
 
This
 
determines
 
how
 
much
 
focus
 
should
 
be
 
placed
 
on
 
each
 
input
 
element.
  
       where   is  the  dimension  of  the  keys.   3.  Weighted  Sum:  The  attention  scores  are  used  to  create  a  weighted  sum  of  the  
value
 
vectors:
      In  summary,  the  Q,  K,  and  V  components  are  essential  for  the  functioning  of

---

nents  are  essential  for  the  functioning  of  the  
attention
 
mechanism
 
in
 
transformer
 
models.
 
They
 
allow
 
the
 
model
 
to
 
selectively
 
focus
 
on
 
relevant
 
parts
 
of
 
the
 
input,
 
facilitating
 
effective
 
processing
 
of
 
sequences
 
and
 
improving
 
performance
 
in
 
tasks
 
such
 
as
 
machine
 
translation,
 
summarization,
 
and
 
text
 
classiﬁcation.
  
Q12.  What  is  a  language  model,  and  how  is  it  evaluated? A  language  model  is  a  statistical

---

evaluated? A  language  model  is  a  statistical  or  neural  network-based  model  that  is  trained  to  
understand
 
and
 
generate
 
human
 
language.
 
It
 
learns
 
the
 
probability
 
distribution
 
of
 
words
 
in
 
a
 
given
 
language
 
based
 
on
 
a
 
corpus
 
of
 
text
 
data,
 
enabling
 
it
 
to
 
predict
 
the
 
next
 
word
 
in
 
a
 
sentence
 
or
 
generate
 
coherent
 
text.
  
Types  of  Language  Models 1.  Statistical  Language  Models:  These  models  use  statistical

---

nguage  Models:  These  models  use  statistical  techniques  to  predict  
the
 
next
 
word
 
based
 
on
 
the
 
previous
 
words.
 
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   35 
 
Examples  include  n-gram  models,  which  consider  a  ﬁxed  number  of  preceding  
words
 
to
 
predict
 
the
 
next
 
one.
  2.  Neural  Language  Models:  These  models  use  neural  networks  to  capture  more  
complex
 
relations

---

networks  to  capture  more  
complex
 
relationships
 
in
 
the
 
data.
 
 Examples  include  recurrent  neural  networks  (RNNs),  long  short-term  memory  
networks
 
(LSTMs),
 
and
 
transformer-based
 
models
 
like
 
BERT
 
and
 
GPT.
  
Evaluation  of  Language  Models Evaluating  language  models  involves  measuring  their  performance  on  various  tasks.  
Common
 
evaluation
 
methods
 
include:
 1.  Perplexity:  ●  Perplexity  is  a  measure  of  how  well  a  probability  model

---

a  measure  of  how  well  a  probability  model  predicts  a  sample.   ●  It  quantiﬁes  the  uncertainty  of  the  model  in  predicting  the  next  word.   ●  Lower  perplexity  indicates  better  performance.  Mathematically,  for  a  sequence  of  words  ,  the  perplexity   is  deﬁned  as:        2.  Accuracy:  ●  For  tasks  like  text  classiﬁcation  or  question  answering,  accuracy  measures  
the
 
percentage
 
of
 
correctly
 
predicted
 
labels
 
or
 
answers.
  3.  BLEU  Score:

---

cted
 
labels
 
or
 
answers.
  3.  BLEU  Score:  ●  The  Bilingual  Evaluation  Understudy  (BLEU)  score  is  commonly  used  for  
evaluating
 
machine
 
translation
 
models.
 
It
 
measures
 
the
 
overlap
 
between
 
the
 
n-grams
 
of
 
the
 
generated
 
text
 
and
 
a
 
reference
 
text,
 
indicating
 
the
 
quality
 
of
 
generated
 
translations.
  4.  ROUGE  Score:  ●  ROUGE  (RecallOriented  Understudy  for  Gisting  Evaluation)  is  used  to  
evaluate
 
summarization
 
models.
 
It

---

used  to  
evaluate
 
summarization
 
models.
 
It
 
measures
 
the
 
overlap
 
of
 
n-grams
 
between
 
the
 
generated
 
summary
 
and
 
reference
 
summaries.
  5.  F1  Score:  ●  The  F1  score  combines  precision  and  recall  into  a  single  metric,  especially  
useful
 
for
 
tasks
 
like
 
named
 
entity
 
recognition
 
and
 
classiﬁcation
.
  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   36 
 
6.  Human  Evalu

---

AI  Interview  Questions   36 
 
6.  Human  Evaluation:  ●  Human  judges  may  evaluate  the  ﬂuency,  coherence,  and  relevance  of  the  
text
 
generated
 
by
 
the
 
language
 
model,
 
providing
 
qualitative
 
feedback.
 In  summary,  language  models  are  essential  tools  in  natural  language  processing,  
enabling
 
machines
 
to
 
understand
 
and
 
generate
 
human-like
 
text.
 
Their
 
evaluation
 
is
 
crucial
 
for
 
assessing
 
their
 
performance
 
and
 
suitability
 
for

---

g
 
their
 
performance
 
and
 
suitability
 
for
 
various
 
applications,
 
such
 
as
 
translation,
 
summarization,
 
and
 
dialogue
 
systems.
 
The
 
choice
 
of
 
evaluation
 
method
 
often
 
depends
 
on
 
the
 
speciﬁc
 
task
 
and
 
the
 
goals
 
of
 
the
 
application.
                                

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   37 
 
                 
 Transformer  and  Its  Extended  Archite

---

Transformer  and  Its  Extended  Architecture                  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   38 
 
Q1.  Describe  the  concept  of  learning  rate  scheduling  and  its  role  in  
optimizing
 
the
 
training
 
process
 
of
 
generative
 
models
 
over
 
time. Learning  Rate  Scheduling  is  a  technique  used  to  dynamically  adjust  the  learning  
rate
 
during
 
the
 
training
 
process
 
of

---

ng  
rate
 
during
 
the
 
training
 
process
 
of
 
neural
 
networks.
 
 This  technique  is  crucial  for  optimizing  the  training  of  generative  models,  such  as  
Generative
 
Adversarial
 
Networks
 
(GANs)
 
and
 
Variational
 
Autoencoders
 
(VAEs).
  
Why  is  Learning  Rate  Scheduling  Important? ➢  Avoiding  Oscillations:  A  high  learning  rate  can  lead  to  oscillations,  where  the  
model's
 
parameters
 
overshoot
 
optimal
 
values,
 
hindering
 
convergence.
  ➢  Preve

---

al
 
values,
 
hindering
 
convergence.
  ➢  Preventing  Overﬁtting:  A  decreasing  learning  rate  can  help  prevent  
overﬁtting,
 
where
 
the
 
model
 
becomes
 
too
 
specialized
 
to
 
the
 
training
 
data
 
and
 
performs
 
poorly
 
on
 
unseen
 
data.
  ➢  Accelerating  Convergence:  A  well-tuned  learning  rate  schedule  can  
signiﬁcantly
 
speed
 
up
 
the
 
convergence
 
of
 
the
 
model
 
to
 
the
 
optimal
 
solution.
  
Common  Learning  Rate  Scheduling  Techniques: Step  De

---

Learning  Rate  Scheduling  Techniques: Step  Decay:  The  learning  rate  is  reduced  by  a  ﬁxed  factor  at  speciﬁc  intervals,  
effective
 
for
 
models
 
that
 
beneﬁt
 
from
 
a
 
gradual
 
decrease.
  Exponential  Decay:  The  learning  rate  decreases  exponentially  over  time,  ensuring  
smooth
 
convergence,
 
commonly
 
used
 
in
 
deep
 
learning
 
models.
  Cosine  Annealing:  The  learning  rate  oscillates  cyclically  between  a  maximum  and  
minimum
 
value,
 
which
 
c

---

en  a  maximum  and  
minimum
 
value,
 
which
 
can
 
lead
 
to
 
faster
 
convergence
 
and
 
better
 
performance.
  Cyclic  Learning  Rate:  The  learning  rate  is  cyclically  increased  and  decreased  over  
time,
 
helping
 
to
 
escape
 
local
 
minima
 
and
 
ﬁnd
 
global
 
optima.
  Adaptive  Learning  Rate  Methods:  Algorithms  like  Adam  and  RMSprop  
automatically
 
adjust
 
the
 
learning
 
rate
 
for
 
each
 
parameter,
 
which
 
can
 
be
 
combined
 
with
 
learning
 
rate

---

h
 
can
 
be
 
combined
 
with
 
learning
 
rate
 
scheduling
 
for
 
further
 
optimization.
    

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   39 
 
Speciﬁc  Considerations  for  Generative  Models:  ❖  GANs:  GANs  are  sensitive  to  learning  rate  settings,  requiring  a  careful  
balance
 
between
 
the
 
generator
 
and
 
discriminator's
 
learning
 
rates.
 
Cyclic
 
learning
 
rate
 
schedules
 
can
 
be
 
parti

---

c
 
learning
 
rate
 
schedules
 
can
 
be
 
particularly
 
effective
 
for
 
GANs.
  ❖  VAEs:  VAEs  often  beneﬁt  from  a  gradual  decrease  in  the  learning  rate,  making  
exponential
 
decay
 
or
 
cosine
 
annealing
 
suitable
 
choices.
  By  carefully  selecting  and  tuning  a  learning  rate  schedule,  we  can  signiﬁcantly  
improve
 
the
 
training
 
process
 
of
 
generative
 
models.
 
It
 
is
 
important
 
to
 
experiment
 
with
 
different
 
techniques
 
and
 
hyperparameter

---

th
 
different
 
techniques
 
and
 
hyperparameters
 
to
 
ﬁnd
 
the
 
optimal
 
conﬁguration
 
for
 
speciﬁc
 
models
 
and
 
datasets.
  
Q2.  Discuss  the  concept  of  transfer  learning  in  the  context  of  natural  
language
 
processing.
 
How
 
do
 
pre-trained
 
language
 
models
 
contribute
 
to
 
various
 
NLP
 
tasks?
 Transfer  learning  is  a  powerful  approach  in  natural  language  processing  (NLP)  that  
allows
 
models
 
to
 
leverage
 
knowledge
 
gained
 
from
 
one

---

to
 
leverage
 
knowledge
 
gained
 
from
 
one
 
task
 
to
 
improve
 
performance
 
on
 
another,
 
often
 
related,
 
task.
 
 This  method  has  become  increasingly  popular  with  the  advent  of  pretrained  
language
 
models,
 
which
 
serve
 
as
 
foundational
 
models
 
that
 
can
 
be
 
ﬁnetuned
 
for
 
speciﬁc
 
applications.
  
Concept  of  Transfer  Learning  in  NLP Pre-training  and  Finetuning:  Transfer  learning  typically  involves  two  main  phases:  ●  Pretraining:  A  mo

---

volves  two  main  phases:  ●  Pretraining:  A  model  is  trained  on  a  large  corpus  of  text  to  learn  language  
representations.
 
During
 
this
 
phase,
 
the
 
model
 
learns
 
to
 
capture
 
syntactic
 
and
 
semantic
 
properties
 
of
 
language
 
without
 
being
 
explicitly
 
told
 
what
 
tasks
 
it
 
is
 
preparing
 
for.
 ●  Fine-tuning:  After  pretraining,  the  model  is  ﬁnetuned  on  a  smaller,  
task-speciﬁc
 
dataset.
 
This
 
phase
 
adapts
 
the
 
general
 
language

---

This
 
phase
 
adapts
 
the
 
general
 
language
 
knowledge
 
to
 
the
 
particular
 
nuances
 
of
 
the
 
target
 
task.
   Beneﬁts:  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   40 
 
➔  Reduced  Training  Time  and  Data:  Since  the  model  starts  with  pre-existing  
knowledge,
 
ﬁne-tuning
 
on
 
speciﬁc
 
tasks
 
requires
 
signiﬁcantly
 
less
 
data
 
and
 
time
 
compared
 
to
 
training
 
a
 
model
 
from

---

e
 
compared
 
to
 
training
 
a
 
model
 
from
 
scratch.
 ➔  Improved  Performance:  Pretrained  models  often  achieve  higher  accuracy  
and
 
performance
 
on
 
NLP
 
tasks,
 
as
 
they
 
leverage
 
learned
 
representations
 
that
 
encapsulate
 
extensive
 
language
 
features.
  
Contribution  of  Pre-trained  Language  Models  to  NLP  Tasks 1.  Versatility  Across  Tasks:  Pretrained  models  like  BERT,  GPT,  and  RoBERTa  can  be  
adapted
 
to
 
various
 
NLP
 
tasks,
 
including:

---

adapted
 
to
 
various
 
NLP
 
tasks,
 
including:
 ●  Text  Classiﬁcation:  Categorizing  text  into  predeﬁned  labels  (e.g.,  sentiment  
analysis).
 ●  Named  Entity  Recognition  (NER):  Identifying  and  classifying  key  entities  in  
text
 
(e.g.,
 
names,
 
dates).
 ●  Question  Answering:  Extracting  answers  from  a  text  based  on  a  query.  ●  Text  Summarization:  Generating  concise  summaries  of  larger  text  
documents.
  2.  Contextual  Understanding:  Pre-trained  model

---

2.  Contextual  Understanding:  Pre-trained  models  capture  context-sensitive  
meanings
 
of
 
words
 
and
 
phrases,
 
thanks
 
to
 
their
 
architecture.
 
 For  instance,  models  like  BERT  use  bidirectional  context,  meaning  they  consider  
the
 
entire
 
context
 
of
 
a
 
word
 
within
 
a
 
sentence,
 
resulting
 
in
 
better
 
understanding
 
and
 
representation
 
of
 
language.
  3.  Transferability:  The  knowledge  encoded  in  these  models  is  often  transferable  
across

---

n  these  models  is  often  transferable  
across
 
domains
 
and
 
tasks.
 
For
 
instance,
 
a
 
model
 
trained
 
on
 
news
 
articles
 
may
 
perform
 
well
 
on
 
scientiﬁc
 
texts
 
when
 
ﬁne-tuned,
 
demonstrating
 
its
 
ability
 
to
 
generalize.
  4.  Layer  Adaptation:  Users  can  choose  to  ﬁnetune  all  or  some  of  the  layers  in  a  
pretrained
 
model,
 
allowing
 
for
 
ﬂexibility
 
based
 
on
 
the
 
speciﬁc
 
task
 
and
 
available
 
data.
 
This
 
adaptability
 
makes

---

available
 
data.
 
This
 
adaptability
 
makes
 
it
 
easier
 
to
 
integrate
 
pretrained
 
models
 
into
 
existing
 
workﬂows.
  In  summary,  transfer  learning  through  pretrained  language  models  has  
revolutionized
 
NLP
 
by
 
providing
 
robust,
 
versatile
 
tools
 
that
 
signiﬁcantly
 
enhance
 
performance
 
across
 
a
 
wide
 
range
 
of
 
tasks.
 
By
 
leveraging
 
the
 
extensive
 
language
 
knowledge
 
embedded
 
in
 
these
 
models,
 
practitioners
 
can
 
achieve
 
stat

---

e
 
models,
 
practitioners
 
can
 
achieve
 
state-of-the-art
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   41 
 
results  with  relatively  less  data  and  computational  resources,  making  it  an  essential  
technique
 
in
 
modern
 
NLP
 
applications.
  
Q3.  Highlight  the  key  differences  between  models  like  GPT  (Generative  
Pre-trained
 
Transformer)
 
and
 
BERT
 
(Bidirectional
 
Encoder
 
Representat

---

nd
 
BERT
 
(Bidirectional
 
Encoder
 
Representations
 
from
 
Transformers)? Here  are  the  key  differences  between  GPT  (Generative  Pre-trained  Transformer)  
and
 
BERT
 
(Bidirectional
 
Encoder
 
Representations
 
from
 
Transformers):
   GPT  (Generative  Pre-trained  Transformer)  
BERT  (Bidirectional  Encoder  Representations  from  Transformers)  
1.  Architecture  Type  GPT  is  a  unidirectional  (or  causal)  transformer  model.  It  processes  text  in  a  left-to-right  man

---

l.  It  processes  text  in  a  left-to-right  manner,  meaning  it  predicts  the  next  word  in  a  sequence  based  solely  on  the  previous  words.  
BERT  is  a  bidirectional  transformer  model.  It  considers  the  context  from  both  the  left  and  right  sides  of  a  word  during  training,  allowing  it  to  understand  the  word's  meaning  based  on  the  entire  sentence.  
2.  Training  Objective  GPT  is  trained  using  a  language  modeling  objective,  speciﬁcally  next-t

---

anguage  modeling  objective,  speciﬁcally  next-token  prediction .  The  model  learns  to  predict  the  next  word  in  a  sentence  given  the  previous  words.  
BERT  is  trained  using  two  primary  objectives:  masked  language  modeling  (MLM)  and  next  sentence  prediction  (NSP).    In  MLM,  some  words  in  the  input  are  masked,  and  the  model  learns  to  predict  them  based  on  their  context.    NSP  involves  predicting  whether  two  sentences  follow  each  other  i

---

g  whether  two  sentences  follow  each  other  in  the  original  text.  
3.  Use  Cases  GPT  is  primarily  used  for  generative  tasks ,  such  as  text  generation,  dialogue  systems,  and  creative  writing.   Its  unidirectional  nature  makes  it  well-suited  for  
BERT  is  mainly  used  for  understanding  tasks ,  such  as  text  classiﬁcation,  named  entity  recognition,  and  question  answering.  Its  bidirectional  context  allows  it  to  excel  at  understanding  and  inter

---

lows  it  to  excel  at  understanding  and  interpreting  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   42 
 
generating  coherent  text  based  on  a  given  prompt.  
text.  
4.  Input  Representation  
GPT  uses  positional  embeddings  to  maintain  the  order  of  words,  as  it  only  looks  at  previous  words  in  the  sequence.  
BERT  incorporates  both  positional  embeddings  and  segment  embeddings ,  allow

---

nal  embeddings  and  segment  embeddings ,  allowing  it  to  handle  tasks  involving  pairs  of  sentences  (e.g.,  question  answering).  
5.  Pre-training  and  Fine-tuning  
GPT  is  pre-trained  on  large  text  corpora  and  can  be  ﬁne-tuned  on  speciﬁc  tasks,  usually  requiring  task-speciﬁc  data  for  optimal  performance.  
Similar  to  GPT,  BERT  is  pre-trained  on  extensive  text  datasets  but  is  often  ﬁne-tuned  for  speciﬁc  tasks  with  smaller  datasets  to  adapt

---

eciﬁc  tasks  with  smaller  datasets  to  adapt  its  understanding  for  various  NLP  applications.   In  summary,  the  key  differences  between  GPT  and  BERT  lie  in  their  architecture,  training  
objectives,
 
primary
 
use
 
cases,
 
input
 
representations,
 
and
 
how
 
they
 
are
 
pre-trained
 
and
 
ﬁne-tuned.
 
While
 
GPT
 
excels
 
in
 
generating
 
text,
 
BERT
 
is
 
designed
 
for
 
understanding
 
and
 
interpreting
 
language,
 
making
 
them
 
suitable
 
for
 
differe

---

nguage,
 
making
 
them
 
suitable
 
for
 
different
 
NLP
 
tasks.
  
Q4.  What  problems  of  RNNs  do  transformer  models  solve?  Transformer  models  were  designed  to  address  several  key  issues  associated  with  
Recurrent
 
Neural
 
Networks
 
(RNNs)
 
and
 
their
 
variants.
 
Here
 
are
 
the
 
main
 
problems
 
that
 
transformers
 
solve:
 1.  Long-Range  Dependencies  ●  RNN  Problem :  RNNs  struggle  to  capture  long-range  dependencies  in  sequences  due  
to
 
issues
 
s

---

dependencies  in  sequences  due  
to
 
issues
 
such
 
as
 
vanishing
 
and
 
exploding
 
gradients.
 
As
 
the
 
sequence
 
length
 
increases,
 
RNNs
 
may
 
have
 
difﬁculty
 
retaining
 
information
 
from
 
earlier
 
inputs.
 ●  Transformer  Solution :  Transformers  use  self-attention  mechanisms  that  allow  
them
 
to
 
directly
 
connect
 
any
 
two
 
positions
 
in
 
the
 
input
 
sequence,
 
regardless
 
of
 
their
 
distance.
 
This
 
enables
 
transformers
 
to
 
effectively
 
mo

---

s
 
enables
 
transformers
 
to
 
effectively
 
model
 
relationships
 
between
 
words
 
that
 
are
 
far
 
apart
 
in
 
the
 
input.
 2.  Parallelization  ●  RNN  Problem :  RNNs  process  sequences  one  time  step  at  a  time,  which  limits  their  
ability
 
to
 
leverage
 
parallel
 
processing
 
during
 
training.
 
This
 
sequential
 
nature
 
makes
 
RNNs
 
slower
 
to
 
train,
 
especially
 
on
 
long
 
sequences.
 ●  Transformer  Solution :  Transformers  process  all  tokens  in  t

---

ution :  Transformers  process  all  tokens  in  the  input  sequence  
simultaneously
 
(in
 
parallel)
 
during
 
training,
 
thanks
 
to
 
their
 
architecture.
 
This
 
signiﬁcantly
 
speeds
 
up
 
training
 
and
 
allows
 
for
 
better
 
utilization
 
of
 
modern
 
hardware,
 
like
 
GPUs.
  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   43 
 
3.  Scalability  ●  RNN  Problem :  As  the  size  of  datasets  and  model

---

Problem :  As  the  size  of  datasets  and  model  architectures  increase,  RNNs  
become
 
difﬁcult
 
to
 
scale
 
effectively.
 
Their
 
sequential
 
processing
 
can
 
become
 
a
 
bottleneck.
 ●  Transformer  Solution :  Transformers  can  easily  scale  to  larger  datasets  and  deeper  
architectures
 
because
 
of
 
their
 
parallel
 
processing
 
capabilities.
 
They
 
are
 
more
 
efﬁcient
 
for
 
large-scale
 
training.
 4.  Flexibility  with  Input  Lengths  ●  RNN  Problem :  Whil

---

ity  with  Input  Lengths  ●  RNN  Problem :  While  RNNs  can  handle  variable-length  sequences,  they  may  require  
careful
 
management
 
of
 
padding
 
and
 
masking,
 
especially
 
with
 
sequences
 
of
 
varying
 
lengths.
 ●  Transformer  Solution :  Transformers  naturally  handle  variable-length  inputs  and  
can
 
process
 
sequences
 
of
 
different
 
lengths
 
without
 
requiring
 
extensive
 
preprocessing,
 
as
 
their
 
attention
 
mechanism
 
can
 
focus
 
on
 
relevant
 
p

---

tion
 
mechanism
 
can
 
focus
 
on
 
relevant
 
parts
 
of
 
the
 
input
 
dynamically.
 5.  Memory  Constraints  ●  RNN  Problem :  Traditional  RNNs  have  ﬁxed  memory  capacity  for  their  hidden  states,  
which
 
can
 
limit
 
their
 
ability
 
to
 
retain
 
information
 
from
 
long
 
sequences.
 ●  Transformer  Solution :  Transformers  do  not  have  a  ﬁxed  memory  state;  instead,  
they
 
use
 
self-attention
 
to
 
consider
 
all
 
positions
 
in
 
the
 
input,
 
allowing
 
them

---

positions
 
in
 
the
 
input,
 
allowing
 
them
 
to
 
effectively
 
aggregate
 
information
 
without
 
the
 
constraints
 
of
 
memory
 
limitations.
  6.  Complexity  in  Sequence  Modeling  ●  RNN  Problem :  RNNs  require  complex  architectures,  such  as  Long  Short-Term  
Memory
 
(LSTM)
 
or
 
Gated
 
Recurrent
 
Units
 
(GRU),
 
to
 
handle
 
the
 
limitations
 
of
 
basic
 
RNNs.
 
This
 
complexity
 
can
 
complicate
 
model
 
training
 
and
 
tuning.
 ●  Transformer  Solution :

---

ning
 
and
 
tuning.
 ●  Transformer  Solution :  The  transformer  architecture  is  relatively  straightforward,  
using
 
a
 
series
 
of
 
stacked
 
layers
 
of
 
self-attention
 
and
 
feed-forward
 
networks.
 
This
 
simplicity
 
leads
 
to
 
more
 
effective
 
and
 
interpretable
 
models.
 
In  summary,  transformers  address  the  limitations  of  RNNs  by  providing  mechanisms  to  
efﬁciently
 
model
 
long-range
 
dependencies,
 
allowing
 
parallel
 
processing,
 
and
 
scaling

---

lowing
 
parallel
 
processing,
 
and
 
scaling
 
effectively
 
for
 
larger
 
datasets.
 
Their
 
architecture
 
has
 
led
 
to
 
signiﬁcant
 
advancements
 
in
 
natural
 
language
 
processing
 
and
 
other
 
sequence-based
 
tasks,
 
making
 
them
 
the
 
dominant
 
choice
 
in
 
many
 
applications
 
today.
 
Q5.  How  is  the  transformer  different  from  RNN  and  LSTM? Transformers  differ  from  Recurrent  Neural  Networks  (RNNs)  and  Long  Short-Term  
Memory
 
(LSTM)
 
networks
 
i

---

Long  Short-Term  
Memory
 
(LSTM)
 
networks
 
in
 
several
 
fundamental
 
ways.
 
Here
 
are
 
the
 
key
 
differences:
 1.  Architecture  ●  RNN :  RNNs  process  sequences  of  data  one  time  step  at  a  time.  They  maintain  a  
hidden
 
state
 
that
 
is
 
updated
 
with
 
each
 
input,
 
capturing
 
temporal
 
dependencies
 
in
 
the
 
sequence.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   44 
 
●  LSTM :

---

tive  AI  Interview  Questions   44 
 
●  LSTM :  LSTMs  are  a  type  of  RNN  that  includes  additional  gates  (input,  output,  and  
forget
 
gates)
 
to
 
control
 
the
 
ﬂow
 
of
 
information
 
and
 
better
 
handle
 
long-range
 
dependencies.
 
This
 
helps
 
mitigate
 
the
 
vanishing
 
gradient
 
problem
 
often
 
encountered
 
in
 
standard
 
RNNs.
 ●  Transformer :  Transformers  use  a  self-attention  mechanism  to  process  the  entire  
sequence
 
simultaneously.
 
They
 
do

---

entire  
sequence
 
simultaneously.
 
They
 
do
 
not
 
have
 
recurrent
 
connections,
 
and
 
instead,
 
they
 
rely
 
on
 
attention
 
scores
 
to
 
weigh
 
the
 
importance
 
of
 
different
 
input
 
tokens
 
relative
 
to
 
each
 
other.
 2.  Handling  of  Long-Range  Dependencies  ●  RNN :  RNNs  struggle  with  long-range  dependencies  due  to  the  vanishing  gradient  
problem,
 
where
 
gradients
 
become
 
too
 
small
 
to
 
propagate
 
effectively
 
through
 
many
 
layers.
 ●  LST

---

e
 
effectively
 
through
 
many
 
layers.
 ●  LSTM :  LSTMs  are  designed  to  better  capture  long-range  dependencies  through  
their
 
gating
 
mechanisms,
 
which
 
help
 
maintain
 
information
 
over
 
longer
 
sequences.
 ●  Transformer :  Transformers  excel  at  modeling  long-range  dependencies  because  
their
 
self-attention
 
mechanism
 
can
 
relate
 
all
 
tokens
 
in
 
the
 
sequence
 
regardless
 
of
 
their
 
distance,
 
allowing
 
for
 
direct
 
access
 
to
 
any
 
part

---

llowing
 
for
 
direct
 
access
 
to
 
any
 
part
 
of
 
the
 
input.
 3.  Processing  Speed  and  Parallelization  ●  RNN :  RNNs  process  sequences  sequentially,  which  can  lead  to  longer  training  times  
as
 
they
 
cannot
 
leverage
 
parallel
 
processing
 
effectively.
 ●  LSTM :  Like  standard  RNNs,  LSTMs  also  process  sequences  one  step  at  a  time,  
maintaining
 
their
 
sequential
 
nature
 
and
 
thus
 
not
 
beneﬁtting
 
from
 
parallelization.
 ●  Transformer :  Tra

---

g
 
from
 
parallelization.
 ●  Transformer :  Transformers  allow  for  parallel  processing  since  they  can  compute  
attention
 
scores
 
for
 
all
 
tokens
 
at
 
once.
 
This
 
signiﬁcantly
 
speeds
 
up
 
training
 
and
 
makes
 
them
 
more
 
efﬁcient
 
for
 
handling
 
large
 
datasets.
 4.  Input  Representation  ●  RNN :  RNNs  typically  require  ﬁxed-length  input  sequences,  which  may  necessitate  
padding
 
for
 
variable-length
 
sequences.
 ●  LSTM :  LSTMs  also  process

---

th
 
sequences.
 ●  LSTM :  LSTMs  also  process  sequences  of  varying  lengths  but  have  similar  constraints  
as
 
RNNs
 
regarding
 
padding
 
and
 
masking.
 ●  Transformer :  Transformers  inherently  handle  variable-length  inputs  due  to  their  
attention
 
mechanism,
 
which
 
evaluates
 
all
 
tokens
 
simultaneously
 
without
 
requiring
 
ﬁxed-length
 
sequences.
 
5.  Memory  and  State  Management  ●  RNN :  RNNs  maintain  a  single  hidden  state  for  each  time  step,  w

---

single  hidden  state  for  each  time  step,  which  can  limit  their  
ability
 
to
 
retain
 
information
 
from
 
long
 
sequences.
 ●  LSTM :  LSTMs  maintain  both  a  hidden  state  and  a  cell  state,  enabling  them  to  store  
and
 
manage
 
information
 
over
 
longer
 
periods
 
more
 
effectively
 
than
 
standard
 
RNNs.
 ●  Transformer :  Transformers  do  not  rely  on  hidden  states.  Instead,  they  use  
self-attention
 
to
 
compute
 
representations
 
that
 
consider

---

to
 
compute
 
representations
 
that
 
consider
 
all
 
tokens
 
in
 
the
 
sequence
 
simultaneously,
 
allowing
 
for
 
a
 
more
 
ﬂexible
 
and
 
dynamic
 
approach
 
to
 
memory.
   

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   45 
 
6.  Applications  ●  RNN  and  LSTM :  Commonly  used  in  tasks  such  as  language  modeling,  sequence  
generation,
 
and
 
time
 
series
 
forecasting,
 
where
 
sequential
 
data

---

eries
 
forecasting,
 
where
 
sequential
 
data
 
is
 
crucial.
 ●  Transformer :  Widely  used  in  natural  language  processing  (NLP)  tasks  such  as  
machine
 
translation,
 
text
 
classiﬁcation,
 
summarization,
 
and
 
question
 
answering.
 
Their
 
ability
 
to
 
handle
 
long-range
 
dependencies
 
and
 
parallelize
 
training
 
makes
 
them
 
particularly
 
suited
 
for
 
large-scale
 
NLP
 
tasks.
 
Q6.  How  does  BERT  work,  and  what  makes  it  different  from  previous  NLP

---

d  what  makes  it  different  from  previous  NLP  
models? BERT  (Bidirectional  Encoder  Representations  from  Transformers)  is  a  transformer-based  
model
 
designed
 
to
 
improve
 
the
 
understanding
 
of
 
the
 
context
 
of
 
words
 
in
 
natural
 
language
 
processing
 
(NLP).
 
 
 
Here's  how  BERT  works:  ●  Architecture:  ○  BERT  is  based  on  the  transformer  architecture,  speciﬁcally  the  encoder  part  
of
 
the
 
transformer.
 
It
 
utilizes
 
multiple
 
layers
 
of

---

sformer.
 
It
 
utilizes
 
multiple
 
layers
 
of
 
self-attention
 
mechanisms
 
to
 
process
 
and
 
represent
 
the
 
input
 
text.
  ●  Bidirectional  Contextualization:  ○  Unlike  traditional  models  that  read  text  sequentially  (left  to  right  or  right  to  
left),
 
BERT
 
reads
 
the
 
entire
 
sequence
 
of
 
words
 
simultaneously.
 
This
 
bidirectional
 
approach
 
allows
 
BERT
 
to
 
consider
 
the
 
context
 
of
 
a
 
word
 
based
 
on
 
all
 
surrounding
 
words,
 
result

---

based
 
on
 
all
 
surrounding
 
words,
 
resulting
 
in
 
a
 
more
 
nuanced
 
understanding
 
of
 
language.
  ●  Pre-training  and  Finetuning:  ○  BERT  undergoes  two  main  phases:  ■  Pretraining:  During  this  phase,  BERT  is  trained  on  a  large  corpus  of  
text
 
using
 
two
 
objectives:
 ●  Masked  Language  Model  (MLM):  Randomly  masks  some  words  
in
 
the
 
input
 
and
 
trains
 
the
 
model
 
to
 
predict
 
these
 
masked
 
words
 
based
 
on
 
their
 
context.
 ●  Ne

---

ked
 
words
 
based
 
on
 
their
 
context.
 ●  Next  Sentence  Prediction  (NSP):  Trains  the  model  to  
understand
 
the
 
relationship
 
between
 
pairs
 
of
 
sentences
 
by
 
predicting
 
whether
 
one
 
sentence
 
follows
 
another
 
in
 
the
 
text.
 ■  Fine-tuning:  After  pretraining,  BERT  can  be  ﬁne-tuned  on  speciﬁc  
downstream
 
tasks
 
(like
 
sentiment
 
analysis,
 
question
 
answering,
 
etc.)
 
by
 
adding
 
a
 
small
 
output
 
layer
 
and
 
training
 
on
 
labeled
 
d

---

output
 
layer
 
and
 
training
 
on
 
labeled
 
data
 
for
 
those
 
tasks.
  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   46 
 
 ●  Tokenization:  ○  BERT  uses  WordPiece  tokenization,  which  breaks  words  into  subwords  or  
tokens,
 
allowing
 
the
 
model
 
to
 
handle
 
out-of-vocabulary
 
words
 
effectively
 
and
 
to
 
understand
 
morphological
 
variations
 
of
 
words.
 
 Key  Differences  from  Previous

---

of
 
words.
 
 Key  Differences  from  Previous  NLP  Models:  1.  Contextual  Word  Representations:  Previous  models  (like  Word2Vec  or  GloVe)  generate  static  embeddings  for  
words,
 
meaning
 
a
 
word
 
has
 
the
 
same
 
representation
 
regardless
 
of
 
context.
 
In
 
contrast,
 
BERT
 
generates
 
dynamic
 
embeddings,
 
allowing
 
the
 
same
 
word
 
to
 
have
 
different
 
representations
 
based
 
on
 
its
 
context
 
in
 
the
 
sentence.
  2.  Bidirectional  Attention:

---

the
 
sentence.
  2.  Bidirectional  Attention:  Models  like  RNNs  and  LSTMs  process  text  sequentially,  which  can  limit  their  
understanding
 
of
 
context.
 
BERT’s
 
bidirectional
 
attention
 
mechanism
 
allows
 
it
 
to
 
consider
 
both
 
preceding
 
and
 
following
 
context
 
simultaneously,
 
leading
 
to
 
a
 
deeper
 
understanding
 
of
 
language
 
nuances.
  3.  Pretraining  on  Large  Datasets:  While  earlier  models  often  required  extensive  feature  engineering  a

---

ften  required  extensive  feature  engineering  and  were  
trained
 
on
 
smaller,
 
task-speciﬁc
 
datasets,
 
BERT
 
is
 
pre-trained
 
on
 
vast
 
amounts
 
of
 
text
 
data
 
(like
 
Wikipedia
 
and
 
BooksCorpus).
 
This
 
extensive
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   47 
 
training  allows  it  to  learn  general  language  patterns  before  being  ﬁne-tuned  
for
 
speciﬁc
 
tasks.
  4.  Versatility:

---

uned  
for
 
speciﬁc
 
tasks.
  4.  Versatility:  BERT  can  be  easily  adapted  to  a  wide  range  of  NLP  tasks  by  ﬁne-tuning  the  
pre-trained
 
model.
 
Previous
 
models
 
often
 
required
 
separate
 
architectures
 
or
 
extensive
 
modiﬁcations
 
for
 
different
 
tasks.
  5.  Handling  of  OutofVocabulary  (OOV)  Words:  BERT's  use  of  subword  tokenization  allows  it  to  better  handle  OOV  words,  
making
 
it
 
more
 
robust
 
in
 
processing
 
realworld
 
text
 
where
 
v

---

t
 
in
 
processing
 
realworld
 
text
 
where
 
vocabulary
 
can
 
vary
 
signiﬁcantly
.
   
Q7.  Why  is  incorporating  relative  positional  information  crucial  in  
transformer
 
models?
 
Discuss
 
scenarios
 
where
 
relative
 
position
 
encoding
 
is
 
particularly
 
beneﬁcial. Incorporating  relative  positional  information  is  crucial  in  transformer  models  because  
transformers
 
are
 
inherently
 
position-agnostic
 
due
 
to
 
their
 
reliance
 
on
 
self-attention
 
mechan

---

their
 
reliance
 
on
 
self-attention
 
mechanisms.
 
This
 
means
 
that,
 
unlike
 
models
 
that
 
process
 
sequences
 
sequentially
 
(such
 
as
 
RNNs),
 
transformers
 
have
 
no
 
inherent
 
sense
 
of
 
the
 
order
 
or
 
relative
 
distance
 
between
 
tokens
 
unless
 
positional
 
information
 
is
 
explicitly
 
added.
 
Relative
 
position
 
encoding
 
addresses
 
this
 
by
 
encoding
 
the
 
position
 
of
 
tokens
 
in
 
a
 
way
 
that
 
emphasizes
 
the
 
relative
 
distances

---

that
 
emphasizes
 
the
 
relative
 
distances
 
between
 
tokens,
 
which
 
is
 
particularly
 
beneﬁcial
 
in
 
scenarios
 
where
 
the
 
relationship
 
between
 
words
 
depends
 
on
 
their
 
relative
 
positions.
 
 
Why
 
Relative
 
Positional
 
Information
 
Matters Relative  position  encoding  captures  the  relative  distances  between  tokens  rather  than  their  
absolute
 
positions,
 
allowing
 
the
 
model
 
to
 
understand
 
not
 
just
 
the
 
order
 
of
 
tokens,
 
but
 
how

---

ot
 
just
 
the
 
order
 
of
 
tokens,
 
but
 
how
 
far
 
apart
 
they
 
are
 
in
 
the
 
sequence.
 
This
 
can
 
improve
 
the
 
model's
 
ability
 
to:
 
1.  Focus  on  Local  Context :  Nearby  words  or  tokens  often  have  stronger  contextual  
relationships,
 
such
 
as
 
in
 
language
 
or
 
time-series
 
data.
 2.  Handle  Variable-Length  Inputs :  Relative  positions  work  well  for  sequences  of  
different
 
lengths,
 
preserving
 
the
 
relationships
 
without
 
requiring
 
sp

---

g
 
the
 
relationships
 
without
 
requiring
 
speciﬁc
 
absolute
 
positions.
 3.  Adapt  Across  Contexts :  Relative  position  encoding  generalizes  better  when  
sequences
 
shift
 
or
 
grow
 
in
 
length,
 
as
 
it
 
preserves
 
the
 
same
 
distances
 
between
 
relevant
 
elements.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   48 
 
Scenarios
 
Where
 
Relative
 
Position
 
Encoding
 
is
 
Beneﬁcial 
1.  Lang

---

e
 
Position
 
Encoding
 
is
 
Beneﬁcial 
1.  Language  Modeling  and  Text  Generation :  ○  In  many  languages,  words'  meanings  are  inﬂuenced  by  nearby  words,  and  
proximity
 
is
 
crucial
 
for
 
syntactic
 
relationships
 
(e.g.,
 
in
 
noun-adjective
 
or
 
subject-verb
 
agreements).
 ○  Relative  encoding  helps  the  model  focus  on  local  dependencies,  like  in  
syntactic
 
parsing
 
or
 
coreference
 
resolution,
 
where
 
certain
 
words
 
relate
 
closely
 
to
 
their

---

certain
 
words
 
relate
 
closely
 
to
 
their
 
immediate
 
neighbors.
 2.  Machine  Translation :  ○  Translation  often  requires  the  model  to  identify  phrases  or  words  with  
corresponding
 
translations
 
at
 
varying
 
positions
 
in
 
both
 
the
 
source
 
and
 
target
 
languages.
 ○  Relative  positions  help  maintain  meaningful  relationships  between  translated  
words
 
or
 
phrases,
 
even
 
when
 
sentence
 
structures
 
vary.
 3.  Question  Answering  and  Information

---

vary.
 3.  Question  Answering  and  Information  Retrieval :  ○  In  question-answering  tasks,  the  answer's  location  relative  to  keywords  in  
the
 
question
 
is
 
important,
 
especially
 
when
 
both
 
the
 
question
 
and
 
answer
 
share
 
common
 
themes
 
or
 
entities.
 ○  Relative  positioning  can  help  the  model  identify  the  answer  by  focusing  on  
text
 
areas
 
near
 
relevant
 
question
 
terms.
 4.  Speech  Recognition  and  Audio  Processing :  ○  In  audio  or

---

nition  and  Audio  Processing :  ○  In  audio  or  speech  data,  local  temporal  dependencies  are  critical  for  
identifying
 
phonetic
 
units
 
or
 
patterns,
 
and
 
relative
 
position
 
encoding
 
captures
 
these
 
temporal
 
relationships
 
effectively.
 ○  Since  audio  data  is  sequential,  relative  distances  between  sound  units  can  
improve
 
phoneme
 
recognition
 
and
 
the
 
understanding
 
of
 
speech
 
segments.
 5.  Time-Series  Analysis :  ○  In  time-series  data,

---

ime-Series  Analysis :  ○  In  time-series  data,  the  importance  of  each  data  point  often  depends  on  its  
time
 
difference
 
from
 
other
 
points.
 ○  Relative  encoding  is  effective  in  forecasting,  anomaly  detection,  and  event  
prediction
 
by
 
capturing
 
dependencies
 
in
 
data
 
across
 
irregular
 
time
 
intervals.
 
Q8.  What  challenges  arise  from  the  ﬁxed  and  limited  attention  span  in  
the
 
vanilla
 
Transformer
 
model?
 
How
 
does
 
this
 
limitatio

---

ansformer
 
model?
 
How
 
does
 
this
 
limitation
 
affect
 
the
 
model's
 
ability
 
to
 
capture
 
long-term
 
dependencies? In  the  Vanilla  Transformer  model,  the  ﬁxed  and  limited  attention  span  arises  primarily  due  
to
 
the
 
self-attention
 
mechanism,
 
which
 
scales
 
quadratically
 
with
 
input
 
sequence
 
length.
 
This
 
limitation
 
affects
 
the
 
model’s
 
capacity
 
to
 
handle
 
long
 
sequences
 
efﬁciently
 
and
 
capture
 
long-term
 
dependencies,
 
creatin

---

nd
 
capture
 
long-term
 
dependencies,
 
creating
 
several
 
challenges:
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   49 
 
 
Key
 
Challenges
 
of
 
Fixed
 
Attention
 
Span
 
in
 
Transformers 1.  Computational  and  Memory  Constraints:  ★  Self-attention  requires  quadratic  memory  and  computational  resources  
proportional
 
to
 
the
 
sequence
 
length,
 
making
 
it
 
impractical
 
for
 
very
 
long
 
inpu

---

king
 
it
 
impractical
 
for
 
very
 
long
 
inputs.
 
 ★  As  the  sequence  length  grows,  so  does  the  resource  demand,  often  exceeding  the  
available
 
memory
 
or
 
computational
 
limits
 
in
 
hardware.
 
 ★  This  challenge  makes  training  and  inference  on  long  sequences,  like  entire  
documents,
 
research
 
articles,
 
or
 
long
 
conversations,
 
inefﬁcient
 
and
 
sometimes
 
unfeasible.
  2.  Loss  of  LongTerm  Context:  ★  Due  to  ﬁxed-length  input  windows,  th

---

ext:  ★  Due  to  ﬁxed-length  input  windows,  the  Vanilla  Transformer  is  often  forced  to  
truncate
 
or
 
segment
 
long
 
sequences.
 
 ★  This  segmentation  limits  the  model’s  ability  to  access  distant  parts  of  the  input,  
effectively
 
breaking
 
the
 
input
 
context.
 ★  As  a  result,  the  model  may  not  fully  understand  relationships  between  distant  
tokens,
 
which
 
can
 
hinder
 
performance
 
in
 
tasks
 
where
 
such
 
dependencies
 
are
 
crucial,
 
such

---

here
 
such
 
dependencies
 
are
 
crucial,
 
such
 
as
 
summarization,
 
document-level
 
translation,
 
and
 
question
 
answering
 
across
 
long
 
contexts.
  3.  Limited  Capacity  for  Hierarchical  Structure:  ★  Language  and  other  sequential  data  often  have  hierarchical  structures,  where  
understanding
 
high-level
 
context
 
depends
 
on
 
aggregating
 
multiple
 
lower-level
 
patterns.
 
The
 
Vanilla
 
Transformer's
 
ﬂat
 
attention
 
structure
 
is
 
less
 
effective

---

attention
 
structure
 
is
 
less
 
effective
 
in
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   50 
 
capturing  this  hierarchy,  especially  in  long  texts,  where  earlier  sentences  may  
establish
 
context
 
that
 
should
 
inﬂuence
 
later
 
parts.
  4.  Contextual  Fragmentation  in  Long  Sequences:  ★  When  dealing  with  long  sequences,  models  that  use  ﬁxed-length  windows  must  
break
 
the
 
text

---

e  ﬁxed-length  windows  must  
break
 
the
 
text
 
into
 
chunks
 
that
 
are
 
processed
 
independently.
 
This
 
fragmentation
 
can
 
lead
 
to
 
contextual
 
gaps,
 
where
 
information
 
that
 
spans
 
across
 
boundaries
 
is
 
lost.
 ★  Tasks  like  conversation  modeling,  where  past  exchanges  inform  current  responses,  
suffer
 
when
 
these
 
context
 
fragments
 
are
 
not
 
handled
 
holistically.
  
Impact
 
on
 
LongTerm
 
Dependency
 
Capture 1.  Degraded  Performance  on

---

ependency
 
Capture 1.  Degraded  Performance  on  Long-Context  Tasks:  Tasks  like  summarization,  story  generation,  and  dialogue  systems,  which  require  
understanding
 
of
 
long-term
 
dependencies,
 
often
 
see
 
degraded
 
performance
 
when
 
limited
 
to
 
ﬁxed
 
attention
 
spans.
 The  inability  to  maintain  a  consistent  representation  across  long  segments  means  the  
model
 
may
 
miss
 
out
 
on
 
relevant
 
information
 
that
 
appeared
 
earlier,
 
leading
 
to

---

ion
 
that
 
appeared
 
earlier,
 
leading
 
to
 
shallow
 
or
 
contextually
 
incomplete
 
responses.
  2.  Difﬁculty  with  Sequential  Dependency  Tracking:  Transformers'  limited  span  makes  it  difﬁcult  to  capture  sequential  dependencies  over  long  
sequences,
 
such
 
as
 
tracking
 
events
 
in
 
narratives
 
or
 
causal
 
chains
 
in
 
technical
 
documents.
 
The
 
model
 
may
 
struggle
 
to
 
preserve
 
the
 
continuity
 
of
 
thought,
 
reference,
 
or
 
theme
 
when
 
dist

---

thought,
 
reference,
 
or
 
theme
 
when
 
distant
 
tokens
 
need
 
to
 
interact.
  3.  Increased  Sensitivity  to  Hyperparameters  and  Attention  Mechanism  Tuning:  To  balance  performance  and  resource  use,  practitioners  must  carefully  tune  
hyperparameters
 
like
 
sequence
 
length,
 
batch
 
size,
 
and
 
model
 
depth.
 
However,
 
these
 
adjustments
 
often
 
trade
 
off
 
between
 
capturing
 
adequate
 
context
 
and
 
ﬁtting
 
within
 
memory
 
constraints,
 
limiting

---

tting
 
within
 
memory
 
constraints,
 
limiting
 
the
 
model's
 
effectiveness
 
on
 
long
 
sequence
 
tasks.
  The  ﬁxed  attention  span  in  the  vanilla  Transformer  restricts  its  ability  to  capture  long-term  
dependencies,
 
leading
 
to
 
challenges
 
in
 
processing
 
and
 
understanding
 
long
 
sequences.
 
Solutions
 
like
 
memory-augmented
 
architectures,
 
efﬁcient
 
attention
 
mechanisms,
 
and
 
hierarchical
 
models
 
help
 
address
 
these
 
issues,
 
making
 
Trans

---

help
 
address
 
these
 
issues,
 
making
 
Transformer
 
models
 
more
 
effective
 
for
 
tasks
 
that
 
require
 
extended
 
context
 
understanding.
  
Q9.  Why  is  naively  increasing  context  length  not  a  straightforward  
solution
 
for
 
handling
 
longer
 
context
 
in
 
transformer
 
models?
 
What
 
computational
 
and
 
memory
 
challenges
 
does
 
it
 
pose? 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions

---

Generative  AI  Interview  Questions   51 
 
Increasing  the  context  length  in  Transformer  models  to  handle  longer  sequences  may  seem  
like
 
a
 
straight-forward
 
solution,
 
but
 
it
 
poses
 
signiﬁcant
 
computational
 
and
 
memory
 
challenges,
 
which
 
make
 
it
 
impractical
 
in
 
many
 
cases.
 
  
Here's
 
a
 
breakdown
 
of
 
why
 
this
 
approach
 
is
 
problematic: 1.  Quadratic  Complexity  in  Self-Attention:  ➢  The  self-attention  mechanism  in  Trans

---

ion:  ➢  The  self-attention  mechanism  in  Transformers  requires  calculating  attention  scores  
between
 
each
 
pair
 
of
 
tokens
 
in
 
the
 
input
 
sequence.
 
 ➢  This  results  in  a  complexity  of  ,  where   is  the  sequence  length.   ➢  Doubling  the  context  length  quadruples  the  computation  required  for  self-attention,  
making
 
it
 
costly
 
to
 
handle
 
even
 
moderate
 
increases
 
in
 
sequence
 
length.
  2.  Memory  Constraints:  ➢  Self-attention  also  consu

---

mory  Constraints:  ➢  Self-attention  also  consumes  memory  at   due  to  the  need  to  store  attention  weights  for  every  pair  of  tokens.  With  large  contexts,  the  memory  required  can  
exceed
 
the
 
capacity
 
of
 
GPUs
 
or
 
TPUs,
 
especially
 
when
 
dealing
 
with
 
multiple
 
layers
 
and
 
large
 
batch
 
sizes.
 ➢  This  limitation  is  exacerbated  in  deep  Transformer  models,  where  each  layer  stores  
its
 
own
 
attention
 
maps.
 
 ➢  For  instance,  extendin

---

attention
 
maps.
 
 ➢  For  instance,  extending  a  512-token  sequence  to  2,048  tokens  would  increase  
memory
 
requirements
 
16-fold
 
just
 
for
 
attention,
 
which
 
becomes
 
infeasible
 
on
 
standard
 
hardware.
  3.  Batch  Processing  Slowdown:  ➢  As  the  sequence  length  increases,  processing  each  batch  becomes  slower  due  to  the  
increased
 
compute
 
and
 
memory
 
load.
 
 ➢  This  limits  the  model’s  training  efﬁciency,  making  each  iteration  signiﬁcant

---

g  efﬁciency,  making  each  iteration  signiﬁcantly  slower  
and
 
increasing
 
the
 
time
 
required
 
for
 
model
 
convergence.
 ➢  Longer  contexts  also  mean  that  fewer  sequences  can  ﬁt  into  a  batch,  further  
reducing
 
computational
 
efﬁciency
 
and
 
often
 
impacting
 
model
 
stability
 
during
 
training.
  4.  Degradation  in  Gradient  Signal:  ➢  Increasing  the  context  length  can  dilute  the  gradient  signal,  especially  in  tasks  
where
 
only
 
certain
 
part

---

ecially  in  tasks  
where
 
only
 
certain
 
parts
 
of
 
the
 
sequence
 
contain
 
relevant
 
information.
 
 ➢  This  can  make  optimization  harder,  leading  to  longer  training  times  and  potentially  
less
 
effective
 
learning.
 ➢  For  instance,  if  only  a  small  portion  of  a  long  sequence  is  crucial  for  a  task  (e.g.,  
answering
 
a
 
question
 
based
 
on
 
a
 
passage),
 
naively
 
increasing
 
the
 
context
 
length
 
can
 
cause
 
the
 
model
 
to
 
struggle
 
in

---

h
 
can
 
cause
 
the
 
model
 
to
 
struggle
 
in
 
focusing
 
on
 
the
 
important
 
parts,
 
reducing
 
training
 
effectiveness.
  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   52 
 
5.  Impact  on  Model  Generalization:  Long  contexts  can  lead  the  model  to  memorize  instead  of  generalizing,  as  it  may  focus  on  
speciﬁc
 
patterns
 
within
 
the
 
extended
 
sequence.
 
 This  can  negatively  impact  g

---

ed
 
sequence.
 
 This  can  negatively  impact  generalization  to  unseen  data,  where  the  context  length  may  
vary
 
or
 
where
 
only
 
selective
 
information
 
from
 
the
 
context
 
is
 
relevant.
  
Q10.  How  does  self-attention  work? Self-attention  is  the  mechanism  in  Transformers  that  enables  each  word  (or  token)  in  a  
sequence
 
to
 
attend
 
to
 
every
 
other
 
word
 
in
 
the
 
sequence,
 
creating
 
context-aware
 
representations
 
that
 
consider
 
relatio

---

ware
 
representations
 
that
 
consider
 
relationships
 
between
 
tokens.
 
 In  self-attention,  each  token  in  a  sequence  is  transformed  into  three  vectors:  Query  (Q),  
Key
 
(K),
 
and
 
Value
 
(V).
 
 
Here’s
 
a
 
step-by-step
 
breakdown
 
of
 
how
 
it
 
works: 1.  Input  Embeddings  and  Transformation  to  Q,  K,  and  V  Vectors  ➔  Each  token  in  the  sequence  is  embedded  as  a  vector.  ➔  This  embedded  vector  is  then  multiplied  by  three  learned  weight  m

---

s  then  multiplied  by  three  learned  weight  matrices  to  
produce
 
three
 
distinct
 
vectors
 
for
 
each
 
token:
 
Query
 
(Q),
 
Key
 
(K),
 
and
 
Value
 
(V).
 ➔  These  vectors  capture  different  aspects  of  the  token.   ◆  The  Query  represents  what  information  this  token  is  looking  for,   ◆  the  Key  represents  the  content  of  each  token,  and   ◆  the  Value  holds  the  information  that  will  ultimately  be  aggregated.   2.  Calculating  Attention  Scores  (

---

ggregated.   2.  Calculating  Attention  Scores  (Q-K  Dot  Product)  ➔  For  each  token,  we  calculate  how  much  attention  it  should  pay  to  other  tokens.   ➔  This  is  done  by  taking  the  dot  product  of  the  Query-vector  of  a  token  with  the  Key  
vectors
 
of
 
all
 
tokens
 
in
 
the
 
sequence.
 ➔  The  dot  product  provides  a  similarity  score  between  the  Query  and  Key  pairs,  
indicating
 
how
 
closely
 
related
 
the
 
tokens
 
are.
 ➔  To  prevent  overly

---

ted
 
the
 
tokens
 
are.
 ➔  To  prevent  overly  large  values  and  stabilize  gradients,  the  dot  products  are  divided  by  the  square  root  of  the  Key  vector  s  dimensionality,  .   
      3.  Applying  Softmax  to  Obtain  Weights  ➔  The  attention  scores  are  passed  through  a  softmax  function  to  produce  a  probability  
distribution
 
(weights)
 
across
 
all
 
tokens.
 
This
 
indicates
 
the
 
level
 
of
 
attention
 
that
 
each
 
token
 
should
 
give
 
to
 
every

---

that
 
each
 
token
 
should
 
give
 
to
 
every
 
other
 
token.
 ➔  The  soft-maxed  scores  emphasize  tokens  that  are  more  relevant  to  the  query  and  
downplay
 
less
 
relevant
 
ones.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   53 
 
 
     4.  Weighted  Sum  of  Values  ➔  The  attention  weights  are  then  used  to  compute  a  weighted  sum  of  the  Value  (V)  
vectors.
 
 ➔  Each  token's  ﬁnal  r

---

Value  (V)  
vectors.
 
 ➔  Each  token's  ﬁnal  representation  is  a  weighted  combination  of  the  Value  vectors  
from
 
all
 
tokens
 
in
 
the
 
sequence,
 
allowing
 
it
 
to
 
incorporate
 
information
 
from
 
other
 
tokens
 
based
 
on
 
their
 
relevance.
       5.  MultiHead  Attention  ➔  In  practice,  the  model  uses  multiple  attention  heads,  each  with  its  own  Q,  K,  and  V  
weight
 
matrices.
 
 ➔  This  allows  the  model  to  learn  various  types  of  relationsh

---

model  to  learn  various  types  of  relationships  between  tokens  in  
parallel.
 ➔  The  outputs  from  each  head  are  concatenated  and  linearly  transformed  to  form  the  
ﬁnal
 
attention
 
output.
  
Q11.  What  pre-training  mechanisms  are  used  for  LLMs,  explain  a  few  Large  Language  Models  (LLMs)  rely  on  pretraining  mechanisms  that  enable  them  to  
understand
 
language
 
structure,
 
grammar,
 
and
 
context
 
by
 
training
 
on
 
vast
 
amounts
 
of
 
text

---

by
 
training
 
on
 
vast
 
amounts
 
of
 
text
 
data.
 
 
Here
 
are
 
some
 
of
 
the
 
most
 
common
 
pre-training
 
mechanisms
 
used
 
in
 
LLMs: 1.  Masked  Language  Modeling  (MLM)  ➔  Description:  Used  in  models  like  BERT  (Bidirectional  Encoder  Representations  from  
Transformers),
 
MLM
 
randomly
 
masks
 
out
 
some
 
words
 
in
 
a
 
sentence,
 
asking
 
the
 
model
 
to
 
predict
 
these
 
masked
 
tokens
 
based
 
on
 
the
 
surrounding
 
context.
 ➔  Goal:  Teaches  t

---

the
 
surrounding
 
context.
 ➔  Goal:  Teaches  the  model  to  understand  context  on  both  sides  of  a  word,  
encouraging
 
a
 
bidirectional
 
understanding
 
of
 
text.
 ➔  Example:  In  the  sentence  "The  cat  sat  on  the  [MASK],"  the  model  must  predict  "mat"  
by
 
considering
 
the
 
entire
 
sentence.
     2.  Causal  Language  Modeling  (CLM)  ➔  Description:  Used  in  models  like  GPT  (Generative  Pretrained  Transformer),  CLM  
processes
 
text
 
in
 
a
 
unidirecti

---

er),  CLM  
processes
 
text
 
in
 
a
 
unidirectional
 
way,
 
predicting
 
each
 
word
 
based
 
only
 
on
 
previous
 
words
 
in
 
the
 
sequence.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   54 
 
➔  Goal:  This  approach  is  useful  for  generative  tasks,  as  the  model  learns  to  produce  
coherent
 
text
 
one
 
word
 
at
 
a
 
time.
 ➔  Example:  In  the  input  "The  cat  sat  on  the,"  the  model  might

---

nput  "The  cat  sat  on  the,"  the  model  might  predict  "mat"  as  the  next  
word.
  3.  Next  Sentence  Prediction  (NSP)  ➔  Description:  An  additional  task  in  BERT’s  pretraining,  NSP  aims  to  help  the  model  
understand
 
relationships
 
between
 
sentences.
 
The
 
model
 
is
 
presented
 
with
 
two
 
sentences
 
and
 
must
 
predict
 
whether
 
the
 
second
 
sentence
 
logically
 
follows
 
the
 
ﬁrst.
 ➔  Goal:  This  helps  the  model  grasp  longer  text  dependencies

---

lps  the  model  grasp  longer  text  dependencies,  which  is  beneﬁcial  for  
tasks
 
like
 
question
 
answering
 
and
 
summarization.
 ➔  Example:  Given  two  sentences,  "The  sky  is  clear."  and  "It  might  rain  tomorrow,"  the  
model
 
learns
 
to
 
determine
 
that
 
these
 
do
 
not
 
naturally
 
follow
 
one
 
another.
  4.  Permutation  Language  Modeling  (PLM)  ➔  Description:  Used  in  models  like  XLNet,  PLM  rearranges  the  order  of  words  in  a  
sequence
 
during

---

the  order  of  words  in  a  
sequence
 
during
 
training,
 
asking
 
the
 
model
 
to
 
predict
 
each
 
word
 
in
 
various
 
possible
 
orders.
 
This
 
blends
 
the
 
beneﬁts
 
of
 
MLM(Masked
 
Language
 
Modeling)
 
and
 
CLM(Casual
 
Language
 
Modeling).
 ➔  Goal:  Allows  for  a  bidirectional  context  like  MLM  while  maintaining  the  
autoregressive
 
property
 
of
 
CLM,
 
providing
 
ﬂexibility
 
in
 
learning
 
dependencies.
 ➔  Example:  In  a  sentence,  "The  cat  sat  on

---

➔  Example:  In  a  sentence,  "The  cat  sat  on  the  mat,"  different  word  order  permutations  
might
 
be
 
used,
 
such
 
as
 
"on
 
the
 
cat
 
mat
 
sat."
  5.  Denoising  Auto-encoding  (DA)  ➔  Description:  Used  in  models  like  T5  (Text-To-Text  Transfer  Transformer),  DA  
involves
 
corrupting
 
parts
 
of
 
an
 
input
 
sequence
 
(by
 
shufﬂing
 
or
 
masking
 
words)
 
and
 
training
 
the
 
model
 
to
 
reconstruct
 
the
 
original
 
sentence.
 ➔  Goal:  Helps  the  model

---

original
 
sentence.
 ➔  Goal:  Helps  the  model  learn  robust  sentence  representations  by  forcing  it  to  
understand
 
context
 
to
 
recover
 
original
 
input.
 ➔  Example:  The  sentence  "The  cat  sat  on  the  mat"  may  be  corrupted  as  "The  [MASK]  
sat
 
on
 
mat,"
 
and
 
the
 
model
 
must
 
reconstruct
 
it
 
correctly.
  
Q12.  Why  is  multi-head  attention  needed? Multihead  attention  is  a  technique  used  in  transformer  models  that  enables  them  to  learn

---

sformer  models  that  enables  them  to  learn  
and
 
focus
 
on
 
different
 
parts
 
of
 
a
 
sequence
 
simultaneously,
 
which
 
improves
 
the
 
model's
 
ability
 
to
 
capture
 
complex
 
dependencies
 
in
 
the
 
data.
 
  
Here's
 
why
 
it's
 
needed
 
and
 
beneﬁcial:
 ➢  Capturing  Diverse  Relationships  ➢  Mitigating  Information  Bottlenecks  ➢  Learning  Contextual  Nuances  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interv

---

Generative  AI  Interview  Questions   55 
 
➢  Improving  Training  Stability  and  Performance   
Q13.  What  is  RLHF,  how  is  it  used? Reinforcement  Learning  from  Human  Feedback  (RLHF)  is  a  method  that  combines  human  
feedback
 
with
 
reinforcement
 
learning
 
to
 
ﬁnetune
 
models
,
 
particularly
 
in
 
areas
 
where
 
models
 
need
 
to
 
generate
 
responses
 
that
 
align
 
closely
 
with
 
human
 
values
 
or
 
preferences.
 
This
 
approach

---

man
 
values
 
or
 
preferences.
 
This
 
approach
 
has
 
been
 
especially
 
impactful
 
in
 
aligning
 
large
 
language
 
models
 
to
 
produce
 
helpful,
 
safe,
 
and
 
coherent
 
responses.
  
RLHF
 
typically
 
involves
 
three
 
main
 
steps: 1.  Pretraining  a  Language  Model  (LM)  Start  with  a  large,  pretrained  language  model  
using
 
standard
 
objectives
 
(e.g.,
 
OpenAI’s
 
GPT,
 
DeepMind’s
 
Gopher).
 
This
 
model
 
may
 
be
 
further
 
ﬁne-tuned
 
with
 
human-generat

---

be
 
further
 
ﬁne-tuned
 
with
 
human-generated
 
text
 
or
 
guided
 
by
 
speciﬁc
 
preferences
 
(e.g.,
 
“helpful,
 
honest,
 
harmless”).
          2.  Training  a  Reward  Model  (RM)  Develop  a  reward  model  to  interpret  human  
preferences.
 
This
 
involves
 
generating
 
text
 
outputs
 
from
 
the
 
LM
 
and
 
then
 
using
 
human
 
annotators
 
to
 
rank
 
these
 
outputs
 
based
 
on
 
preference.
 
These
 
rankings
 
are
 
then
 
converted
 
into
 
a
 
scalar
 
reward
 
s

---

then
 
converted
 
into
 
a
 
scalar
 
reward
 
signal.
 
This
 
reward
 
model,
 
calibrated
 
with
 
human
 
preferences,
 
scores
 
the
 
LM’s
 
outputs
 
for
 
training.
       

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   56 
 
3.  Fine-tuning  the  LM  with  Reinforcement  
Learning
 Use  a  reinforcement  learning  algorithm  (e.g.,  
Proximal
 
Policy
 
Optimization,
 
PPO)
 
to
 
adjust
 
the
 
LM
 
based
 
on

---

on,
 
PPO)
 
to
 
adjust
 
the
 
LM
 
based
 
on
 
the
 
reward
 
model’s
 
scores.
 
To
 
ensure
 
coherence,
 
a
 
KL-divergence
 
penalty
 
limits
 
how
 
much
 
the
 
model
 
diverges
 
from
 
its
 
original
 
pretrained
 
form.
 
The
 
ﬁnal
 
reward
 
signal
 
combines
 
human
 
preference
 
scores
 
with
 
this
 
penalty,
 
reﬁning
 
the
 
model’s
 
alignment
 
with
 
human
 
expectations.
   
Applications
 
of
 
RLHF RLHF  is  most  commonly  applied  to  large  language  models,  but  it

---

ly  applied  to  large  language  models,  but  it  can  be  extended  to  other  
domains
 
that
 
require
 
nuanced
 
outputs,
 
including:
 ★  Improving  Language  Model  Alignment:  Fine-tuning  models  like  ChatGPT  or  GPT4  to  
produce
 
responses
 
that
 
are
 
more
 
aligned
 
with
 
user
 
expectations,
 
values,
 
and
 
ethical
 
considerations.
 ★  Safety  and  Moderation:  RLHF  helps  models  avoid  generating  harmful,  toxic,  or  
biased
 
language
 
by
 
reinforcing
 
safer

---

r  
biased
 
language
 
by
 
reinforcing
 
safer
 
responses.
 ★  Factual  Accuracy:  Reward  models  can  be  tuned  to  prefer  accurate  information,  
helping
 
reduce
 
hallucinations
 
in
 
language
 
models.
 ★  Personalization:  In  customer  service,  education,  or  entertainment,  RLHF  enables  
models
 
to
 
adapt
 
to
 
user
 
preferences
 
for
 
a
 
more
 
personalized
 
interaction.
  
Q14.  What  is  catastrophic  forgetting  in  the  context  of  LLMs  Catastrophic  forgetting

---

the  context  of  LLMs  Catastrophic  forgetting  is  a  phenomenon  in  machine  learning  where  a  model,  particularly  
in
 
the
 
context
 
of
 
large
 
language
 
models
 
(LLMs),
 
"forgets"
 
previously
 
learned
 
information
 
when
 
it
 
is
 
trained
 
on
 
new
 
data.
 
 This  issue  is  common  when  a  model  is  ﬁnetuned  or  updated  on  a  new  dataset  without  
retaining
 
or
 
reinforcing
 
prior
 
knowledge.
 
 Catastrophic  forgetting  can  lead  to  signiﬁcant  performan

---

forgetting  can  lead  to  signiﬁcant  performance  drops  on  tasks  or  knowledge  
that
 
the
 
model
 
previously
 
handled
 
well.
  
How
 
Catastrophic
 
Forgetting
 
Occurs
 
in
 
LLMs In  neural  networks,  weights  are  updated  to  minimize  loss  based  on  the  current  training  
data.
 
When
 
the
 
model
 
encounters
 
a
 
new
 
dataset
 
with
 
different
 
distributions
 
or
 
objectives,
 
these
 
weights
 
are
 
adjusted
 
to
 
optimize
 
performance
 
on
 
the
 
new
 
data.

---

optimize
 
performance
 
on
 
the
 
new
 
data.
 
However,
 
these
 
updates
 
can
 
disrupt
 
representations
 
and
 
patterns
 
learned
 
from
 
earlier
 
data,
 
causing
 
the
 
model
 
to
  
forget
  
or
 
deprioritize
 
previously
 
learned
 
information.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   57 
 
 For  LLMs,  catastrophic  forgetting  is  especially  problematic  because  they  are  often  
ﬁne-tuned
 
o

---

lematic  because  they  are  often  
ﬁne-tuned
 
on
 
speciﬁc
 
tasks
 
or
 
domains
 
after
 
general
 
pretraining.
 
For
 
instance:
 ➢  Domain  Adaptation:  Fine-tuning  a  general  LLM  on  legal  or  medical  data  could  lead  it  
to
 
lose
 
general
 
conversational
 
abilities
 
or
 
common
 
knowledge.
 ➢  TaskSpeciﬁc  Adaptation:  Adapting  a  model  for  sentiment  analysis  or  translation  
might
 
reduce
 
its
 
effectiveness
 
in
 
other
 
tasks
 
like
 
summarization
 
or
 
lan

---

other
 
tasks
 
like
 
summarization
 
or
 
language
 
generation
 
if
 
the
 
training
 
updates
 
disrupt
 
its
 
general
 
language
 
understanding.
  
Q15.  In  a  transformer-based  sequence-to-sequence  model,  what  are  the  
primary
 
functions
 
of
 
the
 
encoder
 
and
 
decoder?
 
How
 
does
 
information
 
ﬂow
 
between
 
them
 
during
 
both
 
training
 
and
 
inference?
 In  a  transformer-based  sequence-to-sequence  model  (e.g.,  for  translation  or  
summarization),
 
the

---

g.,  for  translation  or  
summarization),
 
the
 
encoder
 
and
 
decoder
 
perform
 
distinct
 
roles:
 Encoder :  1.  Input  Embedding :  Transforms  each  token  in  
the
 
input
 
sequence
 
into
 
a
 
high-dimensional
 
vector.
 2.  Positional  Encoding :  Adds  positional  
information
 
to
 
embeddings,
 
enabling
 
the
 
model
 
to
 
understand
 
word
 
order.
 3.  Self-Attention :  Allows  each  token  to  interact  
with
 
others
 
in
 
the
 
sequence,
 
capturing
 
context
 
and
 
d

---

the
 
sequence,
 
capturing
 
context
 
and
 
dependencies.
 4.  Feed-Forward  Layers :  Further  processes  the  
self-attention
 
outputs
 
to
 
reﬁne
 
token
 
representations.
 5.  Output  Representation :  Creates  
context-rich
 
representations
 
of
 
the
 
input
 
sequence,
 
which
 
are
 
passed
 
to
 
the
 
decoder.
  Decoder :  1.  Masked  Self-Attention :  Focuses  on  
previously
 
generated
 
tokens,
 
ensuring
 
future
 
tokens
 
aren’t
 
seen,
 
for
 
sequential
 
prediction.

---

aren’t
 
seen,
 
for
 
sequential
 
prediction.
 2.  Encoder-Decoder  Attention :  Allows  the  
decoder
 
to
 
access
 
the
 
encoder’s
 
output,
 
integrating
 
input
 
sequence
 
information
 
into
 
predictions.
 3.  Feed-Forward  Layers :  Reﬁnes  the  attended  output  to  improve  token  generation.   

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   58 
 
Information  Flow :  ➢  Training :  The  encoder  processes

---

Flow :  ➢  Training :  The  encoder  processes  the  input,  and  the  decoder  receives  these  
representations
 
along
 
with
 
the
 
target
 
sequence
 
up
 
to
 
each
 
token.
 
Teacher
 
Forcing
 
(feeding
 
correct
 
previous
 
tokens)
 
is
 
often
 
used
 
to
 
guide
 
learning.
 ➢  Inference :  The  encoder  processes  the  entire  input  in  one  go,  and  the  decoder  
generates
 
tokens
 
one
 
by
 
one,
 
using
 
its
 
previous
 
outputs
 
and
 
continuously
 
referencing
 
the

---

outputs
 
and
 
continuously
 
referencing
 
the
 
encoder’s
 
output
 
for
 
context.
   
Q16.  Why  is  positional  encoding  crucial  in  transformer  models,  and  what  
issue
 
does
 
it
 
address
 
in
 
the
 
context
 
of
 
self-attention
 
operations? Positional  encoding  is  essential  in  transformer  models  because  it  introduces  token  order  
information,
 
which
 
self-attention
 
alone
 
lacks.
 
 Unlike  RNNs  that  process  tokens  sequentially,  transformers  handle  all  t

---

okens  sequentially,  transformers  handle  all  tokens  at  once  and,  
therefore,
 
do
 
not
 
inherently
 
recognize
 
token
 
positions.
  Positional  encodings  solve  this  by  adding  unique  position-based  vectors  to  each  token's  
embedding
 
before
 
self-attention.
 
 Typically,  sine  and  cosine  functions  are  used  to  encode  positions,  enabling  the  model  to  
identify
 
a
 
token’s
 
place
 
in
 
the
 
sequence.
 
 This  lets  self-attention  consider  both  "what"  ea

---

lets  self-attention  consider  both  "what"  each  token  is  and  "where"  it  is,  which  is  crucial  
for
 
tasks
 
like
 
translation
 
and
 
text
 
generation,
 
where
 
sequence
 
order
 
impacts
 
meaning.
  
Q17.  When  applying  transfer  learning  to  ﬁne-tune  a  pre-trained  
transformer
 
for
 
a
 
speciﬁc
 
NLP
 
task,
 
what
 
strategies
 
can
 
be
 
employed
 
to
 
ensure
 
effective
 
knowledge
 
transfer,
 
especially
 
when
 
dealing
 
with
 
domain-speciﬁc
 
data? To  ﬁne

---

dealing
 
with
 
domain-speciﬁc
 
data? To  ﬁne-tune  a  pre-trained  transformer  for  domain-speciﬁc  NLP  tasks,  here  are  key  
strategies
 
to
 
ensure
 
effective
 
knowledge
 
transfer:
 ➔  Data  Preparation  and  Augmentation :  ◆  Data  Cleaning :  Remove  noise,  inconsistencies,  and  irrelevant  information  for  
better
 
data
 
quality.
 ◆  Data  Augmentation :  Use  techniques  like  back-translation,  synonym  
replacement,
 
and
 
synthetic
 
text
 
generation
 
to
 
expand

---

and
 
synthetic
 
text
 
generation
 
to
 
expand
 
and
 
diversify
 
the
 
dataset.
 ◆  Domain  Adaptation :  When  the  target  domain  differs  from  pre-training,  
consider
 
ﬁne-tuning
 
on
 
a
 
small
 
domain-speciﬁc
 
dataset
 
or
 
using
 
domain-speciﬁc
 
embeddings
 
to
 
bridge
 
the
 
gap.
 ➔  Fine-Tuning  Techniques :  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   59 
 
◆  Feature-Based  Fine-Tuning :  Fre

---

tions   59 
 
◆  Feature-Based  Fine-Tuning :  Freeze  lower  layers  and  train  only  the  top  layers  
to
 
focus
 
on
 
task-speciﬁc
 
features,
 
especially
 
if
 
the
 
task
 
is
 
similar
 
to
 
pre-training.
 ◆  Parameter-Based  Fine-Tuning :  Fine-tune  all  or  a  subset  of  layers  for  tasks  
that
 
differ
 
signiﬁcantly
 
from
 
pre-training.
 ◆  Layer-Wise  Fine-Tuning :  Adjust  learning  rates  across  layers,  giving  more  
focus
 
to
 
task-speciﬁc
 
adaptation
 
in
 
top

---

ocus
 
to
 
task-speciﬁc
 
adaptation
 
in
 
top
 
layers
 
while
 
preserving
 
general
 
knowledge
 
in
 
lower
 
layers.
 ➔  Regularization  Methods :  ◆  Dropout :  Randomly  drop  units  to  prevent  overﬁtting.  ◆  Early  Stopping :  Stop  training  based  on  validation  loss  to  avoid  overﬁtting.  ◆  L1/L2  Regularization :  Penalize  large  weights  to  control  overﬁtting.  ◆  Data  Augmentation  as  Regularization :  Increased  data  variability  can  also  
regularize
 
the
 
model

---

variability  can  also  
regularize
 
the
 
model.
 ➔  Hyperparameter  Tuning :  ◆  Learning  Rate :  Choose  an  optimal  rate  for  faster  convergence.  ◆  Batch  Size :  Select  a  stable  batch  size  for  efﬁcient  training.  ◆  Optimizer :  Experiment  with  optimizers  like  Adam,  SGD,  or  RMSprop  to  
improve
 
learning
 
dynamics.
 ➔  Model  Architecture  and  Hyperparameters :  ◆  Task-Speciﬁc  Architecture :  Use  models  suited  for  the  task,  e.g.,  BERT  for  
classiﬁcation,

---

for  the  task,  e.g.,  BERT  for  
classiﬁcation,
 
GPT-2
 
for
 
generation.
 ◆  Hyperparameter  Tuning:  Experiment  with  the  number  of  layers,  attention  
heads,
 
and
 
hidden
 
dimensions
 
to
 
optimize
 
model
 
performance.
  
Q18.  Discuss  the  role  of  cross-attention  in  transformer-based  
encoder-decoder
 
models.
 
How
 
does
 
it
 
facilitate
 
the
 
generation
 
of
 
output
 
sequences
 
based
 
on
 
information
 
from
 
the
 
input
 
sequence? Cross-attention  is  a  cr

---

the
 
input
 
sequence? Cross-attention  is  a  crucial  mechanism  in  transformer-based  encoder-decoder  models,  
enabling
 
the
 
decoder
 
to
 
effectively
 
leverage
 
information
 
from
 
the
 
input
 
sequence
 
to
 
generate
 
relevant
 
output
 
sequences.
 
It
 
allows
 
the
 
model
 
to
 
dynamically
 
attend
 
to
 
different
 
parts
 
of
 
the
 
input
 
sequence
 
based
 
on
 
the
 
current
 
decoding
 
step,
 
resulting
 
in
 
more
 
contextually
 
aware
 
and
 
coherent
 
outputs

---

contextually
 
aware
 
and
 
coherent
 
outputs.
  
How
 
Cross-Attention
 
Works:  ●  Encoder  Processing:  ○  The  input  sequence  is  fed  into  the  encoder,  which  processes  it  through  
multiple
 
layers
 
of
 
self-attention
 
and
 
feed-forward
 
neural
 
networks.
 ○  Each  layer  in  the  encoder  generates  a  context  vector  for  each  input  token,  
capturing
 
its
 
semantic
 
and
 
syntactic
 
relationships
 
with
 
other
 
tokens.
 

MD  ARBAAZ  KHAN

---

tokens.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   60 
 
●  Decoder  Processing:  ○  The  decoder  processes  the  input  sequence  one  token  at  a  time,  
auto-regressively.
 ○  At  each  decoding  step,  the  decoder's  current  hidden  state  is  used  to  compute  
queries,
 
keys,
 
and
 
values.
 ○  The  queries  are  compared  to  the  keys  from  the  encoder's  output  using  a  
similarity
 
function

---

der's  output  using  a  
similarity
 
function
 
(e.g.,
 
dot
 
product).
 ○  The  attention  scores  are  calculated,  indicating  the  relevance  of  each  
encoder's
 
context
 
vector
 
to
 
the
 
current
 
decoding
 
step.
 ○  The  encoder's  context  vectors  are  weighted  by  the  attention  scores  to  create  
a
 
context
 
vector
 
for
 
the
 
decoder.
 ●  Output  Generation:  ○  The  decoder's  current  hidden  state  and  the  context  vector  from  the  encoder  
are
 
combined

---

xt  vector  from  the  encoder  
are
 
combined
 
to
 
generate
 
the
 
next
 
output
 
token.
 ○  This  process  is  repeated  iteratively  until  the  end-of-sequence  token  is  
generated,
 
producing
 
the
 
complete
 
output
 
sequence.
  Beneﬁts  of  Cross-Attention:  ➔  Contextual  Understanding:  Cross-attention  allows  the  decoder  to  access  relevant  
information
 
from
 
the
 
input
 
sequence
 
at
 
each
 
decoding
 
step,
 
improving
 
its
 
ability
 
to
 
generate
 
contextual

---

oving
 
its
 
ability
 
to
 
generate
 
contextually
 
appropriate
 
outputs.
 ➔  Long-Range  Dependencies:  Transformers  can  capture  long-range  dependencies  in  
the
 
input
 
sequence,
 
enabling
 
the
 
decoder
 
to
 
generate
 
outputs
 
that
 
are
 
inﬂuenced
 
by
 
distant
 
information.
 ➔  Flexible  Generation:  Cross-attention  enables  the  decoder  to  dynamically  adjust  its  
attention
 
focus
 
to
 
different
 
parts
 
of
 
the
 
input
 
sequence
 
based
 
on
 
the
 
current

---

e
 
input
 
sequence
 
based
 
on
 
the
 
current
 
decoding
 
context.
 ➔  Improved  Performance:  Cross-attention  has  been  shown  to  signiﬁcantly  improve  
the
 
performance
 
of
 
transformer-based
 
models
 
on
 
various
 
NLP
 
tasks,
 
including
 
machine
 
translation,
 
text
 
summarization,
 
and
 
dialogue
 
systems.
  
Q19.  Compare  and  contrast  the  impact  of  using  sparse  (e.g.,  
cross-entropy)
 
and
 
dense
 
(e.g.,
 
mean
 
squared
 
error)
 
loss
 
functions
 
in
 
tr

---

an
 
squared
 
error)
 
loss
 
functions
 
in
 
training
 
language
 
models.
 When  training  language  models,  cross-entropy  loss  and  mean  squared  
error
 
(MSE)
 
loss
 
impact
 
the
 
learning
 
process
 
differently,
 
especially
 
for
 
tasks
 
like
 
text
 
generation
 
or
 
classiﬁcation.  
1.  Cross-Entropy  Loss  (Sparse)  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   61 
 
➢  Cross-entropy  loss  is  used

---

Questions   61 
 
➢  Cross-entropy  loss  is  used  in  classiﬁcation  tasks  and  measures  the  
difference
 
between
 
the
 
predicted
 
probability
 
distribution
 
and
 
the
 
true
 
class
 
label,
 
which
 
is
 
typically
 
a
 
one-hot
 
encoded
 
vector.
 ➢  Characteristics:  ○  Sparse  Targets :  One-hot  encoded  vectors.  ○  Probabilistic  Output :  The  model  predicts  a  probability  distribution  
over
 
tokens.
 ➢  Usage :  Common  in  language  modeling  for  predicting  the  nex

---

in  language  modeling  for  predicting  the  next  token  in  a  
sequence.
 ➢  Advantages:  ○  Well-suited  for  classiﬁcation  tasks.  ○  Aligns  with  probabilistic  language  model  predictions.  ➢  Disadvantages:  ○  Can  penalize  low-probability  predictions  heavily.  ○  May  lead  to  overconﬁdent  predictions.   2.  Mean  Squared  Error  (Dense)  ➢  MSE  calculates  the  squared  difference  between  predicted  and  actual  continuous  
values,
 
often
 
used
 
in
 
regression
 
task

---

values,
 
often
 
used
 
in
 
regression
 
tasks.
 ➢  Characteristics:  ○  Dense  Targets :  Continuous  vectors,  like  embeddings.  ○  Non-Probabilistic  Output :  Works  with  continuous  values,  not  probabilities.  ○  Usage :  Rare  in  language  modeling  but  used  in  generative  models  or  
embedding
 
tasks.
 ➢  Advantages:  ○  Simple  and  stable  for  regression  tasks.  ○  Less  sensitive  to  outliers.  ➢  Disadvantages:  ○  Less  suitable  for  classiﬁcation  tasks.  ○  Inefﬁ

---

ss  suitable  for  classiﬁcation  tasks.  ○  Inefﬁcient  for  tasks  with  large  vocabularies.    Aspect  Cross-Entropy  Loss  Mean  Squared  Error  Loss  
Task  Type  Classiﬁcation  (categorical  prediction)  
Regression  (continuous  output  prediction)  
Output  Format  Probability  distribution  (sparse  targets,  one-hot)  
Continuous  vector  (dense  targets)  
Usage  in  NLP  Common  for  language  models  (e.g.,  text  generation)  
Rare  in  NLP  tasks,  but  can  be  used  in  generat

---

in  NLP  tasks,  but  can  be  used  in  generative  models  or  embedding-based  tasks  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   62 
 
Output  Representation  Discrete  (one-hot  encoded  targets)  
Continuous  (e.g.,  embeddings  or  real  values)  
Optimization  Goal  Minimize  the  difference  between  predicted  probabilities  and  true  class  labels  
Minimize  the  squared  difference  between  predicted  a

---

e  the  squared  difference  between  predicted  and  true  values  
Computational  Complexity  Efﬁcient  for  large  vocabularies,  sparse  targets  
Can  be  computationally  expensive  for  large  vocabularies  due  to  dense  output  representations  
Gradient  Behavior  Can  result  in  large  gradients  for  misclassiﬁed  tokens  (logarithmic  penalty)  
More  stable  and  gradual  gradients  across  predictions  
Sensitivity  to  Outliers  Sensitive  to  outliers  when  probability  for

---

Sensitive  to  outliers  when  probability  for  correct  class  is  low  
Less  sensitive  to  outliers,  but  may  not  handle  categorical  outputs  well   
Q20.  How  can  reinforcement  learning  be  integrated  into  the  training  of  
large
 
language
 
models,
 
and
 
what
 
challenges
 
might
 
arise
 
in
 
selecting
 
suitable
 
loss
 
functions
 
for
 
RL-based
 
approaches? 
  

MD  ARBAAZ  KHAN                                                                Generative  AI  Intervie

---

Generative  AI  Interview  Questions   63 
 
 Phase  1:  Pre-training  In  the  pre-training  phase,  the  LLM  is  trained  on  a  vast  corpus  of  text  data  from  diverse  
sources
 
such
 
as
 
books,
 
articles,
 
and
 
web
 
pages.
 
This
 
data
 
is
 
typically
 
uncurated
 
and
 
may
 
include
 
general
 
knowledge,
 
conversational
 
language,
 
and
 
a
 
variety
 
of
 
topics.
 
The
 
model
 
learns
 
patterns,
 
grammar,
 
facts,
 
reasoning
 
skills,
 
and

---

s,
 
grammar,
 
facts,
 
reasoning
 
skills,
 
and
 
a
 
basic
 
understanding
 
of
 
human
 
language
 
during
 
this
 
phase.
 ●  Objective :  To  give  the  model  a  broad  base  of  knowledge  and  language  skills,  
enabling
 
it
 
to
 
understand
 
and
 
generate
 
text
 
on
 
a
 
wide
 
range
 
of
 
topics.
 ●  Method :  The  model  is  trained  using  self-supervised  learning,  predicting  
masked
 
or
 
next
 
words
 
in
 
sentences
 
from
 
the
 
dataset.
 ●  Outcome :  The  pretrai

---

from
 
the
 
dataset.
 ●  Outcome :  The  pretrained  model  can  generate  ﬂuent  text  but  lacks  alignment  
with
 
speciﬁc
 
human
 
preferences
 
or
 
specialized
 
knowledge
 
required
 
for
 
ﬁne-tuned
 
tasks.
 Phase  2:  Supervised  Fine-Tuning  (SFT)  In  the  supervised  ﬁne-tuning  phase,  the  model  is  further  trained  on  higher-quality,  
carefully
 
curated
 
datasets
 
that
 
include
 
human-provided
 
examples
 
of
 
desired
 
behaviors.
 
These
 
examples
 
can
 
include

---

d
 
behaviors.
 
These
 
examples
 
can
 
include
 
speciﬁc
 
task-oriented
 
dialogues,
 
polite
 
conversational
 
responses,
 
or
 
other
 
behavior
 
that
 
aligns
 
with
 
the
 
model's
 
intended
 
use
 ●  Objective :  To  reﬁne  the  model's  outputs,  aligning  them  more  closely  with  
human-preferred
 
behaviors
 
or
 
domain-speciﬁc
 
knowledge.
 ●  Method :  Supervised  learning  is  used,  where  human  annotators  create  
examples
 
or
 
correct
 
responses
 
that
 
the
 
model

---

s
 
or
 
correct
 
responses
 
that
 
the
 
model
 
should
 
learn
 
to
 
emulate.
 ●  Outcome :  The  ﬁne-tuned  model  is  now  better  suited  for  speciﬁc  applications  
and
 
can
 
respond
 
more
 
accurately
 
and
 
appropriately
 
to
 
user
 
prompts,
 
but
 
it
 
may
 
still
 
lack
 
ﬁne-grained
 
alignment
 
with
 
nuanced
 
human
 
preferences.
 Phase  3:  Reinforcement  Learning  from  Human  Feedback  (RLHF)  In  the  RLHF  phase,  the  model  is  trained  to  optimize  its  respons

---

he  model  is  trained  to  optimize  its  responses  based  on  human  
feedback,
 
integrating
 
reinforcement
 
learning
 
(often
 
using
 
algorithms
 
like
 
Proximal
 
Policy
 
Optimization,
 
or
 
PPO).
 
The
 
model
 
interacts
 
with
 
an
 
environment
 
and
 
receives
 
feedback
 
on
 
its
 
responses,
 
which
 
helps
 
it
 
learn
 
complex,
 
human-preferred
 
behaviors
 
that
 
may
 
not
 
be
 
easily
 
captured
 
by
 
traditional
 
supervised
 
training.
 ●  Objective :  To  further

---

upervised
 
training.
 ●  Objective :  To  further  align  the  model's  outputs  with  nuanced  human  
preferences
 
by
 
iteratively
 
improving
 
its
 
responses
 
based
 
on
 
feedback.
 ●  Method :  The  RL  agent  generates  responses,  and  human  feedback  is  used  to  
model
 
rewards.
 
The
 
model
 
optimizes
 
for
 
these
 
rewards,
 
learning
 
to
 
produce
 
responses
 
that
 
satisfy
 
human
 
evaluators.
 ●  Outcome :  The  RLHF  phase  produces  a  model  that  aligns  more  c

---

phase  produces  a  model  that  aligns  more  closely  with  
human
 
preferences,
 
ethical
 
considerations,
 
and
 
desired
 
behaviors,
 
often
 
yielding
 
safer
 
and
 
more
 
useful
 
responses.
    

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   64 
 
Q21.  In  multimodal  language  models,  how  is  information  from  visual  and  
textual
 
modalities
 
effectively
 
integrated
 
to
 
perform
 
tasks
 
such
 
a

---

ely
 
integrated
 
to
 
perform
 
tasks
 
such
 
as
 
image
 
captioning
 
or
 
visual
 
question
 
answering?
 In  multimodal  language  models,  integrating  information  from  visual  and  textual  modalities  
is
 
key
 
to
 
enabling
 
tasks
 
like
 
image
 
captioning
 
and
 
visual
 
question
 
answering
 
(VQA).
 
The
 
integration
 
process
 
typically
 
follows
 
these
 
steps:
 1.  Feature  Extraction  ●  Visual  Modality:  A  pretrained  convolutional  neural  network  (CNN),  Vision

---

ed  convolutional  neural  network  (CNN),  Vision  
Transformer
 
(ViT),
 
or
 
similar
 
model
 
extracts
 
features
 
from
 
the
 
image,
 
representing
 
it
 
as
 
high-dimensional
 
vectors.
 
These
 
features
 
capture
 
elements
 
like
 
objects,
 
textures,
 
and
 
spatial
 
relations.
 ●  Textual  Modality:  A  language  model,  such  as  a  Transformer-based  encoder,  
processes
 
the
 
text
 
to
 
generate
 
embeddings
 
that
 
represent
 
the
 
meaning,
 
syntax,
 
and
 
semantics

---

ent
 
the
 
meaning,
 
syntax,
 
and
 
semantics
 
of
 
the
 
input
 
question
 
or
 
prompt.
  2.  Cross-Modality  Fusion  ●  Alignment  and  Fusion  Mechanisms:  Once  the  image  and  text  embeddings  
are
 
generated,
 
they
 
are
 
combined
 
using
 
attention-based
 
mechanisms
 
or
 
specialized
 
multimodal
 
transformers,
 
such
 
as
 
CLIP
 
or
 
FLAVA.
 
These
 
models
 
use
 
cross-attention
 
layers
 
that
 
allow
 
the
 
text
 
and
 
visual
 
embeddings
 
to
 
attend
 
to
 
each

---

visual
 
embeddings
 
to
 
attend
 
to
 
each
 
other
,
 
ﬁnding
 
relevant
 
alignments
 
between
 
words
 
and
 
image
 
regions.
 ●  Positional  Embeddings:  For  tasks  where  the  spatial  arrangement  is  
important,
 
positional
 
embeddings
 
are
 
added
 
to
 
image
 
features
 
to
 
encode
 
spatial
 
information,
 
helping
 
the
 
model
 
understand
 
relationships
 
between
 
objects
 
and
 
their
 
positions.
  3.  Joint  Representation  Learning  ●  The  combined  (fused)  embed

---

tation  Learning  ●  The  combined  (fused)  embeddings  are  processed  in  a  shared  feature  space,  
allowing
 
the
 
model
 
to
 
learn
 
meaningful
 
interactions
 
between
 
the
 
two
 
modalities.
 
In
 
this
 
shared
 
space,
 
the
 
model
 
can
 
correlate
 
textual
 
concepts
 
with
 
visual
 
elements
 
and
 
use
 
these
 
connections
 
to
 
infer
 
context
 
and
 
answer
 
questions
 
about
 
the
 
image.
  4.  Task-Speciﬁc  Output  Generation  ●  For  image  captioning,  the  fuse

---

Generation  ●  For  image  captioning,  the  fused  representation  is  passed  through  a  decoder  
that
 
generates
 
a
 
descriptive
 
sentence.
 
The
 
model
 
learns
 
to
 
select
 
and
 
arrange
 
words
 
that
 
match
 
the
 
objects
 
and
 
relationships
 
in
 
the
 
image.
 For  VQA,  the  fused  features  are  used  to  predict  a  textual  answer.  The  model  reasons  over  
the
 
visual
 
elements
 
relevant
 
to
 
the
 
question,
 
combining
 
them
 
with
 
language
 
context
 
to

---

combining
 
them
 
with
 
language
 
context
 
to
 
produce
 
a
 
response.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   65 
 
Q22.  Explain  the  role  of  cross-modal  attention  mechanisms  in  models  
like
 
VisualBERT
 
or
 
CLIP.
 
How
 
do
 
these
 
mechanisms
 
enable
 
the
 
model
 
to
 
capture
 
relationships
 
between
 
visual
 
and
 
textual
 
elements? Cross-modal  attention  mechanisms  are  central  to

---

oss-modal  attention  mechanisms  are  central  to  models  like  VisualBERT  and  CLIP,  as  
they
 
enable
 
these
 
models
 
to
 
capture
 
meaningful
 
relationships
 
between
 
visual
 
and
 
textual
 
elements
 
by
 
allowing
 
each
 
modality
 
(text
 
and
 
image)
 
to
 
attend
 
to
 
relevant
 
aspects
 
of
 
the
 
other.
 
 
 
Here’s
 
how
 
these
 
mechanisms
 
function
 
and
 
why
 
they’re
 
eﬀective
 
in
 
integrating
 
multimodal
 
data: 1.   Cross-Modal  Attention  Fundamentals

---

data: 1.   Cross-Modal  Attention  Fundamentals  ●  In  a  typical  cross-modal  attention  setup,  features  from  the  text  and  visual  
modalities
 
are
 
ﬁrst
 
encoded
 
separately
 
into
 
embeddings.
 
 ●  These  embeddings  then  interact  through  cross-attention  layers,  where  
information
 
from
 
one
 
modality
 
inﬂuences
 
the
 
processing
 
of
 
the
 
other.
 
 ●  This  process  allows  the  model  to  establish  connections  between  words  in  the  
text
 
(like
 
"dog"
 
o

---

between  words  in  the  
text
 
(like
 
"dog"
 
or
 
"next
 
to
 
a
 
lake")
 
and
 
speciﬁc
 
regions
 
or
 
features
 
in
 
the
 
image.
 ●  Cross-modal  attention  mechanisms  allow  these  models  to:  ➢  Focus  on  Relevant  Features:  By  attending  to  speciﬁc  words  and  image  
regions,
 
the
 
model
 
can
 
establish
 
connections
 
between
 
related
 
elements,
 
such
 
as
 
objects,
 
actions,
 
and
 
their
 
attributes
 
(e.g.,
 
“red
 
car”
 
attending
 
to
 
a
 
speciﬁc
 
red

---

red
 
car”
 
attending
 
to
 
a
 
speciﬁc
 
red
 
object).
 ➢  Learn  Contextual  Relationships:  Cross-modal  attention  enables  the  
model
 
to
 
capture
 
context-sensitive
 
relationships,
 
such
 
as
 
spatial
 
relations
 
("next
 
to,"
 
"under")
 
or
 
temporal
 
sequences
 
in
 
visual
 
narratives,
 
by
 
linking
 
parts
 
of
 
the
 
text
 
to
 
visual
 
cues.
 ➢  Resolve  Ambiguity:  Cross-modal  attention  helps  clarify  ambiguous  
elements
 
in
 
either
 
modality.
 
For
 
insta

---

elements
 
in
 
either
 
modality.
 
For
 
instance,
 
if
 
a
 
phrase
 
like
 
"by
 
the
 
river"
 
appears
 
in
 
text,
 
attention
 
to
 
relevant
 
visual
 
regions
 
can
 
resolve
 
which
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   66 
 
part  of  the  image  represents  a  river,  ensuring  that  the  generated  
output
 
or
 
matching
 
aligns
 
with
 
the
 
intended
 
meaning.
  
Role
 
in
 
VisualBERT ➢  Vis

---

tended
 
meaning.
  
Role
 
in
 
VisualBERT ➢  VisualBERT  is  a  model  speciﬁcally  designed  to  combine  language  and  vision.  It  
processes
 
visual
 
and
 
textual
 
information
 
jointly
 
by
 
embedding
 
visual
 
features
 
(from
 
a
 
CNN
 
or
 
similar
 
network)
 
alongside
 
token
 
embeddings
 
from
 
text
 
into
 
a
 
single
 
sequence.
 
 ➢  Cross-attention  in  VisualBERT  enables  each  word  token  to  attend  to  relevant  visual  
regions
 
and
 
vice
 
versa.
 
 ➢  For

---

visual  
regions
 
and
 
vice
 
versa.
 
 ➢  For  example,  in  an  image  captioning  task,  the  word  "sunset"  in  a  caption  might  learn  
to
 
focus
 
on
 
regions
 
of
 
the
 
image
 
with
 
warm
 
colors
 
or
 
a
 
horizon
 
line,
 
allowing
 
the
 
model
 
to
 
create
 
coherent
 
text-image
 
associations.
  
Role
 
in
 
CLIP ➢  CLIP  (Contrastive  Language-Image  Pretraining)  uses  cross-modal  attention  
differently,
 
aiming
 
to
 
match
 
an
 
image
 
with
 
its
 
most
 
releva

---

match
 
an
 
image
 
with
 
its
 
most
 
relevant
 
text
 
caption
 
in
 
a
 
vast
 
dataset.
 
 ➢  CLIP’s  cross-modal  attention  works  by  aligning  the  embeddings  from  the  two  
modalities
 
in
 
a
 
shared
 
latent
 
space
 
through
 
a
 
contrastive
 
learning
 
objective.
 
 ➢  In  this  space,  embeddings  of  related  text  and  image  pairs  are  pulled  closer,  while  
unrelated
 
pairs
 
are
 
pushed
 
apart.
 
 ➢  During  training,  CLIP  learns  nuanced  associations—for

---

aining,  CLIP  learns  nuanced  associations—for  instance,  that  "a  dog  running  
on
 
the
 
beach"
 
refers
 
to
 
images
 
with
 
a
 
beach
 
setting,
 
waves,
 
and
 
a
 
running
 
dog—by
 
using
 
cross-modal
 
attention
 
to
 
align
 
and
 
contrast
 
relevant
 
features
 
across
 
text
 
and
 
images.
  
Q23.  For  tasks  like  image-text  matching,  how  is  the  training  data  
typically
 
annotated
 
to
 
create
 
aligned
 
pairs
 
of
 
visual
 
and
 
textual
 
information,
 
and

---

of
 
visual
 
and
 
textual
 
information,
 
and
 
what
 
considerations
 
should
 
be
 
taken
 
into
 
account?
 For  image-text  matching,  training  data  is  annotated  by  pairing  images  with  accurate  and  
contextually
 
relevant
 
captions
 
or
 
question-answer
 
pairs
 
that
 
describe
 
the
 
content
 
and
 
relationships
 
within
 
the
 
image.
 
Key
 
considerations
 
include:
 ❖  Accuracy  and  Relevance:  Captions  should  focus  on  primary  elements  without  
irrelevant
 
de

---

on  primary  elements  without  
irrelevant
 
details.
 ❖  Diversity:  Including  varied  images  and  descriptions  improves  generalization.  ❖  Speciﬁcity:  Avoid  ambiguous  terms;  use  clear,  detailed  language  when  needed.  ❖  Cultural  Sensitivity:  Ensure  language  respects  diversity  to  avoid  reinforcing  
stereotypes.
 ❖  Contextual  Granularity:  Match  detail  level  to  task  requirements.  

MD  ARBAAZ  KHAN                                                                G

---

Generative  AI  Interview  Questions   67 
 
Challenges  include  the  high  cost  of  annotation,  ensuring  consistency,  and  managing  
abstract
 
concepts
 
like
 
emotions
 
or
 
moods.
  
Q24.  When  training  a  generative  model  for  image  synthesis,  what  are  
common
 
loss
 
functions
 
used
 
to
 
evaluate
 
the
 
difference
 
between
 
generated
 
and
 
target
 
images,
 
and
 
how
 
do
 
they
 
contribute
 
to
 
the
 
training

---

do
 
they
 
contribute
 
to
 
the
 
training
 
process?
 In  training  generative  models  for  image  synthesis,  several  common  loss  functions  are  used  
to
 
measure
 
the
 
difference
 
between
 
generated
 
and
 
target
 
images,
 
each
 
contributing
 
uniquely
 
to
 
the
 
training
 
process:
 1.  Pixel-wise  Loss  (L1  or  L2  Loss)  ➔  Purpose:  Measures  the  direct  difference  between  pixels  of  generated  and  
target
 
images.
 
L1
 
loss
 
calculates
 
the
 
mean
 
absol

---

ges.
 
L1
 
loss
 
calculates
 
the
 
mean
 
absolute
 
difference,
 
while
 
L2
 
calculates
 
the
 
mean
 
squared
 
difference.
 ➔  Contribution:  Encourages  overall  similarity  in  pixel  intensity,  helping  the  
model
 
capture
 
basic
 
shapes
 
and
 
structures.
 
However,
 
it
 
may
 
fail
 
to
 
capture
 
ﬁner
 
details
 
or
 
textures
 
well.
  2.  Perceptual  Loss  ➔  Purpose:  Compares  high-level  features  extracted  from  a  pretrained  network  
(like
 
VGG)
 
instead
 
of

---

etrained  network  
(like
 
VGG)
 
instead
 
of
 
raw
 
pixels.
 ➔  Contribution:  Helps  the  model  focus  on  perceptual  similarity  rather  than  
exact
 
pixel
 
values,
 
capturing
 
texture,
 
style,
 
and
 
structure.
 
This
 
leads
 
to
 
more
 
realistic
 
images,
 
as
 
it
 
aligns
 
better
 
with
 
human
 
visual
 
perception.
  3.  Adversarial  Loss  ➔  Purpose:  Used  in  Generative  Adversarial  Networks  (GANs),  where  a  
discriminator
 
network
 
judges
 
the
 
authenticity

---

iminator
 
network
 
judges
 
the
 
authenticity
 
of
 
generated
 
images.
 
The
 
generator
 
is
 
trained
 
to
 
“fool”
 
the
 
discriminator.
 ➔  Contribution:  Drives  the  model  to  generate  more  realistic  and  detailed  
images,
 
as
 
it
 
encourages
 
diversity
 
and
 
reduces
 
blurriness
 
that
 
may
 
result
 
from
 
pixel-based
 
losses
 
alone.
  4.  Style  and  Content  Loss  ➔  Purpose:  Often  used  in  style  transfer  tasks,  where  content  loss  ensures  
structural
 
si

---

,  where  content  loss  ensures  
structural
 
similarity,
 
and
 
style
 
loss
 
focuses
 
on
 
texture
 
and
 
style
 
consistency
 
by
 
matching
 
feature
 
correlations.
 ➔  Contribution:  This  helps  the  model  synthesize  images  with  a  speciﬁc  artistic  
style
 
while
 
preserving
 
the
 
content’s
 
structure,
 
creating
 
a
 
balance
 
between
 
accurate
 
structure
 
and
 
desired
 
stylistic
 
effects.
 

MD  ARBAAZ  KHAN

---

Generative  AI  Interview  Questions   68 
 
5.  Total  Variation  Loss  ➔  Purpose:  Regularizes  image  smoothness  by  penalizing  abrupt  changes  in  
neighboring
 
pixel
 
values.
 ➔  Contribution:  Reduces  noise  and  enhances  smoothness  in  generated  images,  
preventing
 
overly
 
sharp
 
or
 
“checkerboard”
 
artifacts
 
that
 
sometimes
 
appear
 
during
 
synthesis.
  Each  loss  function  has  strengths  in  guiding  the

---

loss  function  has  strengths  in  guiding  the  model’s  output  quality.  Pixel-wise  loss  
ensures
 
basic
 
similarity,
 
perceptual
 
and
 
style
 
losses
 
capture
 
higher-level
 
features,
 
adversarial
 
loss
 
promotes
 
realism,
 
and
 
total
 
variation
 
loss
 
enhances
 
visual
 
coherence,
 
collectively
 
leading
 
to
 
high-quality,
 
realistic
 
image
 
synthesis.
  
Q25.  What  is  perceptual  loss,  and  how  is  it  utilized  in  image  generation  
tasks
 
to
 
measure

---

d  in  image  generation  
tasks
 
to
 
measure
 
the
 
perceptual
 
similarity
 
between
 
generated
 
and
 
target
 
images?
 
How
 
does
 
it
 
differ
 
from
 
traditional
 
pixel-wise
 
loss
 
functions?
 Perceptual  loss  is  a  type  of  loss  function  used  in  image  generation  tasks  to  measure  the  
perceptual
 
similarity
 
between
 
generated
 
and
 
target
 
images.
 
Unlike
 
traditional
 
pixel-wise
 
loss
 
(like
 
L1
 
or
 
L2
 
loss),
 
which
 
calculates
 
the
 
difference

---

2
 
loss),
 
which
 
calculates
 
the
 
difference
 
based
 
directly
 
on
 
pixel
 
intensity,
 
perceptual
 
loss
 
compares
 
higher-level
 
features
 
that
 
capture
 
visual
 
elements
 
as
 
humans
 
perceive
 
them,
 
such
 
as
 
texture,
 
style,
 
and
 
structural
 
content.
  
How
 
Perceptual
 
Loss
 
Works Perceptual  loss  is  computed  by  passing  both  the  generated  and  target  images  through  a  
pretrained
 
convolutional
 
neural
 
network
 
(CNN),
 
typically
 
one
 
trai

---

neural
 
network
 
(CNN),
 
typically
 
one
 
trained
 
on
 
large
 
datasets
 
like
 
ImageNet.
 
A
 
commonly
 
used
 
model
 
is
 
the
 
VGG
 
network.
 
Key
 
steps
 
include:
 1.  Feature  Extraction:  Both  images  are  fed  into  speciﬁc  layers  of  the  pretrained  CNN  to  
extract
 
high-level
 
feature
 
maps.
  2.  Loss  Calculation:  The  perceptual  loss  is  calculated  by  measuring  the  difference  between  
these
 
feature
 
maps,
 
using
 
L1
 
or
 
L2
 
distance
 
on
 
the

---

aps,
 
using
 
L1
 
or
 
L2
 
distance
 
on
 
the
 
activations
 
from
 
selected
 
layers.
  Since  these  feature  maps  represent  more  abstract  image  content  rather  than  exact  pixel  
values,
 
perceptual
 
loss
 
captures
 
differences
 
that
 
affect
 
human
 
perception,
 
such
 
as
 
whether
 
the
 
textures
 
or
 
shapes
 
in
 
a
 
generated
 
image
 
feel
 
natural
 
and
 
aligned
 
with
 
the
 
target
 
image.
  Utilization  in  Image  Generation   In  tasks  like  super-resolu

---

Image  Generation   In  tasks  like  super-resolution,  style  transfer,  and  image  synthesis,  perceptual  loss  helps  the  
model
 
generate
 
images
 
that
 
look
 
realistic
 
by
 
focusing
 
on
 
capturing
 
perceptual
 
qualities
 
rather
 
than
 
exact
 
pixel-level
 
matching.
 
For
 
example:
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   69 
 
●  Super-Resolution:  Perceptual  loss  helps  ensure  that  the

---

tion:  Perceptual  loss  helps  ensure  that  the  upscaled  image  retains  
realistic
 
details.
 ●  Style  Transfer:  It  ensures  that  the  generated  image  aligns  with  the  artistic  style  or  
texture
 
of
 
a
 
target
 
while
 
preserving
 
content
 
structure.
  
Diﬀerence
 
from
 
Pixel-Wise
 
Loss Pixel-Wise  Loss  (L1/L2):  Measures  exact  pixel-by-pixel  differences,  making  it  effective  for  
structural
 
similarity
 
but
 
prone
 
to
 
producing
 
blurry
 
results,
 
espec

---

prone
 
to
 
producing
 
blurry
 
results,
 
especially
 
in
 
high-frequency,
 
detailed
 
areas.
  Perceptual  Loss:  Captures  high-level  features,  aligning  more  with  human  perception  and  
producing
 
images
 
with
 
more
 
realistic
 
textures
 
and
 
details.
 
It
 
avoids
 
blurriness
 
and
 
helps
 
the
 
model
 
focus
 
on
 
maintaining
 
visual
 
qualities
 
that
 
matter
 
for
 
perceptual
 
similarity.
  
Q26.  What  is  Masked  language-image  modeling?  Masked  language-imag

---

language-image  modeling?  Masked  language-image  modeling  (MLIM)  is  a  training  approach  in  multimodal  machine  
learning
 
that
 
combines
 
elements
 
from
 
both
 
masked
 
language
 
modeling
 
(MLM)
 
and
 
image-based
 
masking
 
techniques
 
to
 
learn
 
representations
 
from
 
both
 
text
 
and
 
visual
 
data.
 
It’s
 
particularly
 
useful
 
in
 
tasks
 
that
 
require
 
understanding
 
and
 
generating
 
natural
 
language
 
descriptions
 
based
 
on
 
images,
 
such
 
as

---

descriptions
 
based
 
on
 
images,
 
such
 
as
 
visual
 
question
 
answering,
 
image
 
captioning,
 
and
 
visual
 
reasoning.
  
How
 
Masked
 
Language-Image
 
Modeling
 
Works
 In  MLIM,  the  model  learns  to  predict  or  reconstruct  masked  elements  within  both  text  and  
image
 
inputs.
 
Here’s
 
how
 
it
 
typically
 
operates:
 1.  Text  Masking:  Words  or  tokens  in  a  text  description  related  to  an  image  are  
randomly
 
masked
 
out.
 
The
 
model
 
must
 
predi

---

domly
 
masked
 
out.
 
The
 
model
 
must
 
predict
 
the
 
missing
 
words
 
based
 
on
 
the
 
context
 
from
 
both
 
the
 
remaining
 
text
 
and
 
associated
 
visual
 
content.
 
This
 
is
 
similar
 
to
 
how
 
masked
 
language
 
models
 
like
 
BERT
 
are
 
trained
 
but
 
with
 
added
 
visual
 
context.
  2.  Image  Masking:  Certain  regions  of  the  image  are  masked,  either  as  patches  (in  Vision  
Transformers)
 
or
 
by
 
removing
 
speciﬁc
 
object-related
 
features
 
(i

---

emoving
 
speciﬁc
 
object-related
 
features
 
(in
 
CNNs
 
or
 
other
 
visual
 
encoders).
 
The
 
model
 
then
 
learns
 
to
 
infer
 
these
 
masked
 
regions
 
based
 
on
 
unmasked
 
parts
 
of
 
the
 
image
 
and
 
the
 
accompanying
 
text.
  3.  Cross-Modality  Fusion:  To  jointly  leverage  visual  and  textual  information,  the  model  
uses
 
cross-modal
 
attention
 
mechanisms
 
that
 
allow
 
masked
 
tokens
 
in
 
the
 
text
 
to
 
attend
 
to
 
unmasked
 
visual
 
features

---

to
 
attend
 
to
 
unmasked
 
visual
 
features
 
(and
 
vice
 
versa).
 
This
 
interaction
 
enables
 
the
 
model
 
to
 
learn
 
complex
 
relationships
 
between
 
objects
 
in
 
the
 
image
 
and
 
their
 
linguistic
 
descriptions.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   70 
 
 
Q27.  How  do  attention  weights  obtained  from  the  cross-attention  
mechanism
 
inﬂuence
 
the
 
generation
 
process
 
in
 
m

---

m
 
inﬂuence
 
the
 
generation
 
process
 
in
 
multimodal
 
models?
 
What
 
role
 
do
 
these
 
weights
 
play
 
in
 
determining
 
the
 
importance
 
of
 
different
 
modalities? In  multimodal  models,  attention  weights  from  the  cross-attention  mechanism  guide  the  
generation
 
process
 
by
 
determining
 
which
 
elements
 
of
 
each
 
modality
 
(text
 
and
 
image)
 
are
 
most
 
relevant
 
at
 
each
 
step.
 1.  Selective  Focus:  Attention  weights  allow  the  model  to  focu

---

:  Attention  weights  allow  the  model  to  focus  on  important  features,  
such
 
as
 
relevant
 
image
 
regions
 
for
 
speciﬁc
 
text
 
queries.
  2.  Balancing  Modal  Contributions:  They  help  balance  the  inﬂuence  of  text  and  visual  
information,
 
dynamically
 
adjusting
 
based
 
on
 
context.
 
For
 
example,
 
in
 
image
 
captioning,
 
the
 
model
 
may
 
prioritize
 
visual
 
details,
 
while
 
in
 
question
 
answering,
 
it
 
might
 
emphasize
 
text.
  3.  Improving

---

it
 
might
 
emphasize
 
text.
  3.  Improving  Accuracy:  By  directing  attention  to  relevant  features,  the  model  
generates
 
more
 
accurate
 
and
 
coherent
 
outputs,
 
especially
 
in
 
tasks
 
requiring
 
multimodal
 
reasoning.
  4.  Learning  Associations:  Attention  weights  help  the  model  learn  associations  between  
words
 
and
 
image
 
regions,
 
improving
 
alignment
 
for
 
tasks
 
like
 
retrieval
 
or
 
caption
 
generation.
  In  essence,  attention  weights  en

---

generation.
  In  essence,  attention  weights  ensure  the  model  effectively  combines  and  prioritizes  
information
 
from
 
both
 
modalities,
 
improving
 
output
 
quality
 
and
 
coherence.
  
Q28.  What  are  the  unique  challenges  in  training  multimodal  generative  
models
 
compared
 
to
 
unimodal
 
generative
 
models? Training  multimodal  generative  models,  which  process  and  generate  outputs  across  
multiple
 
data
 
types
 
(e.g.,
 
images,
 
text,
 
audio),
 
pres

---

types
 
(e.g.,
 
images,
 
text,
 
audio),
 
presents
 
several
 
unique
 
challenges
 
that
 
are
 
generally
 
not
 
as
 
pronounced
 
in
 
unimodal
 
models.
 
  
Some
 
of
 
the
 
key
 
challenges
 
include:
 1.  Aligning  Representations  Across  Modalities:  Multimodal  models  need  to  create  a  
shared
 
representation
 
that
 
aligns
 
information
 
across
 
different
 
types
 
of
 
data,
 
like
 
text,
 
images,
 
and
 
audio.
 
Achieving
 
this
 
alignment
 
is
 
complex,
 
as
 
ea

---

eving
 
this
 
alignment
 
is
 
complex,
 
as
 
each
 
modality
 
has
 
distinct
 
structures
 
and
 
features.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   71 
 
2.  Data  Quality  and  Balance:  Multimodal  training  relies  on  paired  data  (e.g.,  image-text  
pairs),
 
which
 
can
 
be
 
scarce
 
or
 
inconsistent.
 
High-quality,
 
balanced
 
datasets
 
are
 
harder
 
to
 
ﬁnd,
 
and
 
misalignment
 
or
 
biases

---

er
 
to
 
ﬁnd,
 
and
 
misalignment
 
or
 
biases
 
in
 
data
 
can
 
hinder
 
the
 
model’s
 
performance.
 3.  Increased  Computational  Demand:  Multimodal  models  generally  require  more  
resources
 
due
 
to
 
complex
 
architectures
 
that
 
integrate
 
different
 
processing
 
components
 
for
 
each
 
modality
 
(e.g.,
 
transformers
 
for
 
text,
 
CNNs
 
for
 
images).
 
This
 
raises
 
the
 
computational
 
and
 
memory
 
costs.
 4.  Maintaining  Cross-Modal  Consistency:  Ensuring

---

Maintaining  Cross-Modal  Consistency:  Ensuring  coherence  across  generated  
outputs
 
is
 
challenging,
 
like
 
making
 
sure
 
an
 
image
 
aligns
 
with
 
a
 
descriptive
 
text
 
prompt.
 
Any
 
inconsistency
 
across
 
modalities
 
can
 
lead
 
to
 
confusing
 
or
 
inaccurate
 
outputs.
 5.  Handling  Modality-Speciﬁc  Noise:  Each  modality  has  unique  noise  and  ambiguities.  
For
 
instance,
 
images
 
might
 
have
 
visual
 
artifacts,
 
while
 
text
 
could
 
be
 
vague.
 
T

---

rtifacts,
 
while
 
text
 
could
 
be
 
vague.
 
The
 
model
 
must
 
account
 
for
 
these
 
without
 
letting
 
one
 
modality
 
dominate.
 6.  Architectural  Design  and  Parameter  Sharing:  Balancing  shared  and  
modality-speciﬁc
 
parameters
 
is
 
tricky—too
 
much
 
sharing
 
can
 
limit
 
specialization,
 
while
 
too
 
little
 
can
 
hinder
 
cross-modal
 
learning.
 7.  Evaluation  Challenges:  Assessing  multimodal  outputs  requires  metrics  that  account  
for
 
both
 
single-mo

---

s  metrics  that  account  
for
 
both
 
single-modal
 
quality
 
and
 
alignment
 
across
 
modalities,
 
which
 
are
 
more
 
complex
 
to
 
design
 
than
 
traditional
 
metrics.
  
Q29.  How  do  multimodal  generative  models  address  the  issue  of  data  
sparsity
 
in
 
training? To  address  data  sparsity,  multimodal  generative  models  often  rely  on  several  techniques  to  
maximize
 
the
 
use
 
of
 
limited,
 
paired
 
multimodal
 
data.
 
Key
 
methods
 
include:
 1.  Pretra

---

dal
 
data.
 
Key
 
methods
 
include:
 1.  Pretraining  on  Unimodal  Data:  By  pretraining  on  large  unimodal  datasets  (e.g.,  
text-only
 
or
 
image-only
 
data),
 
models
 
can
 
learn
 
foundational
 
features
 
for
 
each
 
modality
 
independently.
 
This
 
reduces
 
the
 
need
 
for
 
vast
 
multimodal
 
datasets
 
by
 
allowing
 
the
 
model
 
to
 
later
 
ﬁne-tune
 
on
 
paired
 
data
 
with
 
a
 
better
 
understanding
 
of
 
each
 
individual
 
modality.
 2.  Cross-Modal  Trans

---

h
 
individual
 
modality.
 2.  Cross-Modal  Transfer  Learning:  Transfer  learning  leverages  knowledge  from  one  
modality
 
to
 
beneﬁt
 
others.
 
For
 
instance,
 
models
 
trained
 
on
 
extensive
 
text
 
data
 
can
 
transfer
 
language
 
understanding
 
to
 
tasks
 
requiring
 
text-image
 
pairs,
 
reducing
 
dependence
 
on
 
paired
 
datasets.
 3.  Data  Augmentation:  Multimodal  models  use  augmentation  techniques  that  generate  
synthetic
 
multimodal
 
pairs.
 
For
 
exam

---

ate  
synthetic
 
multimodal
 
pairs.
 
For
 
example,
 
back-translation
 
can
 
create
 
alternative
 
text
 
descriptions,
 
or
 
GANs
 
can
 
generate
 
new
 
image
 
samples.
 
These
 
methods
 
increase
 
the
 
diversity
 
of
 
the
 
training
 
data
 
and
 
help
 
the
 
model
 
generalize
 
better.
 4.  Weak  Supervision  and  Noisy  Labeling:  Using  loosely  related  data  with  less  strict  
alignment
 
(e.g.,
 
web
 
data
 
or
 
partially
 
aligned
 
image-text
 
pairs)
 
allows
 
the

---

lly
 
aligned
 
image-text
 
pairs)
 
allows
 
the
 
model
 
to
 
learn
 
useful
 
multimodal
 
patterns
 
without
 
requiring
 
perfect
 
pairing.
 
Techniques
 
like
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   72 
 
noisy  student  training  or  teacher-student  models  further  reﬁne  performance  from  
weakly
 
supervised
 
data.
 5.  Self-Supervised  Learning:  Self-supervised  learning  leverages  unlabeled  dat

---

lf-supervised  learning  leverages  unlabeled  data  by  
creating
 
tasks
 
that
 
help
 
the
 
model
 
learn
 
cross-modal
 
relationships
 
without
 
strict
 
pairing.
 
For
 
example,
 
contrastive
 
learning
 
can
 
train
 
the
 
model
 
to
 
identify
 
corresponding
 
pairs
 
across
 
modalities,
 
enhancing
 
cross-modal
 
understanding
 
even
 
with
 
limited
 
paired
 
data.
 6.  Multimodal  Fusion  Techniques:  Models  can  learn  to  fuse  unimodal  embeddings,  
allowing
 
them
 
to

---

use  unimodal  embeddings,  
allowing
 
them
 
to
 
adaptively
 
combine
 
information
 
from
 
each
 
modality
 
when
 
paired
 
data
 
is
 
limited.
 
By
 
dynamically
 
leveraging
 
complementary
 
information,
 
fusion
 
techniques
 
help
 
reduce
 
reliance
 
on
 
large
 
paired
 
datasets.
  
Q30.  Explain  the  concept  of  Vision-Language  Pre-training  (VLP)  and  its  
signiﬁcance
 
in
 
developing
 
robust
 
vision-language
 
models. Vision-Language  Pre-training  (VLP)  trains  model

---

ision-Language  Pre-training  (VLP)  trains  models  on  large  datasets  of  paired  images  and  
text,
 
enabling
 
them
 
to
 
understand
 
and
 
generate
 
content
 
across
 
both
 
modalities.
  
Key
 
Aspects 1.  Joint  Vision-Language  Representation:  VLP  creates  a  shared  embedding  space  for  
images
 
and
 
text,
 
helping
 
models
 
align
 
visual
 
and
 
language
 
data.
 2.  Cross-Modal  Learning  Tasks:  It  includes  tasks  like  image-text  matching  and  
masked
 
language

---

ike  image-text  matching  and  
masked
 
language
 
modeling
 
to
 
teach
 
the
 
model
 
connections
 
between
 
visuals
 
and
 
text.
 3.  Large  Paired  Data:  VLP  uses  diverse,  real-world  image-text  datasets,  allowing  
models
 
to
 
generalize
 
better
 
across
 
tasks.
  
Signiﬁcance
 1.  Enhanced  Cross-Modal  Understanding:  Models  can  relate  visual  elements  to  
language,
 
useful
 
for
 
tasks
 
like
 
image
 
captioning.
 2.  Robust  Generalization:  VLP  enables  models

---

.  Robust  Generalization:  VLP  enables  models  to  transfer  knowledge  across  tasks  with  
minimal
 
data.
 3.  Foundation  for  Modern  Vision-Language  Models:  VLP  powers  advanced  
applications,
 
such
 
as
 
search
 
and
 
augmented
 
reality,
 
by
 
equipping
 
models
 
to
 
work
 
with
 
both
 
images
 
and
 
text.
  
Q31.   How  do  models  like  CLIP  and  DALL-E  demonstrate  the  integration  
of
 
vision
 
and
 
language
 
modalities? 

MD  ARBAAZ  KHAN

---

alities? 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   73 
 
CLIP  and  DALL-E  are  groundbreaking  models  from  OpenAI  that  demonstrate  the  
integration
 
of
 
vision
 
and
 
language
 
modalities
 
through
 
multimodal
 
training
 
and
 
shared
 
embeddings,
 
allowing
 
them
 
to
 
process
 
and
 
link
 
visual
 
and
 
textual
 
information.
  
1.
 
CLIP
 
(Contrastive
 
Language–Image
 
Pretraining) CLIP  is  de

---

stive
 
Language–Image
 
Pretraining) CLIP  is  designed  to  learn  a  shared  embedding  space  for  both  images  and  text.  During  
training,
 
it
 
learns
 
to
 
associate
 
an
 
image
 
with
 
a
 
text
 
caption
 
that
 
describes
 
it.
 
Speciﬁcally:
 ➢  Contrastive  Learning:  CLIP  is  trained  on  a  massive  dataset  of  image-text  pairs,  
learning
 
to
 
pull
 
related
 
image-text
 
embeddings
 
closer
 
together
 
and
 
push
 
unrelated
 
ones
 
farther
 
apart.
 
The
 
result

---

nrelated
 
ones
 
farther
 
apart.
 
The
 
result
 
is
 
a
 
model
 
that
 
can
 
understand
 
visual
 
concepts
 
and
 
their
 
descriptions
 
in
 
natural
 
language.
 ➢  Zero-Shot  Capabilities:  Because  of  its  ability  to  link  visual  and  textual  information  in  
a
 
shared
 
space,
 
CLIP
 
can
 
perform
 
zero-shot
 
classiﬁcation
 
across
 
a
 
wide
 
range
 
of
 
image
 
categories
 
without
 
needing
 
further
 
ﬁne-tuning.
 
For
 
example,
 
it
 
can
 
correctly
 
match
 
image

---

example,
 
it
 
can
 
correctly
 
match
 
images
 
with
 
descriptions
 
even
 
if
 
it
 
hasn’t
 
been
 
trained
 
on
 
those
 
speciﬁc
 
images
 
or
 
categories.
 
2.
 
DALL-E
 DALL-E  builds  on  this  vision-language  integration  but  focuses  on  generation  rather  than  
classiﬁcation.
 
It
 
takes
 
text
 
prompts
 
and
 
generates
 
novel
 
images
 
that
 
match
 
the
 
description.
 ➢  Text-to-Image  Generation:  DALL-E  is  trained  to  create  realistic  and  creative  images

---

d  to  create  realistic  and  creative  images  
from
 
textual
 
descriptions.
 
It
 
learns
 
associations
 
between
 
language
 
and
 
visual
 
elements,
 
like
 
objects,
 
colors,
 
styles,
 
and
 
compositions,
 
so
 
it
 
can
 
generate
 
images
 
that
 
align
 
with
 
speciﬁc
 
prompts.
 ➢  Understanding  Complex  Relationships:  Through  its  training,  DALL-E  can  interpret  
complex
 
and
 
nuanced
 
language
 
instructions.
 
For
 
example,
 
it
 
can
 
combine
 
attributes
 
and

---

example,
 
it
 
can
 
combine
 
attributes
 
and
 
styles
 
to
 
create
 
a
 
unique
 
image,
 
like
 
a
 
"two-story
 
house
 
shaped
 
like
 
a
 
shoe"
 
or
 
a
 
"cat
 
in
 
a
 
spacesuit
 
on
 
Mars."
  
Q32.  How  do  attention  mechanisms  enhance  the  performance  of  
vision-language
 
models?
 Attention  mechanisms  signiﬁcantly  enhance  the  performance  of  vision-language  models  
by
 
selectively
 
focusing
 
on
 
the
 
most
 
relevant
 
parts
 
of
 
input
 
data.
 
In
 
multimod

---

evant
 
parts
 
of
 
input
 
data.
 
In
 
multimodal
 
models,
 
attention
 
mechanisms
 
are
 
pivotal
 
for
 
linking
 
visual
 
and
 
textual
 
information,
 
allowing
 
the
 
model
 
to
 
manage
 
the
 
complexity
 
of
 
multimodal
 
integration.
 
  
Here’s
 
how
 
attention
 
mechanisms
 
contribute
 
to
 
vision-language
 
models: 1.  Selective  Focus  on  Relevant  Features  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Quest

---

Generative  AI  Interview  Questions   74 
 
❖  Attention  mechanisms  enable  the  model  to  prioritize  certain  elements  of  an  image  
or
 
text
 
based
 
on
 
context.
 
 ❖  For  example,  if  a  caption  mentions  "a  dog  in  the  park,"  attention  can  help  the  model  
focus
 
on
 
the
 
part
 
of
 
the
 
image
 
that
 
includes
 
the
 
dog
 
and
 
the
 
surrounding
 
park
 
rather
 
than
 
irrelevant
 
areas.
 
This
 
selective
 
focus
 
improves
 
comprehension

---

is
 
selective
 
focus
 
improves
 
comprehension
 
and
 
relevance.
   2.  Cross-Attention  for  Modality  Fusion  In  vision-language  models,  cross-attention  enables  interaction  between  the  visual  and  
textual
 
data.
 
For
 
instance:
  ❖  Cross-Attention  Layers:  These  layers  allow  the  model  to  match  and  align  speciﬁc  
words
 
with
 
corresponding
 
visual
 
regions.
 
This
 
helps
 
the
 
model
 
understand
 
which
 
parts
 
of
 
the
 
image
 
are
 
associated
 
with
 
w

---

s
 
of
 
the
 
image
 
are
 
associated
 
with
 
which
 
parts
 
of
 
the
 
text,
 
crucial
 
for
 
tasks
 
like
 
image
 
captioning
 
and
 
visual
 
question
 
answering.
 ❖  Enhanced  Multimodal  Representation:  By  letting  textual  and  visual  representations  
"attend"
 
to
 
each
 
other,
 
cross-attention
 
effectively
 
merges
 
features
 
from
 
both
 
domains,
 
producing
 
more
 
nuanced
 
and
 
contextually
 
aligned
 
embeddings
 
that
 
improve
 
overall
 
model
 
accuracy
 
and

---

that
 
improve
 
overall
 
model
 
accuracy
 
and
 
coherence.
  3.  Improved  Contextual  Understanding  ❖  Attention  mechanisms,  especially  through  transformers,  enhance  contextual  
awareness
 
within
 
both
 
modalities.
 
They
 
allow
 
the
 
model
 
to
 
understand
 
dependencies
 
and
 
relationships
 
between
 
words
 
in
 
a
 
sentence
 
(e.g.,
 
recognizing
 
that
 
"red
 
ball"
 
refers
 
to
 
a
 
color-object
 
pair)
 
and
 
spatial
 
relations
 
in
 
images
 
(e.g.,
 
identif

---

atial
 
relations
 
in
 
images
 
(e.g.,
 
identifying
 
objects
 
next
 
to
 
or
 
behind
 
others).
 
This
 
enriched
 
context
 
improves
 
the
 
model’s
 
comprehension
 
of
 
complex
 
language
 
descriptions
 
or
 
visually
 
dense
 
scenes.
  4.  Handling  Complexity  and  Reducing  Noise  ❖  With  multimodal  data,  there  can  be  a  lot  of  irrelevant  or  distracting  information.  
Attention
 
mechanisms
 
help
 
ﬁlter
 
out
 
irrelevant
 
details,
 
allowing
 
the
 
model
 
to
 
at

---

vant
 
details,
 
allowing
 
the
 
model
 
to
 
attend
 
to
 
information
 
crucial
 
for
 
a
 
given
 
task,
 
whether
 
that’s
 
identifying
 
an
 
object
 
in
 
an
 
image
 
or
 
generating
 
a
 
description.
 
This
 
results
 
in
 
more
 
efﬁcient
 
computation
 
and
 
enhanced
 
performance.
         

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   75 
 
                   
Fundamental  of  LLMs         
  

MD  ARBAAZ

---

Fundamental  of  LLMs         
  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   76 
 
Q1.  Describe  your  experience  working  with  text  generation  using  
generative
 
models. I've  worked  with  various  text  generation  models  similar  to  Cohere,  LLaMA  3.1,  and  LLaMA  
3.2
 
Vision,
 
accessed
 
through
 
platforms
 
like
 
Cohere,
 
Nvidia,
 
and
 
Ollama.
 
My
 
experience
 
includes
 
prompt
 
engineer

---

.
 
My
 
experience
 
includes
 
prompt
 
engineering
 
and
 
ﬁne-tuning
 
to
 
adapt
 
models
 
for
 
speciﬁc
 
needs,
 
such
 
as
 
contextual
 
understanding
 
and
 
multimodal
 
tasks.
 
With
 
models
 
like
 
LLaMA
 
Vision,
 
I've
 
explored
 
generating
 
descriptive
 
text
 
linked
 
to
 
images,
 
enhancing
 
visual-text
 
comprehension.
 
I
 
also
 
focus
 
on
 
evaluating
 
output
 
for
 
accuracy,
 
creativity,
 
and
 
ﬂuency,
 
ensuring
 
the
 
generated
 
text
 
aligns
 
well
 
wit

---

ing
 
the
 
generated
 
text
 
aligns
 
well
 
with
 
user
 
expectations
 
and
 
task
 
requirements.
  
Q2.  Could  you  illustrate  the  fundamental  differences  between  
discriminative
 
and
 
generative
 
models? Discriminative  and  generative  models  differ  in  their  objectives  and  applications:   Discriminative  Models:  Learn  to  distinguish  between  classes  by  modeling  ,  the  probability  of  a  label   given  data  .  Examples  include  logistic  regression  and  SVMs.  T

---

ples  include  logistic  regression  and  SVMs.  They’re  
used
 
mainly
 
for
 
classiﬁcation
 
tasks
 
and
 
tend
 
to
 
be
 
simpler
 
and
 
more
 
accurate
 
for
 
these
 
purposes.
  Generative  Models:  Learn  how  data  is  generated  by  modeling    or  .  Examples  include  Naive  Bayes,  GANs,  and  VAEs.  They’re  capable  of  generating  new  data  and  
are
 
useful
 
for
 
tasks
 
like
 
image
 
synthesis
 
and
 
anomaly
 
detection
 
but
 
can
 
be
 
more
 
complex
 
to
 
train.

---

but
 
can
 
be
 
more
 
complex
 
to
 
train.
  In  essence,  discriminative  models  classify  data,  while  generative  models  can  create  it.   
Q3.  What  is  multimodal  AI,  and  why  is  it  important  in  modern  machine  
learning
 
applications?
 Multimodal  AI  refers  to  models  that  integrate  and  process  multiple  types  of  data  (e.g.,  text,  
images,
 
audio)
 
to
 
improve
 
understanding
 
and
 
decision-making.
 
It’s
 
important
 
because:
  1.  Enhanced  Context:

---

s
 
important
 
because:
  1.  Enhanced  Context:  Combines  data  from  different  sources  to  improve  comprehension  
(e.g.,
 
combining
 
image
 
and
 
text
 
for
 
better
 
understanding).
  2.  Improved  Accuracy:  Uses  multiple  modalities  to  compensate  for  the  weaknesses  of  a  
single
 
source
 
(e.g.,
 
combining
 
audio
 
and
 
visual
 
cues
 
for
 
better
 
speech
 
recognition).
   

MD  ARBAAZ  KHAN                                                                Generative

---

Generative  AI  Interview  Questions   77 
 
3.  Real-World  Applications:  Used  in  tasks  like  autonomous  vehicles,  healthcare,  and  
customer
 
service,
 
where
 
multiple
 
data
 
types
 
are
 
essential.
  4.  Generative  Capabilities:  Enables  tasks  like  text-to-image  generation  (e.g.,  DALL-E).   5.  Richer  Interactions:  Enhances  user  experiences  in  applications  like  virtual  assistants  by  
combining
 
speech
 
and
 
visual
 
under

---

ts  by  
combining
 
speech
 
and
 
visual
 
understanding.
  
Q4.  Discuss  how  multimodal  AI  combines  different  types  of  data  to  
improve
 
model
 
performance,
 
enhance
 
user
 
experience,
 
and
 
provide
 
richer
 
context
 
for
 
decision-making
 
in
 
applications
 
like
 
search
 
engines
 
and
 
virtual
 
assistants. Multimodal  AI  enhances  model  performance,  user  experience,  and  decision-making  by  
combining
 
different
 
types
 
of
 
data
 
(text,
 
images,
 
audio)

---

nt
 
types
 
of
 
data
 
(text,
 
images,
 
audio)
 
in
 
applications
 
like
 
search
 
engines
 
and
 
virtual
 
assistants:
  1.  Improved  Performance:  Combines  modalities  (e.g.,  text  and  images)  to  better  understand  
queries,
 
leading
 
to
 
more
 
accurate
 
results.
  2.  Enhanced  User  Experience:  Enables  natural,  context-aware  interactions,  like  virtual  
assistants
 
using
 
speech
 
and
 
visual
 
inputs
 
for
 
tailored
 
responses.
  3.  Richer  Context  for  Decis

---

red
 
responses.
  3.  Richer  Context  for  Decision-Making:  Integrates  data  from  various  sources  (e.g.,  medical  
images
 
and
 
patient
 
history)
 
for
 
more
 
informed
 
decisions.
  4.  Personalization:  Offers  personalized  results  based  on  voice,  facial  expressions,  or  other  
inputs
 
in
 
search
 
engines.
  
Q5.  Can  you  explain  the  concept  of  cross-modal  learning  and  provide  
examples
 
of
 
how
 
it
 
is
 
applied? Cross-modal  learning  is  a  machine  lea

---

pplied? Cross-modal  learning  is  a  machine  learning  approach  where  models  learn  to  link  and  
transfer
 
knowledge
 
between
 
different
 
data
 
types
 
(modalities),
 
such
 
as
 
text,
 
images,
 
and
 
audio.
  
Key
 
Features: Data  Integration:  Combining  different  modalities  to  create  richer  representations.  Cross-Attention:  Aligns  relevant  parts  of  different  modalities  to  improve  predictions.  
 

MD  ARBAAZ  KHAN

---

AN                                                                Generative  AI  Interview  Questions   78 
 
Examples: 1.  Image  Captioning:  Generating  text  descriptions  from  images  (e.g.,  describing  a  dog  in  a  
ﬁeld).
 2.  Visual  Question  Answering  (VQA):  Answering  questions  about  an  image  (e.g.,  “What  is  in  
the
 
sky?”).
 3.  Text-to-Image  Generation:  Creating  images  from  text  descriptions  (e.g.,  “A  cat  wearing  a  
hat”).
 4.  Speech-to-Text:  Converting

---

aring  a  
hat”).
 4.  Speech-to-Text:  Converting  speech  (audio)  to  text  and  vice  versa.  5.  Cross-Modal  Retrieval:  Finding  images  based  on  a  text  query  or  vice  versa.   
Q6.  Explore  how  cross-modal  learning  enables  models  to  leverage  
information
 
from
 
one
 
modality
 
(e.g.,
 
text)
 
to
 
improve
 
understanding
 
in
 
another
 
(e.g.,
 
images),
 
citing
 
applications
 
such
 
as
 
image
 
captioning
 
or
 
visual
 
question
 
answering. Cross-modal  learning

---

sual
 
question
 
answering. Cross-modal  learning  allows  models  to  learn  and  leverage  information  across  different  
modalities—such
 
as
 
text,
 
images,
 
and
 
audio—enhancing
 
their
 
ability
 
to
 
process
 
and
 
understand
 
data
 
across
 
these
 
diverse
 
forms.
 
By
 
linking
 
multiple
 
modalities,
 
models
 
can
 
gain
 
a
 
richer
 
context,
 
often
 
leading
 
to
 
improved
 
performance
 
and
 
generalization
 
in
 
tasks
 
that
 
require
 
a
 
combination
 
of
 
inp

---

asks
 
that
 
require
 
a
 
combination
 
of
 
inputs.
 
  
Here’s
 
how
 
it
 
works,
 
with
 
applications
 
like
 
image
 
captioning
 
and
 
visual
 
question
 
answering
 
(VQA):
 1.  Image  Captioning  ➔  In  image  captioning,  a  model  generates  textual  descriptions  for  images  by  learning  
the
 
associations
 
between
 
visual
 
features
 
(such
 
as
 
objects,
 
actions,
 
and
 
scenes)
 
and
 
corresponding
 
words
 
or
 
phrases.
 
Cross-modal
 
learning
 
enables
 
the
 
mode

---

s.
 
Cross-modal
 
learning
 
enables
 
the
 
model
 
to
 
map
 
visual
 
inputs
 
to
 
semantic
 
text
 
outputs
 
by
 
jointly
 
training
 
on
 
paired
 
image-caption
 
datasets.
 
 ➔  Here,  visual  encoders  (e.g.,  CNNs)  extract  image  features,  while  language  decoders  
(e.g.,
 
RNNs
 
or
 
transformers)
 
translate
 
these
 
features
 
into
 
coherent
 
sentences.
 
This
 
modality
 
alignment
 
helps
 
the
 
model
 
to
 
generate
 
more
 
descriptive
 
and
 
contextually
 
relevant

---

more
 
descriptive
 
and
 
contextually
 
relevant
 
captions,
 
improving
 
human
 
interpretation
 
and
 
interaction.
  2.  Visual  Question  Answering  (VQA)  ➔  VQA  requires  a  model  to  answer  questions  about  an  image,  combining  image  
understanding
 
with
 
natural
 
language
 
comprehension.
 
 ➔  Cross-modal  learning  is  essential  here,  as  the  model  needs  to  interpret  both  the  
visual
 
content
 
and
 
the
 
linguistic
 
context
 
of
 
the
 
question.
 

MD  ARBAAZ

---

tic
 
context
 
of
 
the
 
question.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   79 
 
➔   Typically,  an  encoder  processes  the  image,  another  encodes  the  question,  and  a  
cross-modal
 
attention
 
mechanism
 
combines
 
these
 
representations,
 
enabling
 
the
 
model
 
to
 
focus
 
on
 
image
 
regions
 
relevant
 
to
 
the
 
question.
 ➔  This  approach  allows  the  model  to  learn  question-answer  pa

---

allows  the  model  to  learn  question-answer  pairs  linked  to  speciﬁc  
visual
 
details,
 
thereby
 
improving
 
its
 
accuracy
 
and
 
relevancy
 
in
 
answers.
   3.  Underlying  Mechanisms  in  Cross-Modal  Learning  ➔  Key  to  cross-modal  learning  is  attention  mechanisms  and  contrastive  learning.  
Attention
 
mechanisms
 
enable
 
the
 
model
 
to
 
weigh
 
information
 
across
 
modalities,
 
helping
 
it
 
prioritize
 
relevant
 
features
 
(e.g.,
 
focusing
 
on
 
an
 
obje

---

nt
 
features
 
(e.g.,
 
focusing
 
on
 
an
 
object
 
in
 
an
 
image
 
that
 
relates
 
to
 
a
 
question).
 
 ➔  Contrastive  learning  helps  align  and  differentiate  representations  across  modalities,  
strengthening
 
associations
 
(e.g.,
 
aligning
 
text
 
"dog"
 
with
 
images
 
containing
 
dogs
 
while
 
distinguishing
 
it
 
from
 
"cat").
 
These
 
techniques
 
enable
 
models
 
to
 
effectively
 
“transfer”
 
knowledge
 
from
 
one
 
modality
 
to
 
another,
 
improving
 
gene

---

one
 
modality
 
to
 
another,
 
improving
 
generalization
 
across
 
tasks.
  Through  cross-modal  learning,  models  gain  a  more  holistic  understanding,  leading  to  
applications
 
beyond
 
image
 
captioning
 
and
 
VQA,
 
such
 
as
 
multimodal
 
search
 
engines,
 
assistive
 
technologies,
 
and
 
enhanced
 
AI
 
in
 
areas
 
like
 
autonomous
 
vehicles
 
and
 
medical
 
imaging.
 
By
 
aligning
 
visual
 
and
 
textual
 
representations,
 
cross-modal
 
learning
 
offers
 
a
 
p

---

tations,
 
cross-modal
 
learning
 
offers
 
a
 
pathway
 
to
 
more
 
robust,
 
versatile,
 
and
 
contextually
 
aware
 
AI.
  
Q7.  What  are  some  common  challenges  faced  in  developing  multimodal  
models,
 
and
 
how
 
can
 
they
 
be
 
addressed? Developing  multimodal  models  faces  several  key  challenges,  including:  1.  Data  Alignment  and  Quality  Aligning  data  across  modalities  (e.g.,  pairing  relevant  images  with  text)  is  difﬁcult,  especially  
when
 
data
 
so

---

ext)  is  difﬁcult,  especially  
when
 
data
 
sources
 
vary
 
in
 
structure
 
or
 
quality.
 
Misalignment
 
can
 
lead
 
to
 
poor
 
performance
 
or
 
misinterpretation
 
in
 
tasks.
 ➔  Solution:  Using  paired  datasets  (e.g.,  images  with  corresponding  captions)  and  
techniques
 
like
 
self-supervised
 
learning
 
on
 
large,
 
unpaired
 
data
 
can
 
help
 
models
 
learn
 
associations
 
more
 
effectively.
  2.  Modality  Imbalance  Some  modalities  may  have  richer  data  o

---

ance  Some  modalities  may  have  richer  data  or  more  deﬁned  structures  than  others,  causing  the  
model
 
to
 
favor
 
one
 
modality
 
over
 
another.
 ➔  Solution:  Applying  modality  dropout  (intentionally  removing  one  modality  during  
training)
 
or
 
attention
 
mechanisms
 
that
 
balance
 
focus
 
across
 
modalities
 
can
 
help
 
the
 
model
 
learn
 
to
 
integrate
 
all
 
available
 
information.
  

MD  ARBAAZ  KHAN

---

Generative  AI  Interview  Questions   80 
 
3.  Computational  Complexity  Processing  multiple  modalities  requires  signiﬁcant  computational  resources,  especially  
when
 
handling
 
large
 
datasets
 
or
 
complex
 
architectures.
 ➔  Solution:  Model  optimization  techniques  like  parameter  sharing  across  modalities,  
efﬁcient
 
cross-modal
 
transformers,
 
and
 
knowledge
 
distillation
 
can
 
help
 
reduce
 
comput

---

edge
 
distillation
 
can
 
help
 
reduce
 
computational
 
demands
 
while
 
maintaining
 
performance.
  4.  Fusion  of  Heterogeneous  Representations  Combining  representations  from  different  modalities,  such  as  visual  and  textual  
embeddings,
 
can
 
be
 
challenging
 
due
 
to
 
differences
 
in
 
feature
 
space
 
and
 
dimensionality.
 ➔  Solution:  Cross-attention  mechanisms  and  feature  alignment  layers  (which  map  
features
 
from
 
different
 
modalities
 
to
 
a
 
co

---

ures
 
from
 
different
 
modalities
 
to
 
a
 
common
 
space)
 
can
 
aid
 
in
 
effective
 
representation
 
fusion,
 
improving
 
model
 
coherence
 
and
 
interpretability.
  5.  Generalization  Across  Modalities  Models  may  struggle  to  generalize  well  across  modalities  in  unseen  data  or  new  tasks,  
especially
 
when
 
there
 
are
 
domain
 
shifts
 
(e.g.,
 
moving
 
from
 
photographic
 
images
 
to
 
illustrations).
 Solution:  Leveraging  domain  adaptation  techniques  a

---

on:  Leveraging  domain  adaptation  techniques  and  using  contrastive  learning  can  
help
 
align
 
representations
 
across
 
modalities,
 
enhancing
 
generalization
 
and
 
adaptability.
  
Q8.  Identify  issues  such  as  data  alignment,  the  complexity  of  model  
architectures,
 
and
 
the
 
difﬁculty
 
in
 
optimizing
 
for
 
multiple
 
modalities.
 
Discuss
 
potential
 
solutions
 
like
 
attention
 
mechanisms
 
or
 
joint
 
embedding
 
spaces. In  multimodal  learning,  issues

---

edding
 
spaces. In  multimodal  learning,  issues  like  data  alignment,  architectural  complexity,  and  optimizing  
for
 
multiple
 
modalities
 
pose
 
signiﬁcant
 
challenges.
 
Here’s
 
an
 
in-depth
 
look
 
at
 
each,
 
along
 
with
 
solutions
 
like
 
attention
 
mechanisms
 
and
 
joint
 
embedding
 
spaces.
  
1.
 
Data
 
Alignment Misaligned  data,  where  content  from  different  modalities  does  not  correspond  well  (e.g.,  an  
image
 
with
 
irrelevant
 
text),
 
hinders

---

an  
image
 
with
 
irrelevant
 
text),
 
hinders
 
multimodal
 
learning.
 
Models
 
trained
 
on
 
misaligned
 
data
 
struggle
 
to
 
accurately
 
associate
 
elements
 
across
 
modalities,
 
reducing
 
task
 
performance
 
(e.g.,
 
image
 
captioning
 
or
 
visual
 
question
 
answering).
 ➔  Solution:  Paired  datasets  (e.g.,  images  with  accurate  captions)  or  self-supervised  
learning
 
with
 
massive
 
unpaired
 
datasets
 
can
 
improve
 
alignment
 
by
 
allowing
 
models
 
to

---

mprove
 
alignment
 
by
 
allowing
 
models
 
to
 
infer
 
modality
 
relationships.
 
Contrastive
 
learning
 
also
 
helps
 
align
 
representations
 
by
 
teaching
 
models
 
to
 
match
 
related
 
inputs
 
across
 
modalities
 
while
 
distinguishing
 
unrelated
 
pairs.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   81 
 
2.
 
Model
 
Complexity Multimodal  models  are  often  complex,  with  large  architectures  th

---

e  often  complex,  with  large  architectures  that  combine  different  
processing
 
layers
 
(e.g.,
 
CNNs
 
for
 
images
 
and
 
transformers
 
for
 
text).
 
This
 
complexity
 
demands
 
high
 
computational
 
power
 
and
 
often
 
requires
 
extensive
 
tuning
 
to
 
ensure
 
balanced
 
performance
 
across
 
all
 
modalities.
 ➔  Solution:  Attention  mechanisms,  such  as  cross-attention,  help  focus  on  the  most  
relevant
 
parts
 
of
 
each
 
modality,
 
reducing
 
unnecessary

---

of
 
each
 
modality,
 
reducing
 
unnecessary
 
processing
 
and
 
improving
 
interpretability.
 
Additionally,
 
parameter-sharing
 
strategies
 
and
 
efﬁcient
 
architectures
 
like
 
multimodal
 
transformers
 
allow
 
for
 
streamlined
 
architectures
 
that
 
handle
 
multiple
 
inputs
 
without
 
excessive
 
computational
 
overhead.
  
3.
 
Optimizing
 
for
 
Multiple
 
Modalities Optimizing  multimodal  models  can  be  challenging  because  different  modalities  often  have  
dive

---

because  different  modalities  often  have  
diverse
 
structures
 
and
 
feature
 
distributions.
 
Achieving
 
a
 
balance
 
between
 
modalities
 
so
 
one
 
doesn't
 
dominate
 
is
 
crucial,
 
as
 
imbalances
 
can
 
lead
 
to
 
overﬁtting
 
on
 
one
 
modality
 
and
 
poor
 
generalization
 
on
 
others.
 Solution:  Joint  embedding  spaces ,  which  project  different  modalities  into  a  shared  
representation,
 
are
 
useful
 
for
 
learning
 
modality-agnostic
 
features.
 
By
 
map

---

earning
 
modality-agnostic
 
features.
 
By
 
mapping
 
all
 
inputs
 
to
 
a
 
common
 
space,
 
models
 
can
 
better
 
capture
 
cross-modal
 
relationships,
 
leading
 
to
 
improved
 
performance
 
across
 
modalities.
 
Additionally,
 
modality
 
dropout
 
(temporarily
 
ignoring
 
one
 
modality
 
during
 
training)
 
encourages
 
the
 
model
 
to
 
rely
 
on
 
multiple
 
sources
 
of
 
information,
 
balancing
 
the
 
learning
 
process.
  
Q9.  How  do  architects  like  CLIP  and  DAL

---

Q9.  How  do  architects  like  CLIP  and  DALL-E  utilize  multimodal  data,  and  
what
 
innovations
 
do
 
they
 
bring
 
to
 
the
 
ﬁeld? CLIP  and  DALL-E  are  groundbreaking  models  from  OpenAI  that  demonstrate  the  
integration
 
of
 
vision
 
and
 
language
 
modalities
 
through
 
multimodal
 
training
 
and
 
shared
 
embeddings,
 
allowing
 
them
 
to
 
process
 
and
 
link
 
visual
 
and
 
textual
 
information.
  
1.
 
CLIP
 
(Contrastive
 
Language–Image
 
Pretraining) C

---

P
 
(Contrastive
 
Language–Image
 
Pretraining) CLIP  is  designed  to  learn  a  shared  embedding  space  for  both  images  and  text.  During  
training,
 
it
 
learns
 
to
 
associate
 
an
 
image
 
with
 
a
 
text
 
caption
 
that
 
describes
 
it.
 
Speciﬁcally:
 ➢  Contrastive  Learning:  CLIP  is  trained  on  a  massive  dataset  of  image-text  pairs,  
learning
 
to
 
pull
 
related
 
image-text
 
embeddings
 
closer
 
together
 
and
 
push
 
unrelated
 
ones
 
farther
 
apart.
 
Th

---

push
 
unrelated
 
ones
 
farther
 
apart.
 
The
 
result
 
is
 
a
 
model
 
that
 
can
 
understand
 
visual
 
concepts
 
and
 
their
 
descriptions
 
in
 
natural
 
language.
 ➢  Zero-Shot  Capabilities:  Because  of  its  ability  to  link  visual  and  textual  information  in  
a
 
shared
 
space,
 
CLIP
 
can
 
perform
 
zero-shot
 
classiﬁcation
 
across
 
a
 
wide
 
range
 
of
 
image
 
categories
 
without
 
needing
 
further
 
ﬁne-tuning.
 
For
 
example,
 
it
 
can
 
correctly
 

M

---

ning.
 
For
 
example,
 
it
 
can
 
correctly
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   82 
 
match  images  with  descriptions  even  if  it  hasn’t  been  trained  on  those  speciﬁc  
images
 
or
 
categories.
 
2.
 
DALL-E DALL-E  builds  on  this  vision-language  integration  but  focuses  on  generation  rather  than  
classiﬁcation.
 
It
 
takes
 
text
 
prompts
 
and
 
generates
 
novel
 
images
 
that
 
mat

---

ts
 
and
 
generates
 
novel
 
images
 
that
 
match
 
the
 
description.
 ➢  Text-to-Image  Generation:  DALL-E  is  trained  to  create  realistic  and  creative  images  
from
 
textual
 
descriptions.
 
It
 
learns
 
associations
 
between
 
language
 
and
 
visual
 
elements,
 
like
 
objects,
 
colors,
 
styles,
 
and
 
compositions,
 
so
 
it
 
can
 
generate
 
images
 
that
 
align
 
with
 
speciﬁc
 
prompts.
 ➢  Understanding  Complex  Relationships:  Through  its  training,  DALL-E  ca

---

elationships:  Through  its  training,  DALL-E  can  interpret  
complex
 
and
 
nuanced
 
language
 
instructions.
 
For
 
example,
 
it
 
can
 
combine
 
attributes
 
and
 
styles
 
to
 
create
 
a
 
unique
 
image,
 
like
 
a
 
"two-story
 
house
 
shaped
 
like
 
a
 
shoe"
 
or
 
a
 
"cat
 
in
 
a
 
spacesuit
 
on
 
Mars.”
  
Q10.  Explain  how  CLIP  combines  text  and  image  data  for  tasks  like  
zero-shot
 
classiﬁcation,
 
while
 
DALL-E
 
generates
 
images
 
from
 
textual
 
descr

---

LL-E
 
generates
 
images
 
from
 
textual
 
descriptions,
 
emphasizing
 
their
 
impact
 
on
 
creative
 
applications
 
and
 
content
 
generation. CLIP  and  DALL-E  combine  text  and  image  data  in  unique  ways  that  enable  zero-shot  
classiﬁcation
 
and
 
text-to-image
 
generation.
  CLIP  uses  contrastive  learning  on  paired  text  and  image  data  to  create  a  shared  embedding  
space.
 
This
 
allows
 
it
 
to
 
perform
 
zero-shot
 
classiﬁcation
 
by
 
matching
 
images

---

zero-shot
 
classiﬁcation
 
by
 
matching
 
images
 
to
 
descriptions
 
without
 
additional
 
training,
 
making
 
it
 
adaptable
 
to
 
new
 
categories—a
 
powerful
 
tool
 
for
 
applications
 
like
 
visual
 
search
 
and
 
dynamic
 
asset
 
organization.
  DALL-E  generates  images  from  textual  descriptions  by  learning  associations  between  
language
 
and
 
visual
 
features.
 
It
 
interprets
 
complex
 
prompts
 
to
 
produce
 
unique
 
images
 
(e.g.,
 
"an
 
armchair
 
shaped

---

nique
 
images
 
(e.g.,
 
"an
 
armchair
 
shaped
 
like
 
an
 
avocado"),
 
transforming
 
content
 
generation
 
by
 
enabling
 
fast
 
concept
 
visualization,
 
prototyping,
 
and
 
creative
 
design.
  
Q11.  Describe  the  importance  of  data  preprocessing  and  representation  
in
 
multimodal
 
learning.
 
How
 
do
 
you
 
ensure
 
that
 
different
 
modalities
 
can
 
be
 
effectively
 
combined? Data  preprocessing  and  representation  are  essential  in  multimodal  learning  becau

---

n  are  essential  in  multimodal  learning  because  they  
ensure
 
that
 
different
 
types
 
of
 
data
 
(e.g.,
 
text,
 
image,
 
audio)
 
are
 
in
 
a
 
compatible
 
and
 
comparable
 
form
 
for
 
effective
 
integration.
 
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   83 
 
Here’s
 
why
 
data
 
preprocessing
 
and
 
representation 
are
 
crucial: 1.  Data  Quality  and  Consistency:  Preprocessing  cleans  and

---

y  and  Consistency:  Preprocessing  cleans  and  standardizes  data  across  
modalities,
 
reducing
 
noise
 
and
 
handling
 
missing
 
values
 
or
 
outliers.
 
This
 
step
 
is
 
key
 
to
 
minimizing
 
inconsistencies
 
and
 
ensuring
 
that
 
each
 
modality
 
contributes
 
useful
 
information
 
to
 
the
 
model.
  2.  Alignment  of  Data  Dimensions:  Different  modalities  often  have  varying  scales,  
resolutions,
 
and
 
feature
 
spaces
 
(e.g.,
 
pixel
 
values
 
in
 
images,
 
w

---

paces
 
(e.g.,
 
pixel
 
values
 
in
 
images,
 
word
 
embeddings
 
in
 
text).
 
Preprocessing
 
and
 
representation
 
techniques
 
like
 
scaling,
 
normalization,
 
or
 
embedding
 
ensure
 
that
 
each
 
modality’s
 
features
 
can
 
be
 
combined
 
meaningfully.
  3.  Feature  Extraction  and  Dimensionality  Reduction:  Certain  features  from  each  modality  
are
 
more
 
informative
 
than
 
others.
 
Using
 
techniques
 
like
 
PCA,
 
embeddings
 
(e.g.,
 
BERT
 
for
 
text,
 
CNNs

---

embeddings
 
(e.g.,
 
BERT
 
for
 
text,
 
CNNs
 
for
 
images),
 
and
 
other
 
feature
 
extraction
 
methods
 
captures
 
the
 
most
 
relevant
 
information,
 
reducing
 
the
 
data
 
to
 
a
 
manageable
 
size
 
while
 
retaining
 
important
 
details.
  
To
 
ensure
 
eﬀective
 
combination
 
of
 
modalities: Temporal  or  Spatial  Alignment:  For  data  that  needs  synchronization  (e.g.,  video  frames  and  
corresponding
 
audio),
 
aligning
 
them
 
properly
 
is
 
crucial.
 
Techni

---

ligning
 
them
 
properly
 
is
 
crucial.
 
Techniques
 
like
 
Dynamic
 
Time
 
Warping
 
(DTW)
 
help
 
synchronize
 
modalities
 
with
 
time
 
dependencies.
  Uniﬁed  Representations:  Using  embedding  techniques  (e.g.,  transformers  for  text,  CNNs  
for
 
images)
 
that
 
map
 
different
 
modalities
 
to
 
a
 
common
 
vector
 
space
 
makes
 
it
 
easier
 
for
 
the
 
model
 
to
 
process
 
and
 
integrate
 
information.
  Attention  Mechanisms  and  Fusion  Techniques:  These  allow

---

Mechanisms  and  Fusion  Techniques:  These  allow  the  model  to  focus  on  the  
most
 
informative
 
parts
 
of
 
each
 
modality,
 
enabling
 
dynamic
 
fusion
 
at
 
feature
 
or
 
decision
 
levels.
 
Methods
 
like
 
concatenation,
 
gating
 
mechanisms,
 
or
 
cross-attention
 
layers
 
facilitate
 
a
 
richer
 
combination
 
of
 
modalities
 
by
 
allowing
 
ﬂexible
 
integration
 
and
 
weighting
 
of
 
different
 
features.
  In  summary,  careful  preprocessing  and  thoughtful  re

---

mary,  careful  preprocessing  and  thoughtful  representation  are  fundamental  in  
multimodal
 
learning,
 
as
 
they
 
create
 
a
 
foundation
 
where
 
each
 
modality's
 
unique
 
features
 
can
 
complement
 
one
 
another
 
effectively.
  
Q12.  Discuss  techniques  for  normalizing  and  embedding  different  data  
types,
 
such
 
as
 
using
 
CNNs
 
for
 
images
 
and
 
transformers
 
for
 
text,
 
and
 
how
 
these
 
representations
 
facilitate
 
integration
 
in
 
a
 
uniﬁed
 
mod

---

facilitate
 
integration
 
in
 
a
 
uniﬁed
 
model. 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   84 
 
Normalization  and  embedding  are  essential  in  transforming  diverse  data  types  into  
compatible
 
forms
 
for
 
multimodal
 
learning.
 
They
 
standardize
 
and
 
represent
 
data,
 
making
 
it
 
possible
 
for
 
different
 
modalities
 
to
 
integrate
 
effectively
 
within
 
a
 
uniﬁed
 
model.
  
Normali

---

ectively
 
within
 
a
 
uniﬁed
 
model.
  
Normalization
 
Techniques: ➢  Images  are  normalized  by  scaling  pixel  values  to  common  ranges  (e.g.,  [0,  1]),  which  
aligns
 
intensity
 
levels
 
across
 
samples.
 ➢  Text  undergoes  tokenization,  case  conversion,  and  sometimes  stopword  removal,  
with
 
additional
 
padding
 
to
 
ensure
 
consistent
 
token
 
lengths
 
across
 
samples.
 ➢  Audio  often  involves  mean-variance  normalization  to  standardize  loudness,  and  lo

---

normalization  to  standardize  loudness,  and  log  
scaling
 
for
 
amplitude
 
features
 
to
 
ease
 
processing.
  
Embedding
 
Techniques: ➔  CNNs  for  Images  produce  compact  feature  maps  by  learning  spatial  hierarchies  and  
patterns,
 
preserving
 
key
 
spatial
 
relationships
 
in
 
a
 
low-dimensional
 
form.
 ➔  Transformers  for  Text,  like  BERT  or  GPT,  capture  contextual  word  relationships  
through
 
self-attention,
 
creating
 
rich,
 
context-aware
 
embeddings

---

,
 
creating
 
rich,
 
context-aware
 
embeddings
 
that
 
reﬂect
 
both
 
meaning
 
and
 
sentence
 
structure.
 ➔  Other  Modalities  include  audio,  often  represented  via  CNNs  on  spectrograms,  and  
graphs,
 
embedded
 
using
 
Graph
 
Neural
 
Networks
 
(GNNs)
 
to
 
capture
 
node
 
relationships.
  
Integrating
 
Modalities: ●  Common  Embedding  Space:  Embeddings  from  CNNs  and  transformers  are  projected  
to
 
the
 
same
 
vector
 
space,
 
allowing
 
seamless
 
combination

---

ector
 
space,
 
allowing
 
seamless
 
combination
 
through
 
operations
 
like
 
concatenation.
 ●  Attention  Mechanisms  facilitate  dynamic  fusion  by  letting  one  modality  attend  to  
another
 
(e.g.,
 
text
 
attending
 
to
 
image
 
features),
 
improving
 
contextual
 
integration.
 ●  Fusion  Techniques  combine  features  from  each  modality  at  different  stages,  like  
multimodal
 
transformers
 
that
 
process
 
text
 
and
 
image
 
embeddings
 
simultaneously
 
for
 
featu

---

image
 
embeddings
 
simultaneously
 
for
 
feature
 
alignment.
  
Q13.  In  the  context  of  sentiment  analysis,  how  can  multimodal  
approaches
 
improve
 
accuracy
 
compared
 
to
 
text-only
 
models? In  sentiment  analysis,  multimodal  approaches  can  improve  accuracy  by  incorporating  data  
beyond
 
just
 
text,
 
like
 
images,
 
audio,
 
or
 
video,
 
which
 
often
 
provide
 
additional
 
emotional
 
or
 
contextual
 
cues
 
that
 
text
 
alone
 
cannot
 
fully
 
capture.

---

that
 
text
 
alone
 
cannot
 
fully
 
capture.
 
Here’s
 
how:
 1.  Visual  Cues:  Images,  like  facial  expressions  in  a  video  or  emojis  in  a  social  media  
post,
 
provide
 
strong
 
indicators
 
of
 
sentiment.
 
For
 
example,
 
a
 
person’s
 
smile
 
or
 
frown
 
often
 
conveys
 
emotion
 
more
 
directly
 
than
 
text
 
alone.
 
Multimodal
 
models
 
can
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   8

---

Generative  AI  Interview  Questions   85 
 
analyze  these  visual  elements  alongside  text,  leading  to  a  more  accurate  
understanding
 
of
 
sentiment.
 2.  Vocal  Cues  in  Audio:  For  spoken  data,  tone  of  voice,  pitch,  volume,  and  speed  of  
speech
 
are
 
important
 
for
 
interpreting
 
sentiment.
 
An
 
angry
 
tone
 
or
 
enthusiastic
 
pitch
 
adds
 
depth
 
to
 
spoken
 
words
 
that
 
might
 
seem
 
neutral
 
in
 
text
 
form.
 
Integrating
 
audio
 
featur

---

in
 
text
 
form.
 
Integrating
 
audio
 
features
 
with
 
text
 
in
 
multimodal
 
models
 
allows
 
for
 
a
 
more
 
nuanced
 
analysis,
 
especially
 
in
 
cases
 
where
 
words
 
alone
 
may
 
be
 
ambiguous.
 3.  Contextual  Alignment:  Multimodal  models  can  align  data  from  different  sources  
(e.g.,
 
synchronizing
 
spoken
 
words
 
with
 
facial
 
expressions
 
in
 
a
 
video)
 
to
 
reinforce
 
or
 
correct
 
the
 
sentiment
 
implied
 
by
 
one
 
modality.
 
If
 
text
 
is

---

implied
 
by
 
one
 
modality.
 
If
 
text
 
is
 
sarcastic
 
but
 
facial
 
expressions
 
are
 
neutral,
 
the
 
model
 
can
 
interpret
 
the
 
sentiment
 
more
 
accurately
 
by
 
cross-referencing
 
modalities.
 4.  Disambiguation:  Multimodal  approaches  can  help  disambiguate  ambiguous  or  
complex
 
expressions.
 
For
 
example,
 
text
 
alone
 
may
 
be
 
unclear
 
("I'm
 
ﬁne"
 
could
 
be
 
positive
 
or
 
negative),
 
but
 
if
 
paired
 
with
 
an
 
eye-roll
 
emoji
 
or
 
a
 
fru

---

ired
 
with
 
an
 
eye-roll
 
emoji
 
or
 
a
 
frustrated
 
tone
 
in
 
audio,
 
the
 
sentiment
 
becomes
 
clearer.
  
Q14.  Analyze  how  incorporating  visual  or  audio  cues  alongside  textual  
data
 
can
 
enhance
 
the
 
understanding
 
of
 
sentiment,
 
especially
 
in
 
complex
 
contexts
 
like
 
social
 
media
 
or
 
video
 
content. Incorporating  visual  or  audio  cues  alongside  textual  data  signiﬁcantly  enhances  sentiment  
analysis,
 
especially
 
in
 
complex
 
contexts

---

analysis,
 
especially
 
in
 
complex
 
contexts
 
like
 
social
 
media
 
or
 
video
 
content
 
where
 
sentiment
 
may
 
be
 
nuanced
 
or
 
ambiguous.
 
  
Here’s
 
how
 
each
 
modality
 
contributes
 
to
 
a
 
deeper
 
understanding: 1.  Disambiguating  Ambiguous  Text:  Text  on  social  media  is  often  informal  and  can  include  
sarcasm,
 
slang,
 
or
 
irony,
 
which
 
text-only
 
models
 
might
 
struggle
 
to
 
interpret.
 
Visual
 
cues
 
like
 
emojis,
 
GIFs,
 
or
 
accompan

---

al
 
cues
 
like
 
emojis,
 
GIFs,
 
or
 
accompanying
 
images
 
can
 
clarify
 
these
 
sentiments.
 
For
 
example,
 
a
 
post
 
saying
 
"Great!"
 
with
 
a
 
crying
 
emoji
 
implies
 
a
 
negative
 
sentiment,
 
while
 
text
 
alone
 
might
 
interpret
 
it
 
as
 
positive.
  2.  Capturing  Non-verbal  Signals  in  Video:  In  video  content,  facial  expressions,  body  
language,
 
and
 
gestures
 
provide
 
context
 
that
 
text
 
alone
 
can’t
 
convey.
 
A
 
speaker’s
 
subtle
 
frown

---

can’t
 
convey.
 
A
 
speaker’s
 
subtle
 
frown,
 
eye-rolling,
 
or
 
sigh
 
may
 
indicate
 
a
 
negative
 
or
 
skeptical
 
sentiment
 
that
 
doesn’t
 
appear
 
in
 
the
 
spoken
 
words.
 
Integrating
 
these
 
visual
 
signals
 
with
 
textual
 
analysis
 
helps
 
the
 
model
 
recognize
 
emotions
 
more
 
accurately,
 
particularly
 
in
 
mixed
 
or
 
complex
 
expressions.
   

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview

---

Generative  AI  Interview  Questions   86 
 
3.  Enhancing  Emotional  Tone  Through  Audio:  Tone  of  voice,  pitch,  volume,  and  rhythm  in  
spoken
 
data
 
carry
 
strong
 
emotional
 
indicators.
 
A
 
statement
 
may
 
be
 
positive
 
in
 
text
 
but
 
sound
 
sarcastic
 
or
 
angry
 
in
 
audio.
 
By
 
analyzing
 
vocal
 
cues
 
alongside
 
text,
 
models
 
gain
 
a
 
richer
 
understanding
 
of
 
sentiment,
 
especially
 
in
 
cases
 
where
 
emphasis
 
or
 
int

---

ecially
 
in
 
cases
 
where
 
emphasis
 
or
 
intonation
 
drastically
 
changes
 
the
 
meaning
 
of
 
words.
  4.  Contextual  Reinforcement  Across  Modalities:  Multimodal  integration  helps  align  
sentiment
 
across
 
different
 
cues,
 
allowing
 
the
 
model
 
to
 
reinforce
 
or
 
counterbalance
 
sentiments
 
from
 
each
 
source.
 
For
 
instance,
 
if
 
text
 
and
 
audio
 
both
 
express
 
frustration,
 
the
 
combined
 
signals
 
strengthen
 
sentiment
 
conﬁdence;
 
if
 
they

---

strengthen
 
sentiment
 
conﬁdence;
 
if
 
they
 
diverge,
 
the
 
model
 
can
 
interpret
 
sentiment
 
based
 
on
 
the
 
dominant
 
modality.
  
Q15.  What  metrics  would  you  use  to  evaluate  the  performance  of  a  
multimodal
 
model,
 
and
 
why
 
are
 
they
 
different
 
from
 
traditional
 
models?
 
To
 
evaluate
 
a
 
multimodal
 
model,
 
key
 
metrics
 
include:  1.  Accuracy,  Precision,  Recall,  and  F1  Score:  Standard  metrics  for  prediction  accuracy,  
essential
 
fo

---

trics  for  prediction  accuracy,  
essential
 
for
 
imbalanced
 
classes.
 2.  Multimodal  Consistency  Score:  Assesses  how  effectively  the  model  integrates  and  aligns  
data
 
from
 
different
 
modalities.
 3.  Cross-Modal  Retrieval  Accuracy:  Evaluates  the  model’s  ability  to  retrieve  related  
information
 
across
 
modalities,
 
reﬂecting
 
learned
 
relationships.
 4.  Confusion  Matrix  by  Modality:  Identiﬁes  errors  unique  to  each  modality,  highlighting  
integrat

---

ique  to  each  modality,  highlighting  
integration
 
issues.
 5.  Human  Evaluation:  Provides  insights  into  nuanced,  subjective  aspects  like  sentiment  or  
emotional
 
depth,
 
which
 
automated
 
metrics
 
might
 
miss.
  
Q16.  Discuss  evaluation  metrics  that  speciﬁcally  address  the  challenges  
of
 
multimodal
 
data
 
integration,
 
such
 
as
 
precision
 
and
 
recall
 
for
 
each
 
modality
 
and
 
overall
 
task
 
performance.
 Evaluating  multimodal  models  requires

---

mance.
 Evaluating  multimodal  models  requires  metrics  that  capture  both  modality-speciﬁc  
performance
 
and
 
overall
 
task
 
outcomes,
 
as
 
each
 
data
 
type
 
(e.g.,
 
text,
 
images,
 
audio)
 
contributes
 
uniquely
 
to
 
the
 
ﬁnal
 
results.
 
Key
 
metrics
 
include:
  1.  Modality-Speciﬁc  Precision  and  Recall:  Calculating  precision  and  recall  separately  for  
each
 
modality
 
reveals
 
individual
 
strengths
 
and
 
weaknesses,
 
which
 
can
 
prevent
 
one
 
moda

---

weaknesses,
 
which
 
can
 
prevent
 
one
 
modality
 
from
 
masking
 
the
 
issues
 
in
 
another.
 
For
 
instance,
 
in
 
text-image
 
models,
 
precision
 
and
 
recall
 
for
 
text
 
and
 
visual
 
accuracy
 
highlight
 
each
 
modality's
 
reliability.
  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   87 
 
2.  Cross-Modality  Alignment  Metrics:  Alignment  measures  assess  whether  different  
modalities
 
are

---

s  assess  whether  different  
modalities
 
are
 
effectively
 
integrated.
 
Metrics
 
like
 
Contrastive
 
Loss
 
measure
 
the
 
similarity
 
between
 
matched
 
(e.g.,
 
image-text
 
pairs)
 
and
 
unmatched
 
pairs,
 
ensuring
 
the
 
model
 
aligns
 
multimodal
 
data
 
correctly.
 
Cosine
 
Similarity
 
between
 
embeddings
 
from
 
different
 
modalities
 
further
 
veriﬁes
 
alignment
 
quality,
 
indicating
 
whether
 
modalities
 
are
 
projected
 
cohesively
 
into
 
the
 
same
 
sp

---

projected
 
cohesively
 
into
 
the
 
same
 
space.
  3.  Overall  Task  Performance  Metrics:  For  end-to-end  tasks  like  image  captioning  or  
multimodal
 
classiﬁcation,
 
task-speciﬁc
 
metrics
 
like
 
BLEU
 
(for
 
generated
 
text),
 
accuracy
 
(for
 
classiﬁcation),
 
or
 
Intersection
 
over
 
Union
 
(IoU
 
for
 
segmentation)
 
evaluate
 
the
 
ﬁnal
 
output.
 
Additionally,
 
weighted
 
averaging
 
can
 
create
 
a
 
uniﬁed
 
performance
 
measure
 
across
 
modalities,
 
pa

---

performance
 
measure
 
across
 
modalities,
 
particularly
 
when
 
one
 
modality
 
is
 
more
 
crucial
 
to
 
the
 
task.
  4.  Contribution  Analysis:  Metrics  like  SHAP  values  quantify  the  importance  of  each  
modality
 
in
 
the
 
model’s
 
decision-making.
 
Dropout
 
tests
 
(masking
 
modalities)
 
can
 
measure
 
how
 
each
 
modality
 
affects
 
the
 
model’s
 
overall
 
accuracy.
 
This
 
helps
 
determine
 
if
 
certain
 
modalities
 
contribute
 
disproportionately
 
or

---

dalities
 
contribute
 
disproportionately
 
or
 
are
 
redundant,
 
optimizing
 
model
 
efﬁciency
 
and
 
avoiding
 
overﬁtting.
  5.  Error  Analysis  by  Modality:  Separate  confusion  matrices  or  error  rates  for  each  
modality
 
reveal
 
speciﬁc
 
issues,
 
allowing
 
ﬁne-tuned
 
improvements.
 
Additionally,
 
Multimodal
 
Entropy
 
assesses
 
prediction
 
uncertainty
 
when
 
integrating
 
multiple
 
data
 
types,
 
with
 
high
 
entropy
 
potentially
 
indicating
 
misalignment

---

tropy
 
potentially
 
indicating
 
misalignment
 
or
 
noise.
  6.  Temporal  Consistency  (for  Sequential  Data):  For  time-series  data  like  video-audio,  
metrics
 
like
 
Dynamic
 
Time
 
Warping
 
(DTW)
 
check
 
alignment
 
over
 
time,
 
crucial
 
for
 
tasks
 
like
 
action
 
recognition,
 
where
 
modalities
 
must
 
evolve
 
in
 
sync.
  7.  Human  Evaluation:  For  subjective  tasks  like  story  generation,  human  ratings  for  
coherence,
 
naturalness,
 
and
 
relevance
 
are

---

coherence,
 
naturalness,
 
and
 
relevance
 
are
 
valuable
 
for
 
capturing
 
quality
 
beyond
 
numerical
 
metrics.
 
Evaluators
 
also
 
assess
 
multimodal
 
coherence,
 
conﬁrming
 
outputs
 
that
 
are
 
logically
 
and
 
contextually
 
aligned
 
across
 
all
 
modalities.
  
Q17.  How  do  you  handle  the  issue  of  imbalanced  data  when  working  with  
different
 
modalities
 
in
 
a
 
multimodal
 
dataset? Handling  imbalanced  data  in  multimodal  datasets  involves  balancing

---

ta  in  multimodal  datasets  involves  balancing  each  modality’s  
representation
 
and
 
ensuring
 
that
 
underrepresented
 
modalities
 
do
 
not
 
negatively
 
impact
 
model
 
performance.
 
  
Here
 
are
 
strategies
 
to
 
address
 
this: 1.  Modality-Speciﬁc  Sampling:  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   88 
 
➢  Oversampling  and  Undersampling :  Oversampling  data  from  the  less-represented  
mo

---

ersampling  data  from  the  less-represented  
modality
 
or
 
undersampling
 
from
 
the
 
overrepresented
 
one
 
can
 
help
 
balance
 
the
 
dataset.
 
For
 
example,
 
in
 
a
 
text-audio
 
dataset,
 
where
 
audio
 
samples
 
might
 
be
 
fewer,
 
duplicating
 
or
 
augmenting
 
audio
 
samples
 
to
 
match
 
the
 
text
 
distribution
 
can
 
help
 
equalize
 
training
 
data.
 ➢  Modal  Mixup:  In  this  technique,  artiﬁcial  samples  are  created  by  blending  features  
from
 
two

---

e  created  by  blending  features  
from
 
two
 
or
 
more
 
samples.
 
For
 
instance,
 
in
 
image-text
 
datasets,
 
mixing
 
features
 
of
 
images
 
with
 
low-resourced
 
text
 
annotations
 
can
 
help
 
the
 
model
 
generalize
 
without
 
relying
 
too
 
heavily
 
on
 
a
 
single
 
modality.
  2.  Data  Augmentation:  ➢  Cross-Modal  Augmentation:  Techniques  like  generating  synthetic  data  for  one  
modality
 
based
 
on
 
another
 
can
 
create
 
balanced
 
multimodal
 
pairs.

---

can
 
create
 
balanced
 
multimodal
 
pairs.
 
Text-to-image
 
generation
 
(e.g.,
 
using
 
generative
 
models)
 
or
 
audio
 
synthesis
 
from
 
text
 
can
 
help
 
when
 
one
 
modality
 
has
 
limited
 
data.
 ➢  Modality-Speciﬁc  Augmentation:  Augmentation  methods  that  ﬁt  each  modality’s  
characteristics,
 
such
 
as
 
ﬂipping
 
or
 
color
 
adjustments
 
for
 
images
 
and
 
pitch
 
changes
 
or
 
noise
 
addition
 
for
 
audio,
 
can
 
create
 
variability
 
and
 
reduce
 
the

---

can
 
create
 
variability
 
and
 
reduce
 
the
 
risk
 
of
 
overﬁtting
 
on
 
the
 
dominant
 
modality.
  3.  Weighted  Loss  Functions:  ➢  Modality-Speciﬁc  Weighting:  Assign  higher  weights  in  the  loss  function  to  
underrepresented
 
modalities
 
so
 
the
 
model
 
does
 
not
 
ignore
 
them
 
during
 
training.
 
This
 
ensures
 
the
 
model
 
pays
 
more
 
attention
 
to
 
learning
 
from
 
scarce
 
modalities.
 ➢  Task-Aware  Loss  Functions:  Customizing  the  loss  function

---

Loss  Functions:  Customizing  the  loss  function  to  emphasize  
harder-to-predict
 
or
 
underrepresented
 
multimodal
 
tasks
 
can
 
help
 
balance
 
training
 
outcomes
 
across
 
modalities,
 
especially
 
in
 
scenarios
 
where
 
one
 
modality
 
carries
 
higher
 
task
 
importance.
  4.  Multimodal  Ensemble  Learning:  Combining  independent  models  trained  on  each  modality  and  then  integrating  their  
predictions
 
can
 
help
 
balance
 
outcomes.
 
This
 
technique
 
allows

---

balance
 
outcomes.
 
This
 
technique
 
allows
 
each
 
modality
 
to
 
be
 
trained
 
on
 
its
 
own
 
terms,
 
reducing
 
the
 
dominance
 
of
 
overrepresented
 
modalities.
  5.  Modality  Dropout:  Regularly  "dropping  out"  or  masking  certain  modalities  during  training  forces  the  model  to  
learn
 
to
 
perform
 
tasks
 
with
 
incomplete
 
information.
 
This
 
encourages
 
robust
 
learning
 
across
 
modalities
 
and
 
can
 
mitigate
 
biases
 
toward
 
dominant
 
modaliti

---

mitigate
 
biases
 
toward
 
dominant
 
modalities.
  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   89 
 
Q18.  Explore  strategies  such  as  data  augmentation,  balancing  
techniques,
 
or
 
synthetic
 
data
 
generation
 
to
 
ensure
 
that
 
models
 
receive
 
sufﬁcient
 
training
 
from
 
all
 
modalities. To  address  data  imbalance  in  multimodal  datasets,  a  combination  of  data  augmentation,  
balancing

---

combination  of  data  augmentation,  
balancing
 
techniques,
 
and
 
synthetic
 
data
 
generation
 
can
 
ensure
 
models
 
receive
 
sufﬁcient
 
training
 
from
 
each
 
modality:
  1.  Data  Augmentation:  Applying  modality-speciﬁc  augmentations  (e.g.,  ﬂipping  images,  
paraphrasing
 
text,
 
pitch
 
shifts
 
in
 
audio)
 
increases
 
data
 
diversity,
 
preventing
 
over-reliance
 
on
 
any
 
one
 
modality.
 
Cross-modal
 
augmentation,
 
like
 
generating
 
new
 
image-text
 
pair

---

ion,
 
like
 
generating
 
new
 
image-text
 
pairs
 
from
 
existing
 
text
 
captions
 
or
 
descriptions,
 
further
 
balances
 
representation
 
by
 
creating
 
aligned
 
data
 
across
 
modalities.
  2.  Balancing  Techniques:  Oversampling  and  undersampling  help  manage  imbalanced  
modalities
 
by
 
increasing
 
the
 
frequency
 
of
 
underrepresented
 
samples
 
or
 
reducing
 
overrepresented
 
ones.
 
Weighted
 
loss
 
functions
 
can
 
also
 
emphasize
 
underrepresented
 
modalit

---

an
 
also
 
emphasize
 
underrepresented
 
modalities,
 
guiding
 
the
 
model
 
to
 
prioritize
 
learning
 
from
 
scarce
 
data.
  3.  Synthetic  Data  Generation:  Generating  synthetic  samples  for  underrepresented  
modalities,
 
such
 
as
 
using
 
GANs
 
to
 
create
 
images
 
from
 
text
 
or
 
generating
 
speech
 
from
 
text,
 
helps
 
balance
 
the
 
dataset.
 
Additionally,
 
cross-modal
 
synthesis
 
(e.g.,
 
generating
 
captions
 
for
 
unlabeled
 
images)
 
creates
 
new
 
mu

---

s
 
for
 
unlabeled
 
images)
 
creates
 
new
 
multimodal
 
pairs,
 
enhancing
 
integration.
  
Q19.  Can  you  give  examples  of  industries  or  applications  where  
multimodal
 
AI
 
is
 
making
 
a
 
signiﬁcant
 
impact? 
Here
 
are
 
key
 
areas
 
where
 
multimodal
 
AI
 
is
 
impactful: 1.  Healthcare:  Combines  imaging  and  patient  data  for  diagnostics  and  personalized  
treatments.
 2.  E-commerce:  Enhances  recommendations  using  product  images  and  reviews.  3.  Autonom

---

using  product  images  and  reviews.  3.  Autonomous  Vehicles:  Integrates  data  from  cameras,  LiDAR,  and  GPS  for  safer  
navigation.
 4.  Media:  Enables  interactive  content  generation,  enriching  storytelling.  5.  Customer  Service:  Uses  text,  voice,  and  sentiment  analysis  for  personalized  support  6.  Security:  Integrates  video,  audio,  and  biometrics  for  threat  detection.  7.  Education:  Combines  text,  video,  and  exercises  for  adaptive  learning  experien

---

and  exercises  for  adaptive  learning  experiences.   

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   90 
 
Q20.  Highlight  ﬁelds  like  healthcare  (combining  medical  images  with  
patient
 
records),
 
entertainment
 
(personalized
 
recommendations),
 
and
 
autonomous
 
systems
 
(integrating
 
sensory
 
data
 
for
 
navigation).
 
Here’s
 
how
 
AI
 
applications
 
are
 
impacting
 
each
 
of
 
these
 
ﬁelds: 1.

---

are
 
impacting
 
each
 
of
 
these
 
ﬁelds: 1.  Healthcare :  AI  is  revolutionizing  healthcare  by  combining  medical  images  with  patient  
records
 
to
 
improve
 
diagnostics,
 
personalize
 
treatments,
 
and
 
streamline
 
operations.
 
By
 
integrating
 
radiological
 
images
 
(like
 
MRIs
 
and
 
CT
 
scans)
 
with
 
electronic
 
health
 
records,
 
AI
 
models
 
can
 
identify
 
disease
 
patterns,
 
track
 
patient
 
progress,
 
and
 
predict
 
potential
 
health
 
issues
 
e

---

and
 
predict
 
potential
 
health
 
issues
 
earlier.
 
This
 
combination
 
enhances
 
precision
 
medicine
 
and
 
leads
 
to
 
more
 
efﬁcient
 
and
 
effective
 
care.
  2.  Entertainment:  Personalized  recommendations  in  entertainment,  powered  by  AI,  create  
unique
 
experiences
 
for
 
users.
 
By
 
analyzing
 
viewing
 
patterns,
 
AI
 
algorithms
 
can
 
suggest
 
tailored
 
content,
 
enhancing
 
user
 
engagement.
 
This
 
includes
 
recommendations
 
for
 
movies,
 
music,

---

ncludes
 
recommendations
 
for
 
movies,
 
music,
 
and
 
gaming,
 
which
 
helps
 
streaming
 
platforms
 
and
 
other
 
entertainment
 
services
 
deliver
 
highly
 
relevant
 
content
 
to
 
keep
 
users
 
engaged.
  3.  Autonomous  Systems:  AI  in  autonomous  systems  integrates  sensory  data  from  cameras,  
LiDAR,
 
radar,
 
and
 
other
 
sensors
 
for
 
accurate
 
navigation.
 
This
 
technology
 
enables
 
autonomous
 
vehicles,
 
drones,
 
and
 
robots
 
to
 
interpret
 
their
 
en

---

ones,
 
and
 
robots
 
to
 
interpret
 
their
 
environment,
 
detect
 
obstacles,
 
and
 
make
 
real-time
 
decisions,
 
resulting
 
in
 
safer
 
and
 
more
 
reliable
 
autonomous
 
navigation.
 
Integrating
 
sensory
 
data
 
also
 
enhances
 
situational
 
awareness,
 
crucial
 
for
 
both
 
land-based
 
and
 
aerial
 
autonomous
 
vehicles.
  
Q21.  What  future  trends  do  you  foresee  in  the  development  of  
multimodal
 
AI,
 
and
 
how
 
might
 
they
 
shape
 
the
 
way
 
we
 
inte

---

how
 
might
 
they
 
shape
 
the
 
way
 
we
 
interact
 
with
 
technology?
 
Here’s
 
a
 
concise
 
overview
 
of
 
ten
 
future
 
trends
 
in
 
multimodal
 
AI
 
and
 
their
 
potential
 
impacts: 1.  Deeper  Context  Understanding:  AI  will  interpret  emotions,  tone,  and  gestures,  making  
interactions
 
more
 
empathetic
 
and
 
context-aware.
  2.  Uniﬁed  AI  Assistants:  Consistent,  personalized  AI  experiences  across  devices,  offering  
seamless
 
assistance
 
in
 
various
 
s

---

ffering  
seamless
 
assistance
 
in
 
various
 
settings.
  3.  AR/VR  Integration:  Multimodal  AI  will  enhance  immersive  experiences,  enabling  natural  
interactions
 
within
 
augmented
 
and
 
virtual
 
environments.
  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   91 
 
 4.  Creative  Collaboration:  AI  will  assist  creatives  by  blending  sketches,  descriptions,  and  
ideas
 
into
 
cohesive
 
outputs,

---

ions,  and  
ideas
 
into
 
cohesive
 
outputs,
 
boosting
 
artistic
 
productivity.
  5.  Increased  Accessibility:  Multimodal  AI  will  adapt  interactions  to  individual  abilities,  
improving
 
accessibility
 
for
 
people
 
with
 
disabilities.
  6.  Smarter  Autonomous  Systems:  Enhanced  sensor  integration  will  allow  safer  and  more  
precise
 
navigation
 
for
 
autonomous
 
vehicles
 
and
 
drones.
  7.  Data  Integration  Across  Platforms:  AI  will  reduce  data  silos,  c

---

oss  Platforms:  AI  will  reduce  data  silos,  combining  information  from  
various
 
sources
 
for
 
a
 
more
 
holistic
 
view
 
of
 
users
 
or
 
businesses.
  8.  Natural  Language  Data  Exploration:  Users  will  query  and  explore  complex  data  using  
everyday
 
language,
 
simplifying
 
data
 
analysis
 
for
 
non-experts.
  9.  Health  and  Wellness  Insights:  AI  will  integrate  data  from  medical  records  and  wearables,  
offering
 
personalized
 
health
 
insights
 
and

---

offering
 
personalized
 
health
 
insights
 
and
 
proactive
 
care.
  10.  Enhanced  Human-Robot  Collaboration:  Multimodal  AI  will  make  robots  more  
responsive
 
to
 
voice,
 
gestures,
 
and
 
visual
 
cues,
 
aiding
 
human
 
interactions
 
in
 
various
 
settings.
  
Q22.  Discuss  anticipated  advancements  such  as  improved  integration  
techniques,
 
more
 
sophisticated
 
models
 
capable
 
of
 
understanding
 
context
 
across
 
modalities,
 
and
 
potential
 
ethical
 
consi

---

modalities,
 
and
 
potential
 
ethical
 
considerations
 
in
 
their
 
application. 
Here’s
 
a
 
summary
 
of
 
anticipated
 
advancements
 
and
 
ethical
 
considerations
 
in
 
multimodal
 
AI: 
Advancements  1.  Enhanced  Integration:  Advanced  fusion  techniques  will  merge  diverse  data  sources  more  
ﬂuidly,
 
improving
 
real-time
 
interactions
 
in
 
areas
 
like
 
healthcare
 
and
 
autonomous
 
systems.
  2.  Sophisticated  Contextual  Understanding:  Models  will  interpret

---

ontextual  Understanding:  Models  will  interpret  nuanced  inputs  across  
modalities,
 
allowing
 
more
 
natural,
 
context-sensitive
 
responses.
  3.  Continual  Learning:  AI  will  better  adapt  to  user  preferences  over  time,  enhancing  
personalization
 
in
 
entertainment,
 
productivity,
 
and
 
beyond.
  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   92 
 
Ethical  Considerations
 1.  Privacy  and  Secur

---

Ethical  Considerations
 1.  Privacy  and  Security:  Protecting  sensitive  data  from  misuse  or  unauthorized  access  will  
be
 
crucial.
  2.  Bias  and  Fairness:  Ensuring  fair  outcomes  by  minimizing  biases  in  training  data,  
especially
 
in
 
interpreting
 
emotions
 
and
 
intent.
  3.  Transparency:  Models  must  be  explainable,  particularly  in  high-stakes  ﬁelds  like  
healthcare
 
and
 
ﬁnance.
  4.  Human  Oversight:  Clear  protocols  for  intervention  in  auton

---

t:  Clear  protocols  for  intervention  in  autonomous  systems  to  ensure  
accountability.
  5.  Consent  and  Control:  Users  need  clear  choices  over  what  data  is  collected  and  used,  
maintaining
 
control
 
over
 
their
 
personal
 
information.
                             

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   93 
 
                      
Word  and  Sentence  Embeddings 
                  

MD  A

---

d  Sentence  Embeddings 
                  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   94 
 
Q1.  What  is  the  fundamental  concept  of  embeddings  in  machine  learning,  
and
 
how
 
do
 
they
 
represent
 
information
 
in
 
a
 
more
 
compact
 
form
 
compared
 
to
 
raw
 
input
 
data? Embeddings  in  machine  learning  are  a  way  to  represent  complex  data  (like  words,  
images,
 
or
 
even
 
users)
 
in

---

(like  words,  
images,
 
or
 
even
 
users)
 
in
 
a
 
compact,
 
continuous
 
vector
 
space.
 
The
 
fundamental
 
concept
 
of
 
embeddings
 
is
 
to
 
map
 
raw
 
input
 
data,
 
often
 
high-dimensional
 
and
 
sparse,
 
into
 
lower-dimensional,
 
dense
 
vectors
 
while
 
preserving
 
meaningful
 
relationships
 
between
 
data
 
points.
  
Here’s
 
how
 
embeddings
 
work
 
and
 
why
 
they’re
 
valuable: 1.  Dimensionality  Reduction:  Embeddings  transform  high-dimensional  data  (su

---

Embeddings  transform  high-dimensional  data  (such  
as
 
one-hot
 
encoded
 
text
 
or
 
pixel
 
data
 
from
 
images)
 
into
 
a
 
smaller,
 
dense
 
vector
 
form,
 
capturing
 
essential
 
features.
 
 For  example,  instead  of  representing  a  word  by  thousands  of  dimensions,  an  
embedding
 
may
 
reduce
 
it
 
to
 
a
 
100-dimensional
 
vector
 
that
 
holds
 
meaningful
 
semantic
 
information.
  2.  Similarity  Representation:  Embeddings  preserve  relationships  so  that  si

---

Embeddings  preserve  relationships  so  that  similar  
data
 
points
 
are
 
close
 
together
 
in
 
the
 
vector
 
space.
 
 For  instance,  in  word  embeddings,  words  with  similar  meanings  (like  “king”  and  
“queen”)
 
will
 
be
 
closer
 
in
 
the
 
vector
 
space,
 
capturing
 
semantic
 
relationships
 
in
 
a
 
way
 
that
 
one-hot
 
vectors
 
or
 
raw
 
data
 
cannot.
  3.  Improved  Efﬁciency:  Embeddings  allow  models  to  process  and  learn  from  data  
more
 
efﬁciently.

---

ocess  and  learn  from  data  
more
 
efﬁciently.
 
By
 
reducing
 
dimensionality
 
and
 
removing
 
irrelevant
 
information,
 
embeddings
 
make
 
it
 
easier
 
for
 
machine
 
learning
 
models
 
to
 
recognize
 
patterns,
 
especially
 
in
 
large
 
datasets,
 
without
 
being
 
bogged
 
down
 
by
 
high-dimensional
 
noise.
  
Q2.  Compare  and  contrast  word  embeddings  and  sentence  embeddings.  
How
 
do
 
their
 
applications
 
differ,
 
and
 
what
 
considerations
 
come
 
into

---

er,
 
and
 
what
 
considerations
 
come
 
into
 
play
 
when
 
choosing
 
between
 
them? Word  embeddings  and  sentence  embeddings  are  both  techniques  used  to  represent  
text
 
in
 
vector
 
form,
 
but
 
they
 
capture
 
information
 
at
 
different
 
granularities
 
and
 
serve
 
different
 
purposes.
 
Here’s
 
a
 
comparison
 
of
 
the
 
two:
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   95 
 
  
Choosing

---

ive  AI  Interview  Questions   95 
 
  
Choosing
 
Between
 
Word
 
and
 
Sentence
 
Embeddings Granularity  of  the  Task:  Use  word  embeddings  if  the  task  requires  analyzing  
individual
 
words
 
(e.g.,
 
word
 
similarity).
 
Use
 
sentence
 
embeddings
 
when
 
overall
 
sentence
 
meaning
 
or
 
context
 
matters
 
(e.g.,
 
sentence
 
similarity,
 
semantic
 
search).
  Context  Dependence:  If  understanding  word  meaning  in  context  is  important,  
sentence
 
embeddings
 
or

---

text  is  important,  
sentence
 
embeddings
 
or
 
contextualized
 
word
 
embeddings
 
(like
 
BERT)
 
are
 
preferred.
  Computational  Complexity:  Sentence  embeddings  often  require  more  resources  
but
 
provide
 
richer,
 
context-aware
 
representations.
 
Word
 
embeddings
 
are
 
faster
 
and
 
simpler
 
but
 
lack
 
contextual
 
detail.
  
Q3.  Explain  the  concept  of  contextual  embeddings.  How  do  models  like  
BERT
 
generate
 
contextual
 
embeddings,
 
and
 
in
 
what

---

ate
 
contextual
 
embeddings,
 
and
 
in
 
what
 
scenarios
 
are
 
they
 
advantageous
 
compared
 
to
 
traditional
 
word
 
embeddings? 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   96 
 
Contextual  embeddings  are  representations  of  words  that  capture  the  meaning  of  
each
 
word
 
based
 
on
 
its
 
surrounding
 
context
 
in
 
a
 
sentence,
 
addressing
 
limitations
 
of
 
traditional
 
word
 
embeddings

---

imitations
 
of
 
traditional
 
word
 
embeddings
 
that
 
assign
 
a
 
ﬁxed
 
vector
 
to
 
each
 
word.
 
  Models  like  BERT  (Bidirectional  Encoder  Representations  from  Transformers)  
generate
 
contextual
 
embeddings
 
by
 
dynamically
 
adjusting
 
each
 
word’s
 
representation
 
based
 
on
 
its
 
context,
 
allowing
 
the
 
same
 
word
 
to
 
have
 
different
 
embeddings
 
in
 
different
 
sentences.
 
 For  example,  in  BERT,  the  word  "bank"  would  have  one  embedding  in

---

the  word  "bank"  would  have  one  embedding  in  "He  sat  by  the  
river
 
bank"
 
and
 
a
 
different
 
one
 
in
 
"She
 
went
 
to
 
the
 
bank
 
to
 
deposit
 
money."
  
How
 
BERT
 
Generates
 
Contextual
 
Embeddings BERT  uses  a  deep  neural  architecture  based  on  transformers,  which  processes  
input
 
sequences
 
bidirectionally.
 
Unlike
 
previous
 
models
 
that
 
only
 
look
 
at
 
words
 
in
 
a
 
single
 
direction
 
(left-to-right
 
or
 
right-to-left),
 
BERT
 
reads

---

eft-to-right
 
or
 
right-to-left),
 
BERT
 
reads
 
entire
 
sentences
 
simultaneously,
 
capturing
 
relationships
 
across
 
all
 
words.
 
Here’s
 
how
 
it
 
works:
 
 1.  Tokenization:  The  text  is  split  into  tokens,  which  may  represent  words,  subwords,  
or
 
even
 
individual
 
characters.
  2.  Embedding  Layer:  Each  token  is  initially  assigned  a  basic  embedding  that  
includes
 
positional
 
and
 
segment
 
information.
  3.  Transformer  Layers:  These  layers  pro

---

on.
  3.  Transformer  Layers:  These  layers  process  tokens  bidirectionally,  adjusting  each  
token's
 
embedding
 
based
 
on
 
attention,
 
which
 
weighs
 
the
 
importance
 
of
 
every
 
other
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   97 
 
word  in  the  sentence.  Each  word's  ﬁnal  representation  is  a  context-sensitive  vector,  
or
 
"contextual
 
embedding."
  
Advantages
 
of
 
Contextual
 
Embedd

---

bedding."
  
Advantages
 
of
 
Contextual
 
Embeddings Contextual  embeddings  offer  several  advantages  over  traditional  word  embeddings:  ➔  Handling  Polysemy:  Since  each  word’s  meaning  is  derived  from  its  context,  
models
 
like
 
BERT
 
can
 
accurately
 
represent
 
words
 
with
 
multiple
 
meanings,
 
making
 
them
 
valuable
 
for
 
nuanced
 
language
 
understanding
 
tasks.
 ➔  Improved  Accuracy:  Contextual  embeddings  are  particularly  effective  in  
tasks
 
requi

---

are  particularly  effective  in  
tasks
 
requiring
 
deep
 
understanding,
 
such
 
as
 
question
 
answering,
 
named
 
entity
 
recognition,
 
and
 
sentiment
 
analysis.
 ➔  Generalization:  These  embeddings  provide  richer  information  by  capturing  
word
 
dependencies
 
across
 
sentences,
 
making
 
models
 
more
 
effective
 
in
 
complex
 
applications
 
like
 
summarization
 
and
 
translation.
  
Q4.  Discuss  the  challenges  and  strategies  involved  in  generating  
cross-

---

and  strategies  involved  in  generating  
cross-modal
 
embeddings,
 
where
 
information
 
from
 
multiple
 
modalities,
 
such
 
as
 
text
 
and
 
image,
 
is
 
represented
 
in
 
a
 
shared
 
embedding
 
space. Generating  cross-modal  embeddings,  where  information  from  different  modalities  
(e.g.,
 
text
 
and
 
images)
 
is
 
represented
 
in
 
a
 
shared
 
space,
 
presents
 
several
 
challenges:
  
Challenges 1.  Alignment  Across  Modalities:  Images  and  text  have  different

---

Modalities:  Images  and  text  have  different  structures  (images  
as
 
pixel
 
arrays,
 
text
 
as
 
word
 
sequences),
 
making
 
it
 
difﬁcult
 
to
 
map
 
them
 
into
 
a
 
common
 
space.
  2.  Dimensional  and  Semantic  Disparity:  Images  are  high-dimensional  and  
continuous,
 
while
 
text
 
is
 
discrete
 
and
 
sequential,
 
requiring
 
methods
 
to
 
bridge
 
these
 
differences
 
effectively.
  3.  Contextual  Consistency:  Ensuring  that  both  image  and  text  representa

---

Ensuring  that  both  image  and  text  representations  align  
semantically
 
is
 
complex,
 
especially
 
in
 
multimodal
 
tasks
 
requiring
 
deep
 
understanding.
  4.  Computational  Complexity:  Jointly  training  deep  neural  networks  for  both  
modalities
 
requires
 
signiﬁcant
 
computational
 
resources.
  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   98 
 
 5.  Data  Scarcity:  Paired  text-image  dataset

---

5.  Data  Scarcity:  Paired  text-image  datasets  are  limited,  which  can  hinder  model  
performance
 
and
 
generalization.
  
Strategies 1.  Joint  Embedding  Spaces:  Dual-encoder  models  process  each  modality  
independently
 
(e.g.,
 
CNN
 
for
 
images,
 
transformer
 
for
 
text)
 
and
 
project
 
them
 
into
 
a
 
shared
 
space.
 
Contrastive
 
loss
 
minimizes
 
distances
 
between
 
matched
 
pairs
 
(e.g.,
 
image
 
and
 
its
 
caption)
 
while
 
maximizing
 
it
 
for
 
mis

---

s
 
caption)
 
while
 
maximizing
 
it
 
for
 
mismatched
 
pairs.
  2.  Attention  Mechanisms:  Cross-attention  layers  help  the  model  focus  on  relevant  
features
 
across
 
modalities.
 
For
 
example,
 
a
 
text
 
description
 
can
 
guide
 
the
 
image
 
encoder
 
to
 
focus
 
on
 
important
 
regions
 
within
 
an
 
image.
  3.  Self-Supervised  Learning:  Contrastive  learning  and  masked  modeling  can  align  
representations
 
of
 
text
 
and
 
images
 
by
 
using
 
unlabeled

---

text
 
and
 
images
 
by
 
using
 
unlabeled
 
data,
 
reducing
 
the
 
need
 
for
 
large,
 
labeled
 
datasets.
  4.  Multimodal  Transformers:  Uniﬁed  architectures  like  CLIP  and  ALIGN  learn  both  
visual
 
and
 
textual
 
features
 
together,
 
resulting
 
in
 
shared
 
embeddings.
 
These
 
models
 
have
 
been
 
shown
 
to
 
outperform
 
traditional
 
methods
 
by
 
processing
 
both
 
modalities
 
cohesively.
  5.  Regularization  and  Data  Augmentation:  Adding  noise  or  aug

---

and  Data  Augmentation:  Adding  noise  or  augmenting  data  (e.g.,  
paraphrasing
 
text,
 
modifying
 
images)
 
can
 
improve
 
robustness,
 
helping
 
the
 
model
 
generalize
 
across
 
different
 
inputs.
  
Q5.  When  training  word  embeddings,  how  can  models  be  designed  to  
effectively
 
capture
 
representations
 
for
 
rare
 
words
 
with
 
limited
 
occurrences
 
in
 
the
 
training
 
data? Capturing  representations  for  rare  words  in  word  embeddings  is  challenging

---

rare  words  in  word  embeddings  is  challenging,  as  
traditional
 
models
 
(like
 
Word2Vec)
 
rely
 
on
 
sufﬁcient
 
contextual
 
occurrences
 
to
 
learn
 
meaningful
 
vectors.
 
However,
 
several
 
techniques
 
and
 
approaches
 
can
 
be
 
applied
 
to
 
improve
 
the
 
quality
 
of
 
embeddings
 
for
 
rare
 
words:
  1.  Subword  Information  (e.g.,  FastText):  Models  like  FastText  represent  words  as  
collections
 
of
 
character
 
n-grams
 
rather
 
than
 
individual
 
wor

---

acter
 
n-grams
 
rather
 
than
 
individual
 
words.
 
This
 
allows
 
the
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   99 
 
model  to  build  embeddings  even  for  rare  words  by  learning  from  the  subword  
components
 
(character
 
n-grams)
 
that
 
they
 
share
 
with
 
other
 
words.
 
This
 
is
 
particularly
 
useful
 
for
 
morphologically
 
rich
 
languages,
 
where
 
words
 
can
 
share
 
roots
 
or
 
a

---

es,
 
where
 
words
 
can
 
share
 
roots
 
or
 
afﬁxes
 
that
 
contribute
 
to
 
their
 
meaning.
  2.  Pre-trained  Language  Models  (e.g.,  BERT,  GPT):  Transformer-based  models  
pre-trained
 
on
 
massive
 
corpora
 
can
 
capture
 
robust
 
representations
 
for
 
rare
 
words
 
due
 
to
 
their
 
extensive
 
contextual
 
learning.
 
These
 
models
 
assign
 
context-dependent
 
embeddings
 
for
 
each
 
occurrence
 
of
 
a
 
word,
 
which
 
helps
 
with
 
rare
 
words
 
since
 
the

---

ich
 
helps
 
with
 
rare
 
words
 
since
 
the
 
model
 
can
 
use
 
the
 
speciﬁc
 
context
 
to
 
approximate
 
meaning
 
even
 
when
 
occurrences
 
are
 
limited.
  3.  Transfer  Learning  and  Fine-tuning:  Pre-trained  models  on  a  larger,  more  
diverse
 
dataset
 
can
 
be
 
ﬁne-tuned
 
on
 
speciﬁc
 
tasks
 
or
 
domains.
 
By
 
leveraging
 
general
 
linguistic
 
knowledge
 
acquired
 
from
 
large
 
datasets,
 
embeddings
 
can
 
be
 
adapted
 
to
 
new
 
datasets
 
even
 
if
 
th

---

e
 
adapted
 
to
 
new
 
datasets
 
even
 
if
 
they
 
contain
 
rare
 
or
 
domain-speciﬁc
 
words.
  4.  Data  Augmentation  Techniques:  Generating  additional  context  for  rare  words  
can
 
help
 
models
 
learn
 
better
 
embeddings.
 
Techniques
 
like
 
synonym
 
replacement,
 
back-translation,
 
or
 
leveraging
 
word
 
deﬁnitions
 
(using
 
external
 
resources
 
like
 
dictionaries)
 
can
 
increase
 
the
 
contexts
 
for
 
rare
 
words.
  5.  Retroﬁt  Embeddings  with  External

---

words.
  5.  Retroﬁt  Embeddings  with  External  Knowledge  (e.g.,  WordNet):  Retroﬁtting  
modiﬁes
 
pre-trained
 
embeddings
 
by
 
incorporating
 
relationships
 
from
 
lexical
 
resources,
 
such
 
as
 
synonymy
 
or
 
hypernymy
 
from
 
WordNet.
 
This
 
approach
 
enforces
 
that
 
semantically
 
similar
 
words
 
share
 
closer
 
embeddings,
 
which
 
can
 
help
 
adjust
 
the
 
embeddings
 
of
 
rare
 
words
 
by
 
aligning
 
them
 
with
 
more
 
common,
 
semantically
 
similar
 
wor

---

th
 
more
 
common,
 
semantically
 
similar
 
words.
  6.  Few-shot  or  Zero-shot  Learning:  Few-shot  learning  methods  can  be  applied  to  
rare
 
words,
 
where
 
embeddings
 
are
 
inferred
 
by
 
analogizing
 
from
 
similar
 
patterns
 
or
 
structures
 
seen
 
during
 
training.
 
Zero-shot
 
learning
 
can
 
also
 
enable
 
the
 
model
 
to
 
approximate
 
rare
 
word
 
embeddings
 
by
 
understanding
 
shared
 
features
 
with
 
known
 
words.
  7.  Using  Contextual  Averaging:

---

own
 
words.
  7.  Using  Contextual  Averaging:  For  very  rare  words,  averaging  the  embeddings  of  
their
 
surrounding
 
context
 
words
 
in
 
a
 
large
 
corpus
 
can
 
provide
 
a
 
rough
 
estimate
 
of
 
their
 
embedding.
 
This
 
technique
 
leverages
 
the
 
intuition
 
that
 
the
 
context
 
of
 
rare
 
words
 
can
 
still
 
provide
 
clues
 
about
 
their
 
meanings.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Q

---

Generative  AI  Interview  Questions   100 
 
Q6.  Discuss  common  regularization  techniques  used  during  the  training  
of
 
embeddings
 
to
 
prevent
 
overﬁtting
 
and
 
enhance
 
the
 
generalization
 
ability
 
of
 
models. Regularization  techniques  help  prevent  overﬁtting  in  embedding  training  by  
promoting
 
generalization:
  1.  Dropout:  Randomly  "drops"  units  in  the  embedding  layer  during  training,  
encouraging
 
the
 
model
 
to
 
rely
 
on

---

ning,  
encouraging
 
the
 
model
 
to
 
rely
 
on
 
generalized
 
patterns
 
rather
 
than
 
speciﬁc
 
features.
  2.  L2  Regularization:  Adds  a  penalty  for  large  weight  values,  encouraging  
distributed
 
and
 
smaller
 
weights
 
that
 
help
 
embeddings
 
generalize.
  3.  Negative  Sampling:  Used  in  Word2Vec,  this  technique  samples  incorrect  word  
pairs,
 
forcing
 
the
 
model
 
to
 
focus
 
on
 
genuine
 
relationships
 
instead
 
of
 
spurious
 
associations.
  4.  Batc

---

instead
 
of
 
spurious
 
associations.
  4.  Batch  Normalization:  Normalizes  input  features,  stabilizing  training  and  reducing  
reliance
 
on
 
speciﬁc
 
patterns.
  5.  Early  Stopping:  Halts  training  based  on  validation  loss,  preventing  overﬁtting  by  
avoiding
 
excessive
 
parameter
 
tuning.
  6.  Data  Augmentation:  Adds  variability  by  generating  additional  contexts  for  rare  
words,
 
which
 
helps
 
the
 
model
 
avoid
 
overﬁtting
 
to
 
speciﬁc
 
examples.

---

avoid
 
overﬁtting
 
to
 
speciﬁc
 
examples.
  
Q7.  How  can  pre-trained  embeddings  be  leveraged  for  transfer  learning  
in
 
downstream
 
tasks,
 
and
 
what
 
advantages
 
does
 
transfer
 
learning
 
offer
 
in
 
terms
 
of
 
embedding
 
generation? Pre-trained  embeddings  can  be  effectively  leveraged  for  transfer  learning  in  
downstream
 
tasks
 
by
 
initializing
 
models
 
with
 
embeddings
 
that
 
already
 
capture
 
general
 
semantic
 
relationships.
 
  
Here’s
 
h

---

eneral
 
semantic
 
relationships.
 
  
Here’s
 
how
 
they
 
can
 
be
 
applied
 
and
 
the
 
beneﬁts
 
they
 
oﬀer: 1.  Initialization  for  Better  Starting  Point:  Pre-trained  embeddings,  such  as  
Word2Vec,
 
GloVe,
 
or
 
embeddings
 
from
 
language
 
models
 
like
 
BERT,
 
provide
 
a
 
strong
 
initialization
 
for
 
models
 
in
 
downstream
 
tasks.
 
These
 
embeddings
 
have
 
already
 

MD  ARBAAZ  KHAN                                                                Generative

---

Generative  AI  Interview  Questions   101 
 
learned  fundamental  relationships  and  language  patterns,  reducing  the  need  for  
extensive
 
training
 
from
 
scratch
 
and
 
allowing
 
models
 
to
 
start
 
with
 
relevant,
 
semantically
 
rich
 
representations.
  2.  Reduced  Data  Requirements:  Transfer  learning  with  pre-trained  embeddings  is  
particularly
 
useful
 
when
 
labeled
 
data
 
is
 
limited.
 
Since
 
these
 
embeddings
 
capt

---

is
 
limited.
 
Since
 
these
 
embeddings
 
capture
 
general
 
knowledge
 
from
 
large
 
corpora,
 
they
 
enable
 
the
 
model
 
to
 
generalize
 
more
 
effectively
 
even
 
with
 
smaller
 
task-speciﬁc
 
datasets.
  3.  Faster  Convergence:  Pre-trained  embeddings  help  models  converge  more  
quickly
 
since
 
they
 
have
 
already
 
captured
 
signiﬁcant
 
word
 
associations
 
and
 
semantic
 
structures.
 
This
 
reduces
 
computational
 
costs
 
and
 
training
 
time,
 
especiall

---

ional
 
costs
 
and
 
training
 
time,
 
especially
 
for
 
large-scale
 
models.
  4.  Improved  Performance  on  Specialized  Tasks:  By  ﬁne-tuning  pre-trained  
embeddings
 
on
 
task-speciﬁc
 
data,
 
models
 
can
 
adapt
 
general
 
knowledge
 
to
 
domain-speciﬁc
 
nuances
 
(e.g.,
 
medical
 
or
 
legal
 
terms).
 
This
 
results
 
in
 
more
 
accurate
 
and
 
contextually
 
relevant
 
embeddings
 
for
 
specialized
 
applications.
  
Q8.  What  is  quantization  in  the  context  of  e

---

.  What  is  quantization  in  the  context  of  embeddings,  and  how  does  it  
contribute
 
to
 
reducing
 
the
 
memory
 
footprint
 
of
 
models
 
while
 
preserving
 
representation
 
quality?
 Quantization  in  the  context  of  embeddings  is  a  technique  that  reduces  the  
precision
 
of
 
the
 
numerical
 
values
 
in
 
embeddings,
 
allowing
 
for
 
a
 
smaller
 
memory
 
footprint
 
while
 
still
 
aiming
 
to
 
preserve
 
representation
 
quality.
  Embeddings  are  dense  vect

---

entation
 
quality.
  Embeddings  are  dense  vector  representations,  typically  in  ﬂoating-point  format  
(e.g.,
 
32-bit
 
ﬂoating-point
 
numbers)
,
 
which
 
can
 
consume
 
a
 
large
 
amount
 
of
 
memory
 
when
 
dealing
 
with
 
high-dimensional
 
vectors
 
or
 
a
 
large
 
corpus
 
of
 
data.
 
Quantization
 
reduces
 
this
 
memory
 
consumption
 
by
 
representing
 
each
 
embedding
 
vector
 
with
 
lower-precision
 
values
 
(e.g.,
 
16-bit
 
ﬂoating-point,
 
8-bit
 
integers,

---

.,
 
16-bit
 
ﬂoating-point,
 
8-bit
 
integers,
 
or
 
even
 
binary
 
values)
.
  
Here’s
 
how
 
quantization
 
contributes
 
to
 
reducing
 
memory
 
and
 
maintaining
 
quality:  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   102 
 
1.  Memory  Reduction:  Quantization  replaces  high-precision  values  with  
lower-precision
 
approximations.
 
For
 
example,
 
converting
 
a
 
32-bit
 
ﬂoat
 
to
 
an
 
8-bit
 
integ

---

ting
 
a
 
32-bit
 
ﬂoat
 
to
 
an
 
8-bit
 
integer
 
reduces
 
the
 
memory
 
used
 
by
 
each
 
embedding
 
to
 
a
 
quarter
 
of
 
its
 
original
 
size,
 
or
 
even
 
less
 
if
 
more
 
aggressive
 
techniques
 
are
 
applied.
  2.  Types  of  Quantization:  There  are  several  types  of  quantization  used  for  
embeddings:
 ➔  Uniform  Quantization:  Reduces  precision  by  scaling  values  to  a  ﬁxed  range  
(e.g.,
 
[0,
 
255])
 
and
 
rounding.
 ➔  Non-Uniform  Quantization :  Adap

---

rounding.
 ➔  Non-Uniform  Quantization :  Adapts  to  the  distribution  of  values ,  so  clusters  
of
 
similar
 
embeddings
 
can
 
be
 
grouped
 
more
 
compactly.
 ➔  Product  Quantization:  Breaks  embeddings  into  subspaces  and  quantizes  
each
 
independently
.
 
This
 
helps
 
retain
 
quality
 
by
 
focusing
 
on
 
local
 
structures
 
within
 
each
 
part
 
of
 
the
 
vector.
  3.  Preserving  Representation  Quality:  Quantization  methods  typically  aim  to  
maintain
 
the

---

tion  methods  typically  aim  to  
maintain
 
the
 
spatial
 
relationships
 
(like
 
distances
 
or
 
angles)
 
between
 
embeddings,
 
which
 
are
 
essential
 
for
 
preserving
 
semantic
 
similarities.
 
Techniques
 
such
 
as
 
vector
 
quantization
 
(VQ)
 
and
 
product
 
quantization
 
(PQ)
 
cluster
 
embeddings
 
and
 
store
 
only
 
approximate
 
representations,
 
while
 
uniform
 
quantization
 
simply
 
scales
 
values
 
into
 
smaller
 
precision
 
ranges.
 
These
 
methods
 
tr

---

aller
 
precision
 
ranges.
 
These
 
methods
 
try
 
to
 
retain
 
key
 
features
 
of
 
the
 
data
 
structure,
 
so
 
the
 
impact
 
on
 
representation
 
quality
 
is
 
minimal
 
for
 
most
 
tasks.
  4.  Efﬁcient  Computation:  Quantized  embeddings  also  allow  for  faster  
computations,
 
as
 
operating
 
on
 
smaller
 
values
 
requires
 
less
 
computational
 
power
 
and
 
bandwidth,
 
thus
 
accelerating
 
inference.
  
Q9.  When  dealing  with  high-cardinality  categorical  featur

---

aling  with  high-cardinality  categorical  features  in  tabular  
data,
 
how
 
would
 
you
 
efﬁciently
 
implement
 
and
 
train
 
embeddings
 
using
 
a
 
neural
 
network
 
to
 
capture
 
meaningful
 
representations For  efﬁciently  handling  high-cardinality  categorical  features  with  embeddings  in  
neural
 
networks,
 
use
 
these
 
strategies:
  1.  Set  Embedding  Size  by  Cardinality:  Use  a  smaller  embedding  dimension  (e.g.,  
around
  
for
 
cardinality
 
)
 
to
 
balanc

---

.,  
around
  
for
 
cardinality
 
)
 
to
 
balance
 
expressiveness
 
and
 
memory
 
use.
   

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   103 
 
2.  Regularization:  Apply  dropout  or   regularization  on  embeddings  to  avoid  
overﬁtting,
 
especially
 
for
 
high-cardinality
 
cases.
  3.  Shared  and  Factorized  Embeddings:  Use  shared  embeddings  for  related  features,  
or
 
factorized
 
embeddings
 
to
 
re

---

features,  
or
 
factorized
 
embeddings
 
to
 
reduce
 
size
 
without
 
losing
 
much
 
detail.
  4.  Quantization:  Quantize  embeddings  (e.g.,  to  8-bit)  to  reduce  memory  while  
retaining
 
representation
 
quality,
 
as
 
used
 
in
 
large-scale
 
recommendation
 
systems.
  5.  Efﬁcient  Batch  Training:  Use  mini-batches  and  adaptive  sampling  to  handle  
skewed
 
data
 
distributions,
 
ensuring
 
coverage
 
across
 
categories.
  6.  Adaptive  Optimizers :  Use  optimizers

---

s.
  6.  Adaptive  Optimizers :  Use  optimizers  like  Adam  or  RMSprop  to  quickly  converge  
on
 
embeddings,
 
adjusting
 
learning
 
rates
 
for
 
popular
 
vs.
 
rare
 
categories.
  7.  OOV  Handling :  Implement  an  "OOV"  embedding  or  use  hash  embeddings  to  
represent
 
unseen
 
categories
 
efﬁciently.
  8.  Hash  and  Hierarchical  Embeddings:  Use  hash  or  hierarchical  embeddings  for  
ultra-high
 
cardinality
 
features,
 
reducing
 
memory
 
while
 
capturing
 
struct

---

,
 
reducing
 
memory
 
while
 
capturing
 
structure.
  
Q10.  In  scenarios  where  an  LLM  encounters  out-of-vocabulary  words  
during
 
embedding
 
generation,
 
propose
 
strategies
 
for
 
handling
 
such
 
cases.
 When  a  large  language  model  (LLM)  encounters  out-of-vocabulary  (OOV)  words  
during
 
embedding
 
generation,
 
handling
 
these
 
words
 
effectively
 
is
 
crucial
 
for
 
maintaining
 
performance.
 
 
Here
 
are
 
strategies
 
to
 
manage
 
OOV
 
words: 1.  Subwo

---

strategies
 
to
 
manage
 
OOV
 
words: 1.  Subword  Tokenization:  Techniques  like  Byte-Pair  Encoding  (BPE)  and  
WordPiece
 
split
 
words
 
into
 
smaller
 
units
 
(subwords
 
or
 
characters),
 
reducing
 
the
 
likelihood
 
of
 
OOV
 
words.
 
This
 
way,
 
even
 
unknown
 
words
 
are
 
represented
 
by
 
subword
 
embeddings,
 
which
 
capture
 
meaningful
 
parts
 
of
 
words.
  2.  Character-Level  Embeddings:  For  languages  with  many  unique  words  or  
morphology,
 
charact

---

h  many  unique  words  or  
morphology,
 
character-level
 
embeddings
 
create
 
vectors
 
by
 
analyzing
 
words
 
as
 
sequences
 
of
 
characters
,
 
making
 
OOV
 
handling
 
more
 
robust.
 
Models
 
like
 
CharCNN
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   104 
 
and  CharRNN  can  generate  embeddings  directly  from  characters,  retaining  
information
 
from
 
rare
 
or
 
unseen
 
words.
  3.  Embedding  I

---

m
 
rare
 
or
 
unseen
 
words.
  3.  Embedding  Imputation:  When  encountering  an  OOV  word,  some  models  use  the  
mean
 
or
 
random
 
embeddings
 
of
 
existing
 
vocabulary
 
as
 
proxies.
 
While
 
less
 
precise,
 
this
 
maintains
 
consistent
 
vector
 
dimensions
 
without
 
large
 
memory
 
overhead.
  4.  Context-Based  OOV  Generation:  Transformer-based  LLMs  can  generate  
embeddings
 
on
 
the
 
ﬂy
 
by
 
leveraging
 
contextual
 
information
 
around
 
OOV
 
words.
 
Thi

---

extual
 
information
 
around
 
OOV
 
words.
 
This
 
dynamic
 
approach
 
enables
 
meaningful
 
representations
 
for
 
unseen
 
words
 
based
 
on
 
surrounding
 
context,
 
enhancing
 
adaptability
 
in
 
real-time
 
processing.
  5.  Fallback  to  Phonetic  or  Morphological  Similarity:  If  subword  units  are  not  
feasible,
 
OOV
 
words
 
can
 
be
 
matched
 
to
 
similar
 
in-vocabulary
 
words
 
based
 
on
 
phonetic
 
similarity
 
(e.g.,
 
Soundex
 
algorithm)
 
or
 
morphological

---

e.g.,
 
Soundex
 
algorithm)
 
or
 
morphological
 
similarity.
 
This
 
is
 
especially
 
useful
 
in
 
domain-speciﬁc
 
LLMs
 
where
 
certain
 
OOV
 
terms
 
may
 
have
 
close
 
alternatives.
  
Q11.  Propose  metrics  for  quantitatively  evaluating  the  quality  of  
embeddings
 
generated
 
by
 
an
 
LLM.
 
How
 
can
 
the
 
effectiveness
 
of
 
embeddings
 
be
 
assessed
 
in
 
tasks
 
like
 
semantic
 
similarity
 
or
 
information
 
retrieval? To  evaluate  the  quality  of  embedding

---

trieval? To  evaluate  the  quality  of  embeddings  generated  by  a  large  language  model  (LLM),  
several
 
quantitative
 
metrics
 
are
 
commonly
 
used.
 
These
 
metrics
 
assess
 
embeddings’
 
effectiveness
 
in
 
preserving
 
semantic
 
relationships,
 
distinguishing
 
meaning,
 
and
 
supporting
 
downstream
 
tasks
 
like
 
semantic
 
similarity
 
and
 
information
 
retrieval.
 
  
Here
 
are
 
some
 
key
 
metrics
 
and
 
how
 
they
 
apply
 
to
 
these
 
tasks: 1.  Cosine  Sim

---

they
 
apply
 
to
 
these
 
tasks: 1.  Cosine  Similarity :  Measures  the  cosine  of  the  angle  between  two  embeddings,  
ranging
 
from
 
-1
 
(opposite)
 
to
 
1
 
(identical).
 
Measures
 
similarity
 
between
 
embeddings,
 
useful
 
in
 
assessing
 
semantic
 
consistency.
 2.  Mean  Reciprocal  Rank  (MRR)  and  Precision  k:  Mean  Reciprocal  Rank  (MRR)  
evaluates
 
the
 
rank
 
position
 
of
 
the
 
ﬁrst
 
relevant
 
result
 
for
 
a
 
query,
 
while
 
Precision@k
 
measures
 
t

---

a
 
query,
 
while
 
Precision@k
 
measures
 
the
 
proportion
 
of
 
relevant
 
items
 
in
 
the
 
top-k
 
retrieved
 
results.
 
Assesses
 
effectiveness
 
in
 
retrieval
 
tasks
 
by
 
checking
 
how
 
well
 
embeddings
 
rank
 
relevant
 
items.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   105 
 
3.  Average  Pairwise  Distance  (APD)  and  Clustering  Metrics:  APD  measures  the  
average
 
distance
 
between

---

PD  measures  the  
average
 
distance
 
between
 
all
 
pairs
 
of
 
embeddings
 
within
 
clusters.
 
Silhouette
 
Score
 
and
 
Davies-Bouldin
 
Index
 
provide
 
clustering
 
quality
 
measures.
 
Measures
 
within-cluster
 
cohesion
 
and
 
separation,
 
ideal
 
for
 
clustering
 
tasks.
 4.  Correlation  with  Human  Judgments :  Spearman  or  Pearson  correlation  between  
human-assigned
 
similarity
 
scores
 
and
 
model-computed
 
similarities.
 
This
 
similarity
 
scores
 
show
 
ho

---

larities.
 
This
 
similarity
 
scores
 
show
 
how
 
well
 
embeddings
 
align
 
with
 
semantic
 
understanding.
  
Q12.  Explain  the  concept  of  triplet  loss  in  the  context  of  embedding  
learning. Triplet  loss  is  a  loss  function  used  in  embedding  learning  to  train  models  that  
create
 
meaningful
 
vector
 
representations.
 
It
 
is
 
particularly
 
useful
 
for
 
tasks
 
such
 
as
 
face
 
recognition,
 
image
 
retrieval,
 
and
 
semantic
 
similarity,
 
where
 
the

---

eval,
 
and
 
semantic
 
similarity,
 
where
 
the
 
goal
 
is
 
to
 
cluster
 
similar
 
items
 
while
 
separating
 
dissimilar
 
ones
 
in
 
the
 
embedding
 
space
.
  The  loss  operates  on  triplets  of  inputs:  1.  Anchor  (A):  The  sample  for  which  we  want  the  embedding.  2.  Positive  (P):  A  sample  that  is  similar  to  the  anchor.  3.  Negative  (N):  A  sample  that  is  dissimilar  to  the  anchor.   The  objective  of  triplet  loss  is  to  minimize  the  distance  be

---

triplet  loss  is  to  minimize  the  distance  between  the  anchor  and  
positive
 
embeddings,
 
while
 
maximizing
 
the
 
distance
 
between
 
the
 
anchor
 
and
 
negative
 
embeddings,
 
with
 
a
 
margin
 
to
 
ensure
 
separation.
 
The
 
loss
 
function
 
is
 
deﬁned
 
as:
    where   is  the  distance  function  (e.g.,  Euclidean  or  cosine  distance),  and  the  margin  enforces  that  the  anchor-negative  distance  is  greater  than  the  anchor-positive  
distance
 
by
 
at
 
le

---

an  the  anchor-positive  
distance
 
by
 
at
 
least
 
a
 
speciﬁed
 
amount.
  Triplet  loss  creates  a  well-structured  embedding  space  by  ensuring  that  similar  
items
 
are
 
close
 
together
 
and
 
dissimilar
 
items
 
are
 
farther
 
apart.
 
This
 
approach
 
helps
 
models
 
learn
 
more
 
discriminative
 
embeddings,
 
which
 
is
 
beneﬁcial
 
in
 
tasks
 
requiring
 
ﬁne-grained
 
similarity
 
comparisons.
 
Triplet
 
loss
 
enables
 
better
 
generalization
 
by
 
enforcing

---

ables
 
better
 
generalization
 
by
 
enforcing
 
relative
 
distance
 
relationships
 
between
 
samples,
 
improving
 
the
 
model's
 
ability
 
to
 
distinguish
 
between
 
subtle
 
differences
 
in
 
data.
  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   106 
 
Q13.  In  loss  functions  like  triplet  loss  or  contrastive  loss,  what  is  the  
signiﬁcance
 
of
 
the
 
margin
 
parameter? In  loss  functions  like

---

e
 
margin
 
parameter? In  loss  functions  like  triplet  loss  and  contrastive  loss,  the  margin  parameter  sets  a  
minimum
 
desired
 
distance
 
between
 
positive
 
and
 
negative
 
pairs
 
or
 
between
 
anchor-positive
 
and
 
anchor-negative
 
pairs.
 
 The  margin  helps  to  create  a  separation  or  "margin"  between  similar  and  dissimilar  
pairs,
 
encouraging
 
the
 
model
 
to
 
learn
 
embeddings
 
where
 
similar
 
items
 
are
 
closer
 
together,
 
and
 
dissimilar

---

ms
 
are
 
closer
 
together,
 
and
 
dissimilar
 
items
 
are
 
farther
 
apart
 
by
 
at
 
least
 
this
 
distance.
  In  triplet  loss,  for  instance,  the  loss  only  contributes  when  the  distance  between  
the
 
anchor-positive
 
pair
 
is
 
not
 
sufﬁciently
 
smaller
 
than
 
the
 
distance
 
between
 
the
 
anchor-negative
 
pair
 
by
 
the
 
margin
 
value.
 
This
 
helps
 
prevent
 
the
 
model
 
from
 
collapsing
 
all
 
distances
 
to
 
zero
 
and
 
gives
 
it
 
a
 
target
 
fo

---

s
 
to
 
zero
 
and
 
gives
 
it
 
a
 
target
 
for
 
meaningful
 
separability
 
in
 
the
 
learned
 
space.
 
 Similarly,  in  contrastive  loss,  the  margin  penalizes  negative  pairs  only  if  they  are  
within
 
this
 
margin,
 
reinforcing
 
separation
 
without
 
pushing
 
dissimilar
 
pairs
 
unnecessarily
 
far
 
apart.
  
Q14.  Discuss  challenges  related  to  overﬁtting  in  LLMs  during  training.  
What
 
strategies
 
and
 
regularization
 
techniques
 
are
 
effective
 
in
 
p

---

larization
 
techniques
 
are
 
effective
 
in
 
preventing
 
overﬁtting,
 
especially
 
when
 
dealing
 
with
 
massive
 
language
 
corpora? Overﬁtting  in  LLMs  occurs  because  large  models  can  memorize  patterns  instead  of  
generalizing,
 
especially
 
with
 
redundant
 
or
 
biased
 
data.
 
 
 
Eﬀective
 
strategies
 
to
 
prevent
 
this
 
include: 1.  Dropout  and  Layer  Dropout:  Randomly  dropping  units  or  layers  reduces  reliance  
on
 
speciﬁc
 
features,
 
encouraging

---

liance  
on
 
speciﬁc
 
features,
 
encouraging
 
generalization.
 2.  Data  Augmentation:  Adding  noise  (e.g.,  word  masking)  makes  models  robust  to  
variations.
 3.  L2  Regularization:  Penalizes  large  weights  to  avoid  over-complex  mappings.  4.  Diverse  Data:  Curating  diverse  and  less-biased  corpora  reduces  pattern  
memorization.
 5.  Early  Stopping  and  Checkpoint  Averaging :  Stop  training  early  based  on  
validation;
 
averaging
 
checkpoints
 
smooths
 
out

---

dation;
 
averaging
 
checkpoints
 
smooths
 
out
 
noise.
 6.  Knowledge  Distillation :  Training  a  smaller  student  model  from  a  large  teacher  
reduces
 
detail
 
memorization.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   107 
 
7.  Parameter  Sharing:  Reusing  weights  across  layers  pushes  models  to  capture  
general
 
patterns.
  
Q15.  Large  Language  Models  often  require  careful  tuning  of  lea

---

Models  often  require  careful  tuning  of  learning  
rates.
 
How
 
do
 
you
 
adapt
 
learning
 
rates
 
during
 
training
 
to
 
ensure
 
stable
 
convergence
 
and
 
efﬁcient
 
learning
 
for
 
LLMs? Adapting  learning  rates  during  LLM  training  is  crucial  for  stable  convergence  and  
efﬁcient
 
learning.
 
  
Eﬀective
 
strategies
 
include: 1.  Learning  Rate  Warm-Up:  Start  with  a  small  learning  rate  and  gradually  increase  it  
over
 
initial
 
steps.
 
This
 
helps

---

rease  it  
over
 
initial
 
steps.
 
This
 
helps
 
prevent
 
instability
 
from
 
large
 
updates
 
early
 
on
 
when
 
weights
 
are
 
random.
 2.  Learning  Rate  Scheduling:  Use  decay  schedules  like  cosine  decay  or  exponential  
decay
 
to
 
gradually
 
lower
 
the
 
learning
 
rate
 
as
 
training
 
progresses,
 
which
 
stabilizes
 
learning
 
and
 
reﬁnes
 
weights
 
in
 
later
 
stages.
 3.  Adaptive  Optimizers  (e.g.,  AdamW):  Optimizers  like  AdamW  adjust  learning  rates

---

Optimizers  like  AdamW  adjust  learning  rates  
individually
 
for
 
each
 
parameter
 
based
 
on
 
gradient
 
history,
 
helping
 
handle
 
the
 
large
 
parameter
 
space
 
in
 
LLMs.
 4.  Gradient  Clipping:  Combined  with  learning  rate  tuning,  gradient  clipping  
prevents
 
spikes
 
that
 
can
 
destabilize
 
training,
 
especially
 
with
 
high
 
learning
 
rates.
 5.  Cyclic  Learning  Rates:  Cycling  learning  rates  up  and  down  periodically  can  help  
escape
 
local
 
mi

---

own  periodically  can  help  
escape
 
local
 
minima
 
and
 
adapt
 
to
 
different
 
training
 
phases,
 
improving
 
convergence.
  
Q16.  When  generating  sequences  with  LLMs,  how  can  you  handle  long  
context
 
lengths
 
efﬁciently?
 
Discuss
 
techniques
 
for
 
managing
 
long
 
inputs
 
during
 
real-time
 
inference.
 Handling  long  context  lengths  efﬁciently  in  LLMs  during  real-time  inference  is  
challenging
 
but
 
achievable
 
through
 
several
 
key
 
techniques:

---

chievable
 
through
 
several
 
key
 
techniques:
 1.  Sliding  Window/Chunking:  Process  long  sequences  in  overlapping  chunks  
or
 
"windows,"
 
retaining
 
key
 
information
 
while
 
discarding
 
irrelevant
 
details.
 
This
 
approach
 
allows
 
focus
 
on
 
recent
 
context
 
without
 
overloading
 
the
 
model
 
with
 
the
 
entire
 
sequence.
   

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   108 
 
2.  Attenti

---

ive  AI  Interview  Questions   108 
 
2.  Attention  Sparsity:  Use  sparse  or  block-sparse  attention  mechanisms  (e.g.,  
Longformer,
 
BigBird)
 
to
 
limit
 
attention
 
to
 
nearby
 
tokens
 
or
 
predeﬁned
 
blocks,
 
reducing
 
computational
 
cost
 
while
 
preserving
 
relevant
 
context.
 3.  Memory-Enhanced  Models:  Some  models,  like  Transformer-XL ,  incorporate  
memory
 
layers
 
to
 
cache
 
and
 
reuse
 
hidden
 
states
 
across
 
chunks,
 
enabling
 
effective
 
handling

---

across
 
chunks,
 
enabling
 
effective
 
handling
 
of
 
longer
 
dependencies
 
without
 
recomputing
 
them.
 4.  Retrieval-Augmented  Generation:  Incorporate  retrieval  mechanisms  to  
fetch
 
relevant
 
information
 
from
 
an
 
external
 
database,
 
reducing
 
the
 
need
 
for
 
the
 
model
 
to
 
maintain
 
extensive
 
context
 
within
 
its
 
own
 
attention
 
window.
 5.  Efﬁcient  Attention  Mechanisms :  Use  alternative  attention  methods  like  
linearized
 
attention
 
(e.g.,

---

methods  like  
linearized
 
attention
 
(e.g.,
 
Linformer,
 
Reformer)
 
that
 
approximate
 
or
 
restructure
 
the
 
attention
 
mechanism
 
to
 
make
 
it
 
computationally
 
efﬁcient
 
even
 
for
 
long
 
sequences.
  
Q17.  What  evaluation  metrics  can  be  used  to  judge  LLM  generation  
quality?
 To  evaluate  the  quality  of  LLM-generated  text,  a  range  of  metrics  can  be  applied,  
focusing
 
on
 
both
 
linguistic
 
quality
 
and
 
task-speciﬁc
 
performance:
  1.  Per

---

lity
 
and
 
task-speciﬁc
 
performance:
  1.  Perplexity:  Measures  how  well  the  model  predicts  the  likelihood  of  test  
data.
 
Lower
 
perplexity
 
suggests
 
the
 
model
 
has
 
better
 
general
 
language
 
understanding,
 
but
 
it
 
doesn't
 
capture
 
all
 
aspects
 
of
 
generation
 
quality.
 2.  BLEU  (Bilingual  Evaluation  Understudy):  Common  in  translation ,  BLEU  
compares
 
n-grams
 
in
 
generated
 
and
 
reference
 
texts,
 
assessing
 
similarity.
 
It’s
 
useful

---

texts,
 
assessing
 
similarity.
 
It’s
 
useful
 
but
 
limited
 
for
 
creative
 
or
 
diverse
 
responses
 
due
 
to
 
its
 
focus
 
on
 
exact
 
matches.
 3.  ROUGE  (Recall-Oriented  Understudy  for  Gisting  Evaluation):  Typically  used  
for
 
summarization
,
 
ROUGE
 
calculates
 
overlap
 
in
 
n-grams
 
or
 
sequences,
 
emphasizing
 
recall.
 
Variants
 
like
 
ROUGE-1,
 
ROUGE-2,
 
and
 
ROUGE-L
 
capture
 
different
 
aspects
 
of
 
similarity.
 4.  METEOR  (Metric  for  Evaluatio

---

similarity.
 4.  METEOR  (Metric  for  Evaluation  of  Translation  with  Explicit  ORdering):  
Improves
 
on
 
BLEU
 
by
 
considering
 
synonymy,
 
stemming,
 
and
 
reordering,
 
making
 
it
 
more
 
ﬂexible
 
for
 
evaluating
 
semantic
 
similarity.
 5.  BERTScore:  Uses  pre-trained  embeddings  to  evaluate  semantic  similarity  
between
 
generated
 
and
 
reference
 
text,
 
capturing
 
meaning
 
more
 
accurately
 
than
 
surface-level
 
metrics
 
like
 
BLEU
 
or
 
ROUGE.
 

MD  A

---

vel
 
metrics
 
like
 
BLEU
 
or
 
ROUGE.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   109 
 
6.  Precision  and  Recall:  Applied  when  speciﬁc  information  or  entities  need  to  
be
 
mentioned,
 
e.g.,
 
for
 
factual
 
responses.
 
Measures
 
how
 
accurately
 
and
 
comprehensively
 
relevant
 
content
 
is
 
included.
 7.  Human  Evaluation:  Uses  criteria  like  coherence,  ﬂuency,  relevance,  and  
factual

---

ke  coherence,  ﬂuency,  relevance,  and  
factuality,
 
providing
 
holistic
 
quality
 
assessment.
 
Common
 
techniques
 
include
 
Likert
 
scales,
 
comparative
 
rankings,
 
and
 
paired
 
comparisons.
 8.  Diversity  Metrics  (e.g.,  Self-BLEU,  Distinct-n):  Measures  the  variety  of  
generated
 
outputs,
 
with
 
Self-BLEU
 
assessing
 
overlap
 
across
 
generations
 
(lower
 
is
 
better)
 
and
 
Distinct-n
 
counting
 
unique
 
n-grams,
 
useful
 
for
 
judging
 
creative
 
or
 
c

---

rams,
 
useful
 
for
 
judging
 
creative
 
or
 
conversational
 
responses.
 9.  Factual  Consistency  (e.g.,  FactCC,  QA-based  metrics):  Measures  factual  
accuracy,
 
especially
 
important
 
in
 
knowledge-heavy
 
tasks.
 
QA-based
 
metrics
 
assess
 
whether
 
generated
 
text
 
provides
 
correct
 
answers
 
to
 
questions
 
about
 
the
 
content.
  
Q18.  Hallucination  in  LLMs  is  a  known  issue,  how  can  you  evaluate  and  
mitigate
 
it? Hallucination  in  LLMs—when  models

---

tigate
 
it? Hallucination  in  LLMs—when  models  produce  false  or  nonsensical  information—can  
be
 
a
 
critical
 
issue,
 
especially
 
in
 
high-stakes
 
applications.
 
Here's
 
how
 
it
 
can
 
be
 
evaluated
 
and
 
mitigated:
  
Evaluation
 
Techniques ➢  Factual  Consistency  Metrics:  Use  metrics  like  FactCC  or  FEVER,  which  
compare
 
generated
 
statements
 
to
 
factual
 
references,
 
to
 
detect
 
inconsistencies.
 
These
 
metrics
 
are
 
especially
 
helpful
 
in
 
su

---

e
 
metrics
 
are
 
especially
 
helpful
 
in
 
summarization
 
and
 
question-answering
 
tasks.
 ➢  QA-Based  Evaluation:  Convert  generated  text  into  questions  and  evaluate  
whether
 
it
 
provides
 
correct
 
answers.
 
This
 
helps
 
detect
 
factual
 
inconsistencies
 
and
 
test
 
the
 
reliability
 
of
 
the
 
content.
 ➢  Human  Evaluation:  Conduct  human  evaluations  where  annotators  judge  the  
factual
 
accuracy
 
of
 
model
 
outputs.
 
This
 
often
 
provides
 
the
 
mo

---

el
 
outputs.
 
This
 
often
 
provides
 
the
 
most
 
reliable
 
assessment,
 
especially
 
for
 
complex
 
topics.
 ➢  Retrieval-Augmented  Evaluation:  Use  information  retrieval  techniques  to  
cross-check
 
generated
 
facts
 
against
 
a
 
veriﬁed
 
source,
 
ﬂagging
 
unsupported
 
or
 
unveriﬁable
 
content.
 ➢  Conﬁdence  Scoring:  Some  models  generate  a  conﬁdence  score  based  on  the  
likelihood
 
of
 
tokens,
 
which
 
can
 
be
 
used
 
to
 
gauge
 
whether
 
the
 
model
 
"

---

be
 
used
 
to
 
gauge
 
whether
 
the
 
model
 
"trusts"
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   110 
 
its  own  answers.  Low-conﬁdence  responses  can  be  ﬂagged  for  further  
review.
  
Mitigation
 
Techniques 1.  Retrieval-Augmented  Generation  (RAG):  Integrate  retrieval  systems  to  pull  
relevant
 
facts
 
from
 
an
 
external
 
knowledge
 
base
 
(e.g.,
 
a
 
search
 
engine
 
or
 
database)
 
in

---

.g.,
 
a
 
search
 
engine
 
or
 
database)
 
in
 
real
 
time,
 
reducing
 
reliance
 
on
 
potentially
 
outdated
 
or
 
erroneous
 
internal
 
knowledge.
 2.  Prompt  Engineering  and  Instruction  Tuning:  Guide  the  model  with  prompts  
that
 
encourage
 
accuracy
 
and
 
discourage
 
speculative
 
responses.
 
For
 
example,
 
prompts
 
can
 
specify
 
"answer
 
only
 
if
 
certain"
 
or
 
encourage
 
citing
 
sources.
 3.  Post-Processing  and  Veriﬁcation :  Apply  rule-based  or  add

---

ng  and  Veriﬁcation :  Apply  rule-based  or  additional  models  to  
verify
 
facts
 
after
 
generation,
 
ﬁltering
 
out
 
or
 
ﬂagging
 
unsupported
 
claims
 
before
 
they
 
reach
 
the
 
user.
 4.  Knowledge  Distillation  and  Fine-Tuning  on  Factual  Data:  Fine-tune  the  
model
 
on
 
high-quality,
 
veriﬁed
 
sources
 
to
 
reinforce
 
factual
 
information
 
and
 
reduce
 
hallucination
 
tendencies.
 
Knowledge
 
distillation
 
from
 
smaller,
 
fact-focused
 
models
 
may
 
als

---

rom
 
smaller,
 
fact-focused
 
models
 
may
 
also
 
help
 
transfer
 
a
 
bias
 
toward
 
factual
 
accuracy.
 5.  Conﬁdence  Thresholding:  Set  a  minimum  conﬁdence  threshold,  only  
generating
 
responses
 
when
 
the
 
model
 
meets
 
this
 
criterion,
 
reducing
 
the
 
risk
 
of
 
hallucinated
 
output.
 6.  Hybrid  Architectures   Use  modular  approaches  where  LLMs  work  alongside  
structured
 
knowledge
 
systems
 
(e.g.,
 
knowledge
 
graphs
 
or
 
databases)
 
to
 
ensure
 
f

---

ledge
 
graphs
 
or
 
databases)
 
to
 
ensure
 
factual
 
accuracy
 
on
 
critical
 
information.
  
Q19.  What  is  a  mixture  of  expert  models? A  mixture  of  expert  (MoE)  models  is  an  architecture  where  multiple  specialized  
models
 
(experts)
 
are
 
trained
 
for
 
speciﬁc
 
tasks
 
or
 
data
 
types.
 
 A  gating  network  dynamically  selects  which  experts  to  activate  for  each  input,  
using
 
only
 
a
 
subset
 
of
 
experts,
 
which
 
enables
 
efﬁcient,
 
sparse

---

experts,
 
which
 
enables
 
efﬁcient,
 
sparse
 
activation.
 
 This  design  allows  MoE  models  to  scale  to  massive  sizes,  improve  specialization,  
and
 
reduce
 
computational
 
costs
 
by
 
focusing
 
on
 
relevant
 
expertise
 
for
 
each
 
input.
 
MoE
 
models
 
are
 
widely
 
used
 
in
 
NLP
 
and
 
other
 
ﬁelds
 
needing
 
specialized
 
processing,
 
like
 
recommendation
 
systems
 
and
 
vision.
 

MD  ARBAAZ  KHAN

---

Generative  AI  Interview  Questions   111 
 
  
Q20.  Why  might  over-reliance  on  perplexity  as  a  metric  be  problematic  in  
evaluating
 
LLMs?
 
What
 
aspects
 
of
 
language
 
understanding
 
might
 
it
 
overlook? Over-reliance  on  perplexity  as  an  evaluation  metric  for  LLMs  can  be  problematic  
because,
 
while
 
it
 
measures
 
how
 
well
 
a
 
model
 
predicts
 
the
 
probability
 
of
 
a
 
sequence,
 
it
 
primarily

---

probability
 
of
 
a
 
sequence,
 
it
 
primarily
 
focuses
 
on
 
syntactic
 
ﬂuency
 
and
 
likelihood
 
rather
 
than
 
broader
 
aspects
 
of
 
language
 
understanding.
 
  
Here’s
 
what
 
perplexity
 
may
 
overlook: 1.  Semantic  Accuracy:  Perplexity  doesn’t  evaluate  whether  the  content  is  
factually
 
correct
 
or
 
contextually
 
accurate.
 
A
 
low-perplexity
 
output
 
might
 
be
 
ﬂuent
 
but
 
could
 
contain
 
hallucinated
 
or
 
incorrect
 
information.
 2.  Coherence  a

---

or
 
incorrect
 
information.
 2.  Coherence  and  Consistency:  Perplexity  scores  individual  token  
probabilities,
 
so
 
it
 
may
 
not
 
capture
 
long-range
 
coherence
 
or
 
logical
 
consistency
 
across
 
a
 
full
 
response.
 
The
 
model
 
might
 
produce
 
sentences
 
that
 
are
 
individually
 
likely
 
but
 
incoherent
 
together.
 3.  Relevance  and  Speciﬁcity:  Perplexity  favors  common  language  patterns ,  so  
it
 
might
 
overlook
 
whether
 
the
 
response
 
is
 
sp

---

ht
 
overlook
 
whether
 
the
 
response
 
is
 
speciﬁcally
 
relevant
 
or
 
uniquely
 
tailored
 
to
 
the
 
input
 
prompt.
 
This
 
could
 
lead
 
to
 
generic
 
answers
 
that
 
sound
 
ﬂuent
 
but
 
lack
 
depth
 
or
 
precision.
 4.  Creativity  and  Diversity:  High-perplexity  outputs  might  be  discouraged,  
even
 
when
 
they
 
reﬂect
 
creative
 
or
 
novel
 
responses.
 
This
 
limits
 
the
 
model's
 
ability
 
to
 
generate
 
diverse
 
answers,
 
especially
 
important
 
in
 
ta

---

erse
 
answers,
 
especially
 
important
 
in
 
tasks
 
like
 
storytelling
 
or
 
open-ended
 
conversation.
   

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   112 
 
5.  Pragmatic  Understanding:  Perplexity  doesn’t  measure  the  model's  grasp  of  
social
 
norms,
 
tone,
 
or
 
other
 
contextual
 
nuances
 
essential
 
for
 
effective
 
communication,
 
especially
 
in
 
sensitive
 
or
 
ambiguous
 
topics.
  
Q21.

---

in
 
sensitive
 
or
 
ambiguous
 
topics.
  
Q21.  How  do  models  like  Stability  Diffusion  leverage  LLMs  to  understand  
complex
 
text
 
prompts
 
and
 
generate
 
high-quality
 
images?(internal
 
mechanism
 
of
 
stable
 
diffusion
 
model) Stability  AI's  models,  such  as  Stable  Diffusion,  integrate  large  language  models  
(LLMs)
 
to
 
enhance
 
image
 
generation
 
by
 
interpreting
 
complex
 
text
 
prompts
 
more
 
effectively.
 
This
 
capability
 
helps
 
to
 
map
 
de

---

ively.
 
This
 
capability
 
helps
 
to
 
map
 
detailed
 
text
 
prompts
 
into
 
high-quality,
 
contextually
 
accurate
 
images.
 
  
Here’s
 
a
 
breakdown
 
of
 
the
 
mechanism
 
that
 
underlies
 
Stable
 
Diﬀusion: 1.  Text  Encoding  with  Large  Language  Models  (LLMs)  ➔  Text  Encoding  with  CLIP :  Stable  Diffusion  typically  leverages  CLIP  
(Contrastive
 
Language–Image
 
Pretraining),
 
a
 
model
 
trained
 
to
 
associate
 
images
 
with
 
their
 
textual
 
descriptions.

---

images
 
with
 
their
 
textual
 
descriptions.
 
Given
 
a
 
prompt,
 
CLIP
 
encodes
 
the
 
text
 
into
 
a
 
latent
 
vector,
 
a
 
numerical
 
representation
 
of
 
the
 
text's
 
meaning.
 
This
 
encoded
 
vector
 
captures
 
both
 
literal
 
and
 
nuanced
 
aspects
 
of
 
the
 
prompt,
 
allowing
 
the
 
model
 
to
 
understand
 
even
 
complex
 
or
 
stylistic
 
instructions.
 ➔  Prompt  Contextualization:  When  a  prompt  contains  multiple  
elements
 
(like
 
"a
 
futuristic
 
city

---

ltiple  
elements
 
(like
 
"a
 
futuristic
 
cityscape
 
under
 
a
 
purple
 
sky,
 
in
 
the
 
style
 
of
 
art
 
nouveau"),
 
the
 
LLM
 
within
 
CLIP
 
encodes
 
these
 
elements
 
to
 
capture
 
associations
 
and
 
hierarchies
 
between
 
concepts,
 
understanding
 
that
 
"futuristic"
 
applies
 
to
 
the
 
"cityscape"
 
and
 
"art
 
nouveau"
 
is
 
a
 
stylistic
 
instruction.
 2.  Latent  Diffusion  Process  ➔  Noise  Addition  and  Denoising  Process:  Stable  Diffusion  operates  in

---

oising  Process:  Stable  Diffusion  operates  in  a  
"latent
 
space,"
 
which
 
is
 
a
 
compressed
 
version
 
of
 
the
 
image
 
space
 
where
 
features
 
are
 
represented
 
as
 
simpler
 
mathematical
 
constructs.
 
In
 
latent
 
diffusion,
 
a
 
Gaussian
 
noise
 
is
 
iteratively
 
added
 
to
 
an
 
initially
 
empty
 
(or
 
fully
 
noisy)
 
latent
 
space
 
representation.
 ➔  Guided  Denoising  Using  Text  Encoding:  During  the  iterative  
denoising
 
process,
 
the
 
model
 
ref

---

erative  
denoising
 
process,
 
the
 
model
 
references
 
the
 
encoded
 
prompt
 
from
 
CLIP
 
to
 
guide
 
each
 
step
 
of
 
denoising.
 
This
 
guidance
 
shapes
 
the
 
latent
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   113 
 
space  representation  toward  an  image  that  matches  the  meaning  of  
the
 
prompt,
 
progressively
 
reﬁning
 
details,
 
composition,
 
and
 
style
 
as
 
the
 
noise
 
is
 
remov

---

ion,
 
and
 
style
 
as
 
the
 
noise
 
is
 
removed.
 3.  Cross-Attention  Layers  ➔  Attention  Mechanism  for  Fine  Detail:  Stable  Diffusion  uses  
cross-attention
 
layers
 
to
 
align
 
elements
 
of
 
the
 
encoded
 
text
 
prompt
 
with
 
speciﬁc
 
parts
 
of
 
the
 
latent
 
image.
 
This
 
enables
 
precise
 
control
 
over
 
the
 
generated
 
content,
 
particularly
 
in
 
terms
 
of
 
spatial
 
relationships
 
and
 
stylistic
 
cues.
 
 For  instance,  if  the  prompt  speciﬁes  "

---

s.
 
 For  instance,  if  the  prompt  speciﬁes  "a  red  bird  in  a  golden  forest,"  the  
attention
 
layers
 
help
 
ensure
 
that
 
"red"
 
applies
 
to
 
the
 
bird,
 
not
 
the
 
forest.
 ➔  Balancing  Text  and  Visual  Cues:  The  cross-attention  layers  balance  
literal
 
visual
 
cues
 
from
 
the
 
prompt
 
with
 
stylistic
 
and
 
contextual
 
interpretation,
 
so
 
even
 
abstract
 
prompts
 
or
 
highly
 
speciﬁc
 
descriptors
 
are
 
visualized
 
accurately.
 4.  Sampling  Te

---

are
 
visualized
 
accurately.
 4.  Sampling  Techniques  for  Enhanced  Realism  ➔  Sampling  and  Iterative  Reﬁnement:  After  the  main  diffusion  process,  
various
 
sampling
 
techniques
 
(e.g.,
 
DDIM
 
or
 
PNDM
 
samplers)
 
are
 
applied
 
to
 
increase
 
image
 
sharpness
 
and
 
reduce
 
artifacts.
 
 These  methods  optimize  the  ﬁnal  image  to  appear  more  natural  and  
realistic
 
by
 
reﬁning
 
edges,
 
textures,
 
and
 
color
 
gradients
 
in
 
the
 
latent
 
space.
 ➔

---

color
 
gradients
 
in
 
the
 
latent
 
space.
 ➔  Optional  Post-Processing:  In  some  cases,  Stable  Diffusion  might  apply  
a
 
slight
 
additional
 
post-processing
 
step
 
to
 
ﬁne-tune
 
lighting
 
or
 
contrast,
 
especially
 
for
 
speciﬁc
 
styles
 
or
 
realistic
 
textures.
 5.  Image  Generation  and  Decoding  ➔  Decoding  from  Latent  Space:  Once  the  denoising  process  completes,  
the
 
latent
 
representation
 
is
 
decoded
 
back
 
into
 
an
 
image.
 
The
 
latent-to

---

oded
 
back
 
into
 
an
 
image.
 
The
 
latent-to-image
 
decoder
 
maps
 
the
 
ﬁnal
 
latent
 
features
 
into
 
pixel
 
values,
 
producing
 
the
 
image
 
seen
 
by
 
the
 
user.
 ➔  Iterative  Adjustment  with  Prompt  Feedback:  If  speciﬁed,  Stable  
Diffusion
 
can
 
also
 
run
 
additional
 
iterations
 
where
 
it
 
re-references
 
the
 
original
 
prompt
 
and
 
adjusts
 
the
 
output
 
image
 
accordingly,
 
ensuring
 
high
 
ﬁdelity
 
to
 
the
 
prompt's
 
complexity
 
and
 
styli

---

y
 
to
 
the
 
prompt's
 
complexity
 
and
 
stylistic
 
intent.
     

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   114 
 
                    
Retrieval-Augmented  Generation                 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   115 
 
Q1.  What  is  Retrieval-Augmented  Generation  (RAG)? Retrieval-Augmented  Generation  (RAG)  is  an  ad

---

Retrieval-Augmented  Generation  (RAG)  is  an  advanced  natural  language  
processing
 
(NLP)
 
framework
 
that
 
combines
 
data
 
ingestion,
 
information
 
retrieval,
 
and
 
generative
 
models
 
to
 
create
 
accurate
 
and
 
contextually
 
relevant
 
outputs.
 
It
 
enables
 
models
 
to
 
access
 
external
 
knowledge
 
dynamically,
 
ensuring
 
responses
 
are
 
grounded
 
in
 
real-world
 
data.
  
How
 
RAG
 
Works 1.  Data  Ingestion:  Data  from  diverse  sources  (e.g.,  databas

---

on:  Data  from  diverse  sources  (e.g.,  databases,  documents,  APIs)  is  
collected,
 
preprocessed,
 
and
 
indexed
 
using
 
tools
 
like
 
FAISS
 
or
 
ElasticSearch
 
to
 
create
 
a
 
searchable
 
knowledge
 
base.
 2.  Retriever:  Identiﬁes  and  retrieves  relevant  information  from  the  ingested  data  
based
 
on
 
the
 
input
 
query.
 3.  Generator:  A  generative  language  model  (e.g.,  GPT,  LLaMA)  uses  the  retrieved  
information
 
along
 
with
 
the
 
query
 
to
 
prod

---

formation
 
along
 
with
 
the
 
query
 
to
 
produce
 
a
 
coherent,
 
data-driven
 
response.
 
 
Advantages Accuracy:  Reduces  hallucination  by  grounding  outputs  in  real  data.  Up-to-Date  Knowledge:  Dynamically  integrates  the  latest  information  without  
requiring
 
retraining.
 Scalability:  Handles  vast  datasets  efﬁciently.   
Q2.  Can  you  explain  the  text  generation  difference  between  RAG  and  
direct
 
language
 
models? The  primary  difference  between  Retriev

---

models? The  primary  difference  between  Retrieval-Augmented  Generation  (RAG)  and  direct  
language
 
models
 
(LMs)
 
lies
 
in
 
how
 
they
 
generate
 
text
 
and
 
handle
 
knowledge.
 
  
Here's
 
a
 
detailed
 
comparison: 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   116 
 
  Direct  Language  Models  (LMs)  
Retrieval-Augmented  Generation  (RAG)  
Knowledge  Source  LMs  rely  entirely  on  their  pre-train

---

Source  LMs  rely  entirely  on  their  pre-trained  internal  knowledge.   This  knowledge  is  ﬁxed  post-training,  making  them  unable  to  incorporate  new  or  dynamic  information  without  retraining.  
RAG  systems  combine  internal  knowledge  of  the  language  model  with  external  data  sources.   They  dynamically  retrieve  relevant  information  from  a  knowledge  base  to  augment  their  responses,  ensuring  accuracy  and  up-to-date  content.  
Text  Generation  Process

---

up-to-date  content.  
Text  Generation  Process  Generate  responses  solely  based  on  patterns  and  information  encoded  during  training.   Risk  of  hallucination  (producing  plausible  but  incorrect  or  fabricated  information)  is  higher  due  to  limited  grounding  in  external  facts.  
First  retrieves  contextually  relevant  data  through  a  retrieval  mechanism.   Generates  text  by  combining  the  retrieved  data  with  the  model's  pre-trained  knowledge,  grounding

---

the  model's  pre-trained  knowledge,  grounding  the  response  in  real-world  information.  
Flexibility  and  Updateability  
Difﬁcult  to  update  knowledge;  requires  expensive  retraining  to  incorporate  new  information.   Limited  adaptability  to  changing  contexts  or  domain-speciﬁc  requirements.  
Easily  updated  by  modifying  or  expanding  the  external  knowledge  base  without  retraining  the  model.   Adaptable  to  speciﬁc  domains  by  feeding  the  retriever  with  r

---

iﬁc  domains  by  feeding  the  retriever  with  relevant  data  sources.  
Accuracy  May  generate  less  accurate  or  outdated  responses  due  to  reliance  on  static,  pre-trained  data.   
Enhances  accuracy  by  grounding  responses  in  retrieved,  real-time,  or  domain-speciﬁc  data.   Reduces  hallucination  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   117 
 
Limited  ability  to  verify  or  validate  output

---

Limited  ability  to  verify  or  validate  output.  
risks  by  anchoring  text  generation  to  external  facts.  
Use  Cases  General-purpose  text  generation  (e.g.,  storytelling,  creative  writing).   Scenarios  where  factual  accuracy  is  less  critical.  
Applications  requiring  high  accuracy,  contextual  relevance,  and  real-time  knowledge  (e.g.,  customer  support,  knowledge-based  Q&A,  legal  or  medical  advice).   
Q3.  What  are  some  common  applications  of  RAG  in

---

hat  are  some  common  applications  of  RAG  in  AI? Retrieval-Augmented  Generation  (RAG)  is  widely  used  in  applications  requiring  accurate,  
context-aware,
 
and
 
dynamic
 
knowledge
 
integration.
 
  
Key
 
use
 
cases
 
include: 1.  Customer  Support:  AI  chatbots  retrieving  FAQs  or  manuals  to  provide  accurate  answers.  2.  Search  Engines:  Generating  concise,  data-backed  search  results  (e.g.,  Google  SGE).  3.  Specialized  Domains:  Summarizing  legal  document

---

pecialized  Domains:  Summarizing  legal  documents,  medical  research,  or  ﬁnancial  
reports.
 4.  Education:  AI  tutors  creating  personalized  learning  materials.  5.  Content  Creation:  Generating  fact-based  articles,  blogs,  or  reports.  6.  Real-Time  Support:  Assisting  professionals  with  data-driven  decision-making.  7.  E-commerce:  Product  recommendations  and  comparisons  using  live  data.  8.  Code  Assistance:  Explaining  code  or  retrieving  documentation  for

---

aining  code  or  retrieving  documentation  for  developers.  9.  Compliance  Checks:  Automating  policy  veriﬁcation  and  reports.  10.  Personalized  Recommendations:  Tailoring  suggestions  for  entertainment  or  learning.   
Q4.  How  does  RAG  improve  the  accuracy  of  responses  in  AI  models? 
Retrieval-Augmented
 
Generation
 
(RAG)
 
improves
 
AI
 
response
 
accuracy
 
by:  1.  Accessing  Updated  Knowledge:  Retrieves  current,  relevant  information  from  an  external  
da

---

t,  relevant  information  from  an  external  
database.
 2.  Context-Speciﬁc  Retrieval:  Focuses  on  the  most  relevant  information  for  each  query.  3.  Enhancing  Niche  Topic  Comprehension:  Brings  in  specialized  details  that  a  general  
model
 
might
 
lack.
 4.  Reducing  Hallucinations:  Grounds  responses  in  real  data,  minimizing  fabricated  
answers.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions

---

Generative  AI  Interview  Questions   118 
 
5.  Providing  Interpretability:  Allows  users  to  verify  sources,  increasing  trustworthiness.   
Q5.  What  is  the  signiﬁcance  of  retrieval  models  in  RAG? In  RAG,  retrieval  models  are  crucial  because  they  fetch  relevant,  up-to-date  information  
from
 
a
 
knowledge
 
base.
 
This
 
ensures
 
the
 
generative
 
model
 
has
 
accurate
 
context,
 
reduces
 
hallucinations,
 
and
 
improves
 
response
 
precision
,

---

nations,
 
and
 
improves
 
response
 
precision
,
 
especially
 
on
 
speciﬁc
 
or
 
evolving
 
topics.
  
Q6.  What  types  of  data  sources  are  typically  used  in  RAG  systems? 
RAG
 
systems
 
typically
 
use
 
the
 
following
 
data
 
sources:  1.  Document  Databases:  Indexed  collections  of  text  documents,  articles,  or  reports.  2.  Knowledge  Bases:  Structured  data  sources  like  Wikidata  or  domain-speciﬁc  databases.  3.  Web  Content:  Public  web  pages  for  real-tim

---

Web  Content:  Public  web  pages  for  real-time  or  recent  information.  4.  Company/Internal  Data:  Proprietary  documents,  FAQs,  and  customer  support  data.  5.  Research  Papers:  Academic  or  scientiﬁc  articles  for  in-depth,  factual  responses.   
Q7.  How  does  RAG  contribute  to  the  ﬁeld  of  conversational  AI? 
RAG
 
enhances
 
conversational
 
AI
 
by:  1.  Providing  Up-to-Date  Information:  It  retrieves  real-time  knowledge,  keeping  responses  
current.
 2.  I

---

knowledge,  keeping  responses  
current.
 2.  Improving  Accuracy:  Grounding  answers  in  retrieved  data  reduces  errors  and  
hallucinations.
 3.  Enabling  Domain-Speciﬁc  Responses:  Pulls  relevant  details  from  specialized  sources,  
allowing
 
for
 
accurate
 
responses
 
on
 
niche
 
topics.
 4.  Enhancing  Contextual  Relevance:  Tailors  responses  to  the  query  by  accessing  the  most  
pertinent
 
information.
 5.  Increasing  Transparency:  Allows  users  to  trace  res

---

sing  Transparency:  Allows  users  to  trace  responses  back  to  source  information,  
building
 
trust.
  
Q8.  What  is  the  role  of  the  retrieval  component  in  RAG?  The  retrieval  component  in  RAG  ﬁnds  relevant,  up-to-date  information  from  external  
sources
 
to
 
ground
 
the
 
generative
 
model’s
 
responses,
 
improving
 
accuracy,
 
context
 
relevance,
 
and
 
reducing
 
hallucinations.
  
Q9.  How  does  RAG  handle  bias  and  misinformation? RAG  (Retrieval-Augme

---

bias  and  misinformation? RAG  (Retrieval-Augmented  Generation)  addresses  bias  and  misinformation  through  several  
key
 
strategies:
  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   119 
 
➔  External  Knowledge  Sources:  It  combines  the  language  model  with  an  external  
database,
 
pulling
 
real-time,
 
curated
 
information
 
to
 
avoid
 
outdated
 
data
 
and
 
limit
 
biases
 
embedded
 
in
 
model

---

ta
 
and
 
limit
 
biases
 
embedded
 
in
 
model
 
training
 
alone.
 ➔  Source  Selection  and  Control:  By  choosing  reliable  databases,  RAG  enables  users  to  
minimize
 
bias
 
and
 
misinformation.
 
Trusted
 
sources
 
can
 
be
 
speciﬁed,
 
allowing
 
for
 
a
 
tailored
 
balance
 
that
 
aligns
 
with
 
accuracy
 
or
 
speciﬁc
 
values.
 ➔  Lowered  Hallucination  Risk:  Since  the  model  retrieves  factual  data  instead  of  
relying
 
only
 
on
 
generation,
 
it’s
 
less
 
li

---

lying
 
only
 
on
 
generation,
 
it’s
 
less
 
likely
 
to
 
produce
 
unsupported
 
or
 
fabricated
 
information.
 ➔  Transparency  and  Citations :  RAG  can  show  sources  for  retrieved  information,  
enhancing
 
transparency
 
and
 
enabling
 
users
 
to
 
check
 
for
 
potential
 
bias
 
or
 
errors
 
in
 
sources.
 ➔  Cross-Referencing:  It  can  pull  from  multiple  sources,  helping  to  validate  the  
information
 
and
 
provide
 
a
 
balanced
 
view,
 
thus
 
reducing
 
the
 
im

---

a
 
balanced
 
view,
 
thus
 
reducing
 
the
 
impact
 
of
 
any
 
single
 
biased
 
source.
 ➔  Curation  and  Human  Oversight:  Regularly  updating  and  curating  source  data  
minimizes
 
misinformation
 
by
 
ﬁltering
 
out
 
unreliable
 
sources
 
and
 
ensuring
 
relevance.
  
Q10.  What  are  the  beneﬁts  of  using  RAG  over  other  NLP  techniques?  
RAG
 
outperforms
 
traditional
 
NLP
 
techniques
 
by: ➢  Providing  real-time  information  from  external  sources.  ➢  Reducing

---

nformation  from  external  sources.  ➢  Reducing  hallucination  through  factual  retrieval.  ➢  Controlling  bias  via  curated  sources.  ➢  Offering  transparency  with  source  citations.  ➢  Improving  memory  efﬁciency  by  not  storing  all  knowledge  in  the  model.  ➢  Customizing  for  speciﬁc  domains  with  specialized  databases.  ➢  Enhancing  answer  quality  for  complex  queries.  These  beneﬁts  make  RAG  more  accurate,  adaptable,  and  reliable.   
Q11.  Can  you  discus

---

aptable,  and  reliable.   
Q11.  Can  you  discuss  a  scenario  where  RAG  would  be  particularly  useful? 
Here
 
are
 
some
 
real
 
scenarios
 
where
 
RAG
 
proves
 
useful: ★  Financial  Services:  In  ﬁnance,  RAG  can  assist  advisors  or  clients  by  pulling  recent  
ﬁnancial
 
data,
 
regulatory
 
updates,
 
or
 
market
 
analyses
 
from
 
reliable
 
sources.
 
This
 
is
 
crucial
 
for
 
making
 
accurate,
 
informed
 
decisions
 
based
 
on
 
current
 
regulations
 
and
 
trend

---

based
 
on
 
current
 
regulations
 
and
 
trends.
 ★  Customer  Support  for  Tech  Companies:  For  complex  tech  products,  RAG  can  
enhance
 
customer
 
support
 
by
 
pulling
 
relevant
 
troubleshooting
 
steps,
 
recent
 
bug
 
ﬁxes,
 
and
 
documentation
 
from
 
internal
 
knowledge
 
bases
 
and
 
forums.
 
This
 
helps
 
support
 
agents
 
resolve
 
issues
 
efﬁciently.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Q

---

Generative  AI  Interview  Questions   120 
 
★  Legal  Research:  Lawyers  and  paralegals  can  use  RAG  to  access  recent  case  law,  
statutes,
 
and
 
legal
 
interpretations
 
from
 
legal
 
databases
 
(e.g.,
 
LexisNexis).
 
This
 
allows
 
them
 
to
 
stay
 
updated
 
on
 
new
 
precedents
 
and
 
apply
 
relevant
 
laws
 
accurately.
 ★  E-commerce  Chatbots:  In  e-commerce,  RAG-enabled  chatbots  can  provide  
real-time
 
answers
 
about
 
product
 
availab

---

real-time
 
answers
 
about
 
product
 
availability,
 
shipping
 
updates,
 
and
 
promotions
 
by
 
pulling
 
from
 
inventory
 
databases
 
and
 
recent
 
sales
 
information.
 
This
 
enhances
 
the
 
customer
 
experience
 
with
 
accurate,
 
timely
 
information.
  
Q12.  How  does  RAG  integrate  with  existing  machine  learning  pipelines? RAG  integrates  smoothly  with  existing  machine  learning  pipelines  by  adding  a  retrieval  
layer
 
that
 
enhances
 
the
 
model's
 
kno

---

al  
layer
 
that
 
enhances
 
the
 
model's
 
knowledge
 
and
 
output
 
accuracy.
 
 
Here’s
 
how
 
it
 
typically
 
works:
 1.  Data  Source  Integration:  RAG  can  connect  to  databases,  APIs,  and  document  stores,  
where
 
it
 
pulls
 
relevant
 
information
 
at
 
runtime.
 
This
 
setup
 
allows
 
the
 
RAG
 
model
 
to
 
stay
 
updated
 
without
 
retraining,
 
as
 
new
 
information
 
sources
 
can
 
be
 
added
 
to
 
the
 
retrieval
 
component.
 2.  Retrieval  and  Ranking  Lay

---

val
 
component.
 2.  Retrieval  and  Ranking  Layer:  RAG  pipelines  often  include  a  retriever  (e.g.,  dense  vector  
search
 
or
 
traditional
 
search
 
methods)
 
to
 
fetch
 
relevant
 
documents
 
and
 
a
 
ranking
 
model
 
to
 
prioritize
 
the
 
most
 
relevant
 
sources.
 
This
 
process
 
can
 
integrate
 
with
 
existing
 
NLP
 
or
 
search
 
components
 
already
 
in
 
place.
 3.  Embedding-Based  Retrieval:  RAG  uses  pretrained  models,  such  as  BERT  or  Sentence  
Trans

---

ned  models,  such  as  BERT  or  Sentence  
Transformers,
 
to
 
encode
 
queries
 
and
 
documents
 
into
 
embeddings.
 
If
 
an
 
ML
 
pipeline
 
already
 
includes
 
embeddings,
 
these
 
can
 
often
 
be
 
reused
 
or
 
ﬁne-tuned
 
speciﬁcally
 
for
 
retrieval.
 4.  Generation  Component:  The  retrieved  information  feeds  into  a  generation  model  (e.g.,  
GPT
 
or
 
BERT-based
 
transformers),
 
which
 
combines
 
retrieved
 
data
 
with
 
generative
 
capabilities
 
to
 
craft
 
mo

---

with
 
generative
 
capabilities
 
to
 
craft
 
more
 
accurate
 
and
 
context-aware
 
responses.
 
This
 
complements
 
existing
 
generative
 
components
 
in
 
the
 
pipeline
 
by
 
reducing
 
hallucinations
 
and
 
improving
 
factuality.
 5.  Feedback  Loop  and  Fine-Tuning:  RAG  can  integrate  with  feedback  mechanisms  in  ML  
pipelines,
 
allowing
 
user
 
interactions
 
(like
 
thumbs
 
up/down
 
or
 
corrections)
 
to
 
improve
 
retrieval
 
accuracy
 
and
 
ﬁne-tune
 
the
 
mode

---

retrieval
 
accuracy
 
and
 
ﬁne-tune
 
the
 
model
 
over
 
time,
 
enhancing
 
its
 
relevance
 
and
 
bias
 
control.
 6.  API-Driven  Design:  RAG  systems  are  often  modular  and  can  connect  with  existing  ML  
pipelines
 
via
 
APIs,
 
making
 
it
 
straightforward
 
to
 
add
 
RAG
 
as
 
a
 
component
 
to
 
existing
 
models
 
or
 
applications.
  
Q13.  What  challenges  does  RAG  solve  in  natural  language  processing? RAG  (Retrieval-Augmented  Generation)  addresses  key  NL

---

trieval-Augmented  Generation)  addresses  key  NLP  challenges  by  combining  
retrieval
 
and
 
generation,
 
allowing
 
models
 
to:
  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   121 
 
➢  Access  up-to-date,  external  information,  reducing  reliance  on  memorized  
knowledge.
 ➢  Improve  accuracy  in  responses  by  grounding  answers  in  speciﬁc  retrieved  
documents.
 ➢  Handle  complex  or  niche  queries

---

ocuments.
 ➢  Handle  complex  or  niche  queries  that  may  be  outside  the  model's  training  data.  ➢  Reduce  hallucinations,  as  retrieved  information  provides  context  for  generating  
factual
 
answers.
  
Q14.  How  does  the  RAG  pipeline  ensure  the  retrieved  information  is  
up-to-date? The  RAG  pipeline  ensures  up-to-date  information  by  using  a  retriever  component  that  
searches
 
a
 
dynamic,
 
frequently
 
updated
 
database
 
or
 
knowledge
 
source
 
(e.g.

---

dated
 
database
 
or
 
knowledge
 
source
 
(e.g.,
 
web
 
documents,
 
indexed
 
ﬁles).
 
This
 
retriever
 
fetches
 
the
 
latest
 
relevant
 
documents
 
or
 
passages
 
based
 
on
 
the
 
input
 
query.
 
By
 
grounding
 
its
 
responses
 
on
 
these
 
freshly
 
retrieved
 
documents,
 
the
 
RAG
 
pipeline
 
can
 
provide
 
answers
 
based
 
on
 
the
 
most
 
current
 
information
 
available
 
in
 
the
 
database.
  
Q15.  Can  you  explain  how  RAG  models  are  trained? RAG  models  a

---

in  how  RAG  models  are  trained? RAG  models  are  trained  in  two  main  stages:  retrieval  training  and  generation  training.   1.  Retriever  Training:  The  retriever,  often  based  on  a  bi-encoder  (like  a  dense  passage  
retrieval
 
model),
 
is
 
trained
 
to
 
select
 
relevant
 
documents
 
or
 
passages
 
from
 
a
 
large
 
corpus.
 
During
 
training,
 
it
 
learns
 
to
 
identify
 
and
 
rank
 
documents
 
that
 
are
 
likely
 
to
 
contain
 
information
 
relevant
 
to

---

kely
 
to
 
contain
 
information
 
relevant
 
to
 
a
 
given
 
query.
 
This
 
process
 
can
 
involve
 
contrastive
 
learning,
 
where
 
the
 
model
 
distinguishes
 
between
 
relevant
 
and
 
non-relevant
 
passages.
 2.  Generator  Training:  The  generator,  typically  a  sequence-to-sequence  model  (like  BART  
or
 
T5),
 
is
 
trained
 
to
 
generate
 
responses
 
based
 
on
 
the
 
text
 
retrieved
 
by
 
the
 
retriever.
 
In
 
training,
 
the
 
model
 
learns
 
to
 
synthesize
 
co

---

ing,
 
the
 
model
 
learns
 
to
 
synthesize
 
coherent
 
and
 
contextually
 
accurate
 
answers
 
using
 
the
 
retrieved
 
documents
 
as
 
reference.
 
Fine-tuning
 
with
 
retrieved
 
information
 
helps
 
the
 
generator
 
produce
 
responses
 
that
 
align
 
closely
 
with
 
the
 
retrieved
 
content.
  Joint  training  or  end-to-end  ﬁne-tuning  can  also  be  used,  where  the  retriever  and  
generator
 
learn
 
together,
 
optimizing
 
both
 
components
 
to
 
improve
 
overall
 
r

---

g
 
both
 
components
 
to
 
improve
 
overall
 
response
 
relevance
 
and
 
accuracy.
  
Q16.  What  is  the  impact  of  RAG  on  the  efﬁciency  of  language  models? Retrieval-Augmented  Generation  (RAG)  improves  language  model  efﬁciency  by  enabling  
smaller
 
models
 
to
 
handle
 
knowledge-intensive
 
tasks
 
without
 
extensive
 
ﬁne-tuning.
 
By
 
retrieving
 
relevant
 
information
 
from
 
external
 
sources,
 
RAG
 
reduces
 
the
 
need
 
for
 
large,
 
internally-stored
 
d

---

the
 
need
 
for
 
large,
 
internally-stored
 
datasets,
 
saves
 
computational
 
resources,
 
and
 
enhances
 
accuracy
 
in
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   122 
 
fact-based  tasks.  This  approach  enables  efﬁcient,  real-time  applications  without  constant  
retraining.
  
Q17.  How  does  RAG  differ  from  Parameter-Efﬁcient  Fine-Tuning  (PEFT)?  RAG  (Retrieval-Augmented  Generation)  and  Par

---

RAG  (Retrieval-Augmented  Generation)  and  Parameter-Efﬁcient  Fine-Tuning  (PEFT)  differ  
in
 
their
 
approaches
 
to
 
enhancing
 
language
 
models
 
without
 
extensive
 
retraining:
  1.  RAG:  Combines  retrieval  and  generation  by  allowing  a  model  to  pull  relevant  information  
from
 
external
 
sources
 
in
 
real-time.
 
It
 
doesn’t
 
require
 
the
 
model
 
to
 
store
 
all
 
knowledge
 
internally,
 
making
 
it
 
useful
 
for
 
dynamic
 
or
 
specialized
 
knowledge

---

ul
 
for
 
dynamic
 
or
 
specialized
 
knowledge
 
tasks
 
without
 
ﬁne-tuning.
 2.  PEFT:  Focuses  on  selectively  ﬁne-tuning  only  a  subset  of  the  model's  parameters,  
reducing
 
computational
 
cost
 
while
 
adapting
 
the
 
model
 
to
 
new
 
tasks
 
or
 
domains.
 
PEFT
 
modiﬁes
 
the
 
model's
 
internal
 
parameters,
 
unlike
 
RAG,
 
which
 
relies
 
on
 
external
 
data.
  
Q18.  In  what  ways  can  RAG  enhance  human-AI  collaboration? RAG  (Retrieval-Augmented  Generati

---

collaboration? RAG  (Retrieval-Augmented  Generation)  enhances  human-AI  collaboration  by  i mproving  
access
 
to
 
accurate,
 
relevant
 
information
 
in
 
real
 
time.
 
Unlike
 
traditional
 
models
 
that
 
rely
 
solely
 
on
 
internal
 
data,
 
RAG-equipped
 
AI
 
pulls
 
from
 
external
 
sources,
 
making
 
it
 
adaptable
 
to
 
speciﬁc,
 
evolving
 
information
 
needs
 
without
 
requiring
 
extensive
 
retraining.
 
This
 
capability
 
is
 
valuable
 
in
 
ﬁelds
 
like
 
medicin

---

lity
 
is
 
valuable
 
in
 
ﬁelds
 
like
 
medicine,
 
law,
 
and
 
customer
 
support,
 
where
 
access
 
to
 
current,
 
domain-speciﬁc
 
knowledge
 
is
 
critical.
  By  retrieving  information  as  needed,  RAG  reduces  the  likelihood  of  AI  “hallucinations,”  or  
generating
 
inaccurate
 
details,
 
which
 
strengthens
 
trust
 
and
 
reliability,
 
especially
 
in
 
decision-making
 
contexts.
 
In
 
collaborative
 
settings,
 
RAG
 
supports
 
users
 
by
 
acting
 
as
 
a
 
real-time

---

upports
 
users
 
by
 
acting
 
as
 
a
 
real-time
 
research
 
assistant,
 
streamlining
 
workﬂows
 
and
 
enhancing
 
productivity
 
with
 
tailored
 
insights.
  Additionally,  RAG’s  ability  to  provide  contextually  relevant  responses  creates  more  
engaging
 
and
 
meaningful
 
interactions,
 
as
 
the
 
AI
 
can
 
adapt
 
dynamically
 
to
 
user
 
needs.
 
Overall,
 
RAG
 
fosters
 
a
 
more
 
effective
 
and
 
accurate
 
partnership,
 
making
 
AI
 
a
 
reliable
 
collaborator
 
ac

---

,
 
making
 
AI
 
a
 
reliable
 
collaborator
 
across
 
diverse
 
applications.
  
Q19.  Can  you  explain  the  technical  architecture  of  a  RAG  system? The  architecture  of  a  Retrieval-Augmented  Generation  (RAG)  system  consists  of  three  main  
stages:
 
data
 
ingestion,
 
retriever,
 
and
 
generator.
  1.  Data  Ingestion:  In  this  stage,  the  system  gathers  and  processes  data  from  various  
sources,
 
such
 
as
 
documents,
 
databases,
 
or
 
web
 
content,
 
and

---

ments,
 
databases,
 
or
 
web
 
content,
 
and
 
stores
 
it
 
in
 
an
 
indexed
 
format.
 
This
 
involves
 
cleaning,
 
tokenizing,
 
and
 
embedding
 
the
 
data
 
into
 
a
 
structured
 
vector
 
space,
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   123 
 
making  it  searchable  and  retrievable.  The  data  ingestion  pipeline  ensures  that  information  
is
 
up-to-date
 
and
 
relevant
 
to
 
potential
 
querie

---

to-date
 
and
 
relevant
 
to
 
potential
 
queries.
 
2.  Retriever:  The  retriever  module  uses  the  indexed  data  to  identify  relevant  information  
based
 
on
 
the
 
user’s
 
query.
 
It
 
employs
 
a
 
dense
 
retrieval
 
technique,
 
typically
 
powered
 
by
 
bi-encoder
 
models,
 
to
 
match
 
the
 
query
 
with
 
stored
 
data
 
in
 
the
 
vector
 
space.
 
By
 
pulling
 
relevant
 
documents
 
or
 
text
 
snippets,
 
the
 
retriever
 
ﬁlters
 
out
 
the
 
most
 
useful
 
inform

---

ever
 
ﬁlters
 
out
 
the
 
most
 
useful
 
information
 
from
 
large
 
datasets,
 
reducing
 
the
 
load
 
on
 
the
 
generator.
  3.  Generator:  Finally,  the  generator,  often  a  transformer-based  model,  combines  the  user’s  
query
 
with
 
retrieved
 
content
 
to
 
produce
 
a
 
coherent
 
and
 
contextually
 
accurate
 
response.
 
This
 
model
 
synthesizes
 
information
 
from
 
the
 
retrieval
 
output,
 
allowing
 
it
 
to
 
deliver
 
precise
 
answers
 
or
 
narratives
 
based

---

iver
 
precise
 
answers
 
or
 
narratives
 
based
 
on
 
external
 
knowledge.
  
Q20.  How  does  RAG  maintain  context  in  a  conversation?  In  RAG  systems,  conversational  context  is  maintained  through:  ➢  Memory  Management:  Using  short-term  memory  for  recent  exchanges  and  
long-term
 
memory
 
for
 
facts
 
about
 
the
 
user.
 ➢  History  Tracking:  Keeping  dialogue  history  to  build  coherent  responses.  ➢  Dynamic  Retrieval:  Fetching  relevant  data  based  on  th

---

etrieval:  Fetching  relevant  data  based  on  the  latest  query  and  
conversation
 
history.
 ➢  Context-Aware  Generation:  Crafting  responses  that  blend  retrieved  info  with  
recent
 
context.
 ➢  State  Management:  Storing  user-speciﬁc  info  to  personalize  responses  across  
sessions.
  
Q21.  What  are  the  limitations  of  RAG? RAG  systems  have  these  main  limitations:  ➢  Reliance  on  Document  Quality:  Responses  depend  on  the  accuracy  and  relevance  of  
avai

---

pend  on  the  accuracy  and  relevance  of  
available
 
documents.
 ➢  Higher  Latency:  Retrieval  and  generation  add  computational  overhead,  slowing  
down
 
responses.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   124 
 
➢  Context  Limitations:  Struggle  to  maintain  long-term  conversation  context.  ➢  Irrelevant  Retrievals:  Sometimes  retrieve  off-topic  information,  confusing  
responses.
 ➢  Ambigui

---

information,  confusing  
responses.
 ➢  Ambiguity  in  Intent:  Can  misinterpret  unclear  queries  or  nuanced  user  intent.  ➢  Privacy  Concerns:  Storing  and  retrieving  data  may  risk  privacy  if  not  handled  
securely.
  
Q22.  How  does  RAG  handle  complex  queries  that  require  multi-hop  
reasoning? RAG  systems  handle  complex,  multi-hop  reasoning  queries  by  combining  iterative  retrieval  
with
 
sequential
 
context-aware
 
generation.
 
 
Here’s
 
how
 
they

---

ext-aware
 
generation.
 
 
Here’s
 
how
 
they
 
typically
 
approach
 
such
 
queries:
 ➔  Iterative  Retrieval:  For  complex  queries,  RAG  systems  can  break  down  the  query  
into
 
multiple
 
"hops"
 
or
 
sub-queries.
 
Each
 
hop
 
retrieves
 
documents
 
incrementally,
 
building
 
on
 
the
 
previously
 
retrieved
 
information
 
to
 
reach
 
deeper
 
insights
 
step-by-step.
 ➔  Context  Accumulation:  As  each  retrieval  step  contributes  new  information,  it’s  
passed
 
to

---

ontributes  new  information,  it’s  
passed
 
to
 
the
 
next
 
query
 
stage,
 
gradually
 
building
 
a
 
richer
 
context.
 
This
 
iterative
 
process
 
allows
 
the
 
system
 
to
 
connect
 
related
 
pieces
 
of
 
information
 
across
 
different
 
documents.
 ➔  Sequential  Generation:  After  gathering  enough  relevant  data,  RAG  systems  use  the  
accumulated
 
context
 
to
 
generate
 
a
 
coherent
 
response.
 
The
 
model
 
synthesizes
 
information
 
from
 
multiple
 
sources

---

esizes
 
information
 
from
 
multiple
 
sources
 
to
 
answer
 
multi-step
 
questions
 
more
 
accurately.
 ➔  Chained  Queries  with  Prompt  Engineering:  In  complex  RAG  setups,  prompt  
engineering
 
techniques
 
help
 
structure
 
each
 
retrieval
 
step
 
based
 
on
 
the
 
evolving
 
context,
 
guiding
 
the
 
model
 
through
 
the
 
reasoning
 
process.
  
Q23.  Can  you  discuss  the  role  of  knowledge  graphs  in  RAG? Knowledge  graphs  (KGs)  can  play  a  valuable  role  in

---

graphs  (KGs)  can  play  a  valuable  role  in  enhancing  Retrieval-Augmented  
Generation
 
(RAG)
 
by
 
providing
 
structured,
 
interconnected
 
data
 
that
 
complements
 
document-based
 
retrieval.
 
 
Here’s
 
how
 
they
 
contribute
 
to
 
RAG: ➔  Enhanced  Retrieval  Accuracy:  Knowledge  graphs  store  information  as  entities  and  
relationships
 
in
 
a
 
structured
 
format,
 
making
 
it
 
easier
 
to
 
retrieve
 
speciﬁc,
 
relevant
 
facts
 
for
 
a
 
given
 
query.
 
By

---

elevant
 
facts
 
for
 
a
 
given
 
query.
 
By
 
linking
 
concepts,
 
KGs
 
can
 
offer
 
targeted
 
retrieval
 
pathways
 
for
 
multi-hop
 
queries,
 
improving
 
RAG's
 
response
 
accuracy.
 ➔  Multi-Hop  Reasoning  Support:  KGs  naturally  support  multi-hop  reasoning  by  
representing
 
connections
 
between
 
entities.
 
When
 
a
 
query
 
requires
 
combining
 
information
 
across
 
multiple
 
data
 
points,
 
RAG
 
systems
 
can
 
leverage
 
KGs
 
to
 
retrieve
 
and
 
trace
 
the

---

leverage
 
KGs
 
to
 
retrieve
 
and
 
trace
 
these
 
connections,
 
allowing
 
for
 
more
 
precise
 
answers.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   125 
 
➔  Contextual  Query  Reﬁnement:  KGs  can  help  reﬁne  queries  by  using  entity  
relationships
 
to
 
interpret
 
user
 
intent
 
and
 
identify
 
relevant
 
subtopics.
 
For
 
example,
 
if
 
a
 
user
 
asks
 
about
 
a
 
person’s
 
achievements,
 
the

---

asks
 
about
 
a
 
person’s
 
achievements,
 
the
 
KG
 
can
 
focus
 
retrieval
 
on
 
connected
 
entities
 
like
 
“awards”
 
or
 
“projects,”
 
narrowing
 
down
 
the
 
scope
 
and
 
providing
 
more
 
accurate
 
context.
 ➔  Fact  Veriﬁcation  and  Consistency:  Knowledge  graphs  provide  a  source  of  veriﬁed,  
structured
 
facts
 
that
 
RAG
 
can
 
use
 
to
 
cross-check
 
information
 
from
 
unstructured
 
documents,
 
improving
 
reliability
 
and
 
reducing
 
hallucinations.
 ➔

---

eliability
 
and
 
reducing
 
hallucinations.
 ➔  Personalization  and  Memory:  When  a  KG  is  tailored  to  a  user  (e.g.,  representing  
their
 
interests
 
or
 
past
 
interactions),
 
it
 
can
 
serve
 
as
 
a
 
long-term
 
memory
 
for
 
a
 
RAG
 
system.
 
By
 
dynamically
 
updating
 
the
 
KG
 
with
 
user-speciﬁc
 
data,
 
the
 
RAG
 
model
 
can
 
personalize
 
responses,
 
maintaining
 
context
 
over
 
longer
 
conversations
 
or
 
sessions.
 ➔  Efﬁcient  Storage  of  High-Value

---

sessions.
 ➔  Efﬁcient  Storage  of  High-Value  Knowledge:  Instead  of  keeping  vast  amounts  of  
unstructured
 
data,
 
a
 
KG
 
stores
 
core
 
information
 
in
 
a
 
compact,
 
highly
 
relevant
 
format.
 
This
 
improves
 
retrieval
 
efﬁciency,
 
especially
 
for
 
frequently
 
asked
 
questions
 
or
 
common
 
reasoning
 
paths.
  
Q24.  What  are  the  ethical  considerations  when  implementing  RAG  
systems? 
Ethical
 
considerations
 
for
 
RAG
 
systems
 
include: ➔  Privacy

---

ations
 
for
 
RAG
 
systems
 
include: ➔  Privacy  and  Security:  Protect  user  data  and  handle  sensitive  information  securely.  ➔  Bias  and  Fairness:  Mitigate  biases  from  models  and  sources  to  ensure  fair  
responses.
 ➔  Misinformation  Risks:  Implement  fact-checking  to  prevent  hallucinations  and  
inaccuracies.
 ➔  Transparency:  Provide  explanations  for  complex  responses  to  foster  user  trust.  ➔  Content  Moderation:  Filter  inappropriate  or  harmful  conte

---

ration:  Filter  inappropriate  or  harmful  content,  especially  in  sensitive  
domains.
 ➔  IP  Compliance:  Respect  copyright  and  data  usage  rights  in  retrieved  content.  ➔  Source  Reliability:  Curate  credible  sources  to  prevent  misinformation.  ➔  User  Autonomy:  Use  disclaimers  to  prevent  over-reliance  on  RAG  for  critical  
information.
  
Q25.  How  can  multimodal  data  be  utilized  within  RAG  frameworks  to  
improve
 
information
 
retrieval
 
and
 
generat

---

improve
 
information
 
retrieval
 
and
 
generation? Integrating  various  modalities—such  as  text,  images,  and  audio—into  Retrieval-Augmented  
Generation
 
(RAG)
 
frameworks
 
signiﬁcantly
 
enhances
 
both
 
the
 
retrieval
 
process
 
and
 
the
 
quality
 
of
 
generated
 
responses.
 
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   126 
 
Here's
 
how
 
multimodal
 
data
 
contributes
 
to
 
these
 
improveme

---

odal
 
data
 
contributes
 
to
 
these
 
improvements:  ★  Enriched  Retrieval  Process:  ○  Uniﬁed  Embedding  Space:  Utilizing  multimodal  embedding  models,  like  
Meta's
 
ImageBind,
 
enables
 
the
 
encoding
 
of
 
diverse
 
data
 
types
 
into
 
a
 
single
 
vector
 
space.
 
This
 
facilitates
 
efﬁcient
 
cross-modal
 
searches,
 
allowing,
 
for
 
example,
 
a
 
text
 
query
 
to
 
retrieve
 
relevant
 
images
 
or
 
audio
 
clips.
 ○  Cross-Modal  Retrieval:  By  embedding  differe

---

○  Cross-Modal  Retrieval:  By  embedding  different  modalities  together,  RAG  
systems
 
can
 
handle
 
queries
 
that
 
span
 
multiple
 
data
 
types,
 
enhancing
 
the
 
relevance
 
and
 
comprehensiveness
 
of
 
retrieved
 
information.
 
 ★  Enhanced  Context  for  Generation:  ○  Multimodal  Conditioning:  Incorporating  retrieved  images,  audio,  or  videos  as  
conditional
 
inputs
 
allows
 
the
 
generation
 
model
 
to
 
produce
 
responses
 
that
 
are
 
contextually
 
rich
 
a

---

responses
 
that
 
are
 
contextually
 
rich
 
and
 
tailored
 
to
 
the
 
speciﬁc
 
query.
 
For
 
instance,
 
generating
 
a
 
detailed
 
description
 
based
 
on
 
a
 
retrieved
 
image
 
provides
 
a
 
more
 
nuanced
 
response.
 ○  Comprehensive  Outputs:  By  leveraging  information  from  multiple  modalities,  
RAG
 
systems
 
can
 
generate
 
outputs
 
that
 
are
 
not
 
only
 
textually
 
informative
 
but
 
also
 
visually
 
or
 
auditorily
 
enriched,
 
leading
 
to
 
a
 
more
 
en

---

itorily
 
enriched,
 
leading
 
to
 
a
 
more
 
engaging
 
user
 
experience.
  ★  Practical  Applications:  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   127 
 
○  E-Commerce:  Enhancing  product  search  by  retrieving  and  generating  
descriptions
 
that
 
integrate
 
text,
 
images,
 
and
 
reviews,
 
thereby
 
providing
 
a
 
holistic
 
view
 
of
 
the
 
product.
 ○  Healthcare:  Combining  patient  notes  with  me

---

Healthcare:  Combining  patient  notes  with  medical  images  (e.g.,  X-rays)  to  
retrieve
 
relevant
 
cases
 
or
 
generate
 
diagnostic
 
insights,
 
supporting
 
more
 
informed
 
decision-making.
 ○  Education:  Retrieving  and  generating  answers  or  summaries  that  include  
text,
 
diagrams,
 
and
 
videos,
 
offering
 
a
 
richer
 
learning
 
experience.
 
  
Q26.  What  are  the  challenges  of  implementing  multimodal  RAG,  
particularly
 
regarding
 
data
 
integration
 
an

---

particularly
 
regarding
 
data
 
integration
 
and
 
model
 
training? Implementing  multimodal  Retrieval-Augmented  Generation  (RAG)  systems  presents  several  
challenges,
 
particularly
 
in
 
data
 
integration
 
and
 
model
 
training:
  ➢  Data  Integration  Challenges:  ○  Heterogeneous  Data  Formats:  Combining  diverse  data  types—such  as  text,  
images,
 
and
 
audio—requires
 
standardizing
 
formats
 
and
 
representations
 
to
 
ensure
 
seamless
 
integration.
 ○  Alignmen

---

to
 
ensure
 
seamless
 
integration.
 ○  Alignment  Across  Modalities:  Establishing  meaningful  correspondences  
between
 
different
 
modalities
 
is
 
complex.
 
For
 
instance,
 
linking
 
textual
 
descriptions
 
accurately
 
with
 
relevant
 
images
 
or
 
audio
 
clips
 
demands
 
sophisticated
 
alignment
 
techniques.
 ○  Data  Quality  and  Consistency:  Ensuring  high-quality,  consistent  data  across  
modalities
 
is
 
crucial.
 
Variations
 
in
 
data
 
quality
 
can
 
adverse

---

Variations
 
in
 
data
 
quality
 
can
 
adversely
 
affect
 
the
 
performance
 
of
 
multimodal
 
RAG
 
systems.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   128 
 
➢  Model  Training  Challenges:  ○  Uniﬁed  Embedding  Space:  Training  models  to  encode  various  modalities  into  
a
 
shared
 
semantic
 
space
 
is
 
challenging.
 
It
 
requires
 
models
 
capable
 
of
 
understanding
 
and
 
representing
 
the

---

e
 
of
 
understanding
 
and
 
representing
 
the
 
nuances
 
of
 
each
 
modality
 
effectively.
 ○  Computational  Complexity:  Processing  and  integrating  multimodal  data  
signiﬁcantly
 
increases
 
computational
 
demands,
 
necessitating
 
advanced
 
hardware
 
and
 
optimized
 
algorithms
 
to
 
manage
 
the
 
complexity.
 ○  Limited  Multimodal  Datasets:  The  scarcity  of  large-scale,  annotated  
multimodal
 
datasets
 
hampers
 
effective
 
training,
 
making
 
it
 
difﬁcult
 
to

---

ffective
 
training,
 
making
 
it
 
difﬁcult
 
to
 
achieve
 
robust
 
performance
 
across
 
all
 
modalities.
 ○  Modality-Speciﬁc  Noise:  Each  modality  may  introduce  unique  noise  or  
irrelevant
 
information,
 
complicating
 
the
 
training
 
process
 
and
 
potentially
 
degrading
 
model
 
performance.
  
Q27.  Can  you  describe  a  speciﬁc  application  of  multimodal  RAG  in  a  
real-world
 
scenario?
 
What
 
are
 
its
 
beneﬁts
 
over
 
unimodal
 
approaches? A  multimodal

---

ts
 
over
 
unimodal
 
approaches? A  multimodal  Retrieval-Augmented  Generation  (RAG)  system  combines  information  from  
multiple
 
sources
 
(e.g.,
 
text,
 
images,
 
audio,
 
or
 
video)
 
to
 
improve
 
responses
 
in
 
applications
 
where
 
relying
 
on
 
a
 
single
 
data
 
type
 
may
 
be
 
limiting.
 
  
Here’s
 
a
 
real-world
 
scenario: Shopping  Assistant  with  Image  and  Text  Search  ❖  User  Uploads  a  Photo:  Suppose  a  user  is  looking  for  a  speciﬁc  type  of  ch

---

user  is  looking  for  a  speciﬁc  type  of  chair.  
They
 
upload
 
a
 
photo
 
of
 
a
 
chair
 
they
 
like
 
or
 
describe
 
it
 
using
 
text,
 
like
 
"modern
 
style
 
wooden
 
dining
 
chair."
 ❖  Multimodal  Retrieval:  The  RAG  system  uses  the  photo  (visual  data)  and  the  
description
 
(text
 
data)
 
to
 
search
 
a
 
database
 
of
 
furniture
 
items.
 
It
 
matches
 
similar-looking
 
chairs
 
and
 
those
 
that
 
align
 
with
 
the
 
descriptive
 
keywords.
 ❖  Recommen

---

with
 
the
 
descriptive
 
keywords.
 ❖  Recommendations:  The  system  then  generates  a  list  of  suggested  items  that  
closely
 
resemble
 
the
 
uploaded
 
photo
 
and
 
meet
 
the
 
description.
 
It
 
could
 
also
 
provide
 
suggestions
 
for
 
items
 
that
 
complement
 
the
 
chair,
 
like
 
a
 
matching
 
table.
  Beneﬁts  Over  Unimodal  Approaches  ➔  Better  Matching:  Using  both  text  and  image  data  ensures  a  more  accurate  
match,
 
especially
 
if
 
the
 
photo
 
al

---

rate  
match,
 
especially
 
if
 
the
 
photo
 
alone
 
doesn’t
 
capture
 
the
 
style,
 
or
 
if
 
the
 
text
 
alone
 
doesn’t
 
fully
 
describe
 
the
 
design.
 ➔  Enhanced  Search  Experience:  The  multimodal  approach  is  convenient  for  
users
 
who
 
may
 
have
 
a
 
picture
 
of
 
what
 
they
 
want
 
but
 
struggle
 
to
 
describe
 
it
 
in
 
words
 
or
 
vice
 
versa.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Ques

---

Generative  AI  Interview  Questions   129 
 
➔  Personalized  Results:  The  system  can  recommend  more  tailored  results  by  
combining
 
what
 
the
 
user
 
shows
 
with
 
what
 
they
 
say,
 
improving
 
the
 
relevance
 
of
 
the
 
suggestions.
 
 
  
Q28.  What  evaluation  metrics  would  be  suitable  for  assessing  the  
performance
 
of
 
multimodal
 
RAG
 
systems?
 
How
 
do
 
they
 
differ
 
from
 
those
 
used
 
in
 
traditional
 
RAG
 
models?
 In  multimod

---

d
 
in
 
traditional
 
RAG
 
models?
 In  multimodal  RAG  systems,  evaluation  metrics  are  expanded  to  address  the  added  
complexity
 
of
 
handling
 
both
 
text
 
and
 
image
 
data.
 
  
Here’s
 
a
 
closer
 
look
 
at
 
relevant
 
metrics:
 1.  Relevancy:  Measures  whether  retrieved  content  (text  or  image)  aligns  with  the  
query.
 
Unlike
 
traditional
 
text-only
 
RAG
 
models,
 
multimodal
 
relevancy
 
must
 
evaluate
 
both
 
the
 
individual
 
relevance
 
of
 
text

---

both
 
the
 
individual
 
relevance
 
of
 
text
 
and
 
images
 
and
 
their
 
joint
 
relevance.
 2.  Faithfulness:  Ensures  generated  content  accurately  represents  the  retrieved  
sources.
 
Multimodal
 
systems
 
need
 
to
 
verify
 
that
 
generated
 
text
 
and
 
images
 
don't
 
contradict
 
each
 
other,
 
requiring
 
checks
 
across
 
modalities
 
to
 
maintain
 
factual
 
integrity.
 3.  Consistency  Across  Modalities:  Measures  coherence  between  text  and  images,  
ensuring

---

coherence  between  text  and  images,  
ensuring
 
they
 
work
 
together
 
meaningfully.
 
This
 
is
 
unique
 
to
 
multimodal
 
RAGs,
 
as
 
text-only
 
models
 
don't
 
need
 
to
 
account
 
for
 
visual-textual
 
alignment.
 4.  Multi-Modal  Evaluation  Frameworks:  Methods  like  “LMM-As-A-Judge”  leverage  
large
 
multimodal
 
models
 
to
 
judge
 
outputs
 
by
 
scoring
 
the
 
alignment
 
between
 
text
 
and
 
images.
 
This
 
model-generated
 
scoring
 
is
 
unique
 
to
 
multimoda

---

generated
 
scoring
 
is
 
unique
 
to
 
multimodal
 
setups
 
and
 
adds
 
an
 
AI-driven
 
judgment
 
layer
 
to
 
traditional
 
human
 
evaluations.
   
Q29.  How  would  you  design  a  multimodal  RAG  system  for  a  speciﬁc  
industry,
 
such
 
as
 
healthcare
 
or
 
education?
 
What
 
key
 
components
 
would
 
you
 
include,
 
and
 
how
 
would
 
they
 
interact?
 Designing  a  multimodal  RAG  system  for  industries  like  healthcare  or  education  would  
involve
 
specialized
 
co

---

or  education  would  
involve
 
specialized
 
components
 
to
 
address
 
domain-speciﬁc
 
needs.
 
  
Here’s
 
how
 
a
 
system
 
might
 
be
 
structured: 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   130 
 
1.  Data  Ingestion  and  Preprocessing  ➢  Domain-Speciﬁc  Databases:  Incorporate  electronic  health  records,  medical  
images,
 
or
 
educational
 
texts
 
and
 
videos.
 ➢  Preprocessing  Pipeline:  Clean,

---

d
 
videos.
 ➢  Preprocessing  Pipeline:  Clean,  standardize,  and  format  data  for  both  text  
(e.g.,
 
clinical
 
notes
 
or
 
textbooks)
 
and
 
images
 
(e.g.,
 
X-rays
 
or
 
diagrams)
 
for
 
effective
 
retrieval
 
and
 
generation.
 2.  Multimodal  Retrieval  Mechanism  ➢  Text  and  Image  Encoders:  Use  specialized  encoders  for  each  modality,  like  
BERT
 
for
 
text
 
and
 
CNNs
 
(e.g.,
 
ResNet)
 
for
 
images,
 
to
 
create
 
representations
 
of
 
data.
 ➢  Cross-Modal

---

te
 
representations
 
of
 
data.
 ➢  Cross-Modal  Retrieval  Models:  Implement  retrieval  mechanisms  that  align  
representations
 
from
 
different
 
modalities,
 
allowing
 
retrieval
 
of
 
the
 
most
 
relevant
 
text
 
and
 
images
 
based
 
on
 
the
 
query.
  3.  Generation  Module  ➢  Language  Generation  Models:  Use  RAG  models  tailored  for  the  speciﬁc  
industry,
 
like
 
GPT-4
 
with
 
domain-speciﬁc
 
ﬁne-tuning,
 
to
 
generate
 
responses
 
that
 
synthesize
 
informati

---

nerate
 
responses
 
that
 
synthesize
 
information
 
across
 
text
 
and
 
images.
 ➢  Image  Generation  or  Annotation:  For  healthcare,  the  system  might  suggest  
annotated
 
areas
 
in
 
medical
 
images,
 
while
 
in
 
education,
 
it
 
could
 
generate
 
relevant
 
diagrams.
  4.  Evaluation  and  Feedback  Loop  ➢  Relevancy  and  Faithfulness  Metrics:  Incorporate  metrics  such  as  relevancy,  
factual
 
faithfulness,
 
and
 
cross-modal
 
coherence
 
to
 
assess
 
and
 
improv

---

oss-modal
 
coherence
 
to
 
assess
 
and
 
improve
 
the
 
output
 
quality.
 ➢  Human-in-the-Loop  Feedback:  Especially  in  regulated  ﬁelds  like  healthcare,  
human
 
review
 
can
 
be
 
a
 
ﬁnal
 
checkpoint
 
to
 
ensure
 
reliability.
  
Example
 
Interaction
 
in
 
Healthcare A  doctor  queries  a  system  for  information  on  a  particular  disease  with  symptoms.  The  
system
 
retrieves
 
relevant
 
case
 
studies
 
(text)
 
and
 
example
 
X-rays
 
(images).
 
It
 
generates

---

example
 
X-rays
 
(images).
 
It
 
generates
 
a
 
response
 
synthesizing
 
this
 
information,
 
potentially
 
highlighting
 
relevant
 
parts
 
of
 
the
 
X-ray
 
with
 
generated
 
explanatory
 
text
 
to
 
guide
 
interpretation.
  
Q30.  What  techniques  can  be  used  to  ensure  effective  alignment  and  
integration
 
of
 
different
 
modalities
 
in
 
a
 
RAG
 
pipeline? To  ensure  effective  alignment  and  integration  of  different  modalities  in  a  
Retrieval-Augmented
 
Ge

---

rent  modalities  in  a  
Retrieval-Augmented
 
Generation
 
(RAG)
 
pipeline,
 
several
 
techniques
 
can
 
be
 
applied
 
across
 
preprocessing,
 
retrieval,
 
and
 
generation
 
stages.
 
  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   131 
 
Here’s
 
a
 
breakdown
 
of
 
key
 
techniques:
  1.  Uniﬁed  Representations  for  Multi-Modal  Input  ➢  Cross-Modal  Embeddings:  Use  joint  embedding  spaces  where  each

---

ngs:  Use  joint  embedding  spaces  where  each  modality  
can
 
be
 
mapped
 
to
 
a
 
shared
 
latent
 
space.
 
For
 
example,
 
text
 
and
 
images
 
can
 
be
 
embedded
 
into
 
the
 
same
 
vector
 
space
 
using
 
models
 
like
 
CLIP
 
or
 
ALIGN.
 
This
 
allows
 
the
 
system
 
to
 
compare
 
and
 
retrieve
 
similar
 
information
 
across
 
modalities.
 ➢  Modality-Agnostic  Representations:  Certain  architectures,  such  as  Perceiver  
IO,
 
encode
 
various
 
modalities
 
into

---

ver  
IO,
 
encode
 
various
 
modalities
 
into
 
a
 
common
 
latent
 
space
 
where
 
inputs
 
and
 
outputs
 
are
 
more
 
interchangeable.
 
This
 
enables
 
easier
 
cross-modal
 
understanding
 
and
 
integration.
 ➢  Embedding  Projection:  When  dealing  with  embeddings  from  multiple  models  
(e.g.,
 
text
 
and
 
image
 
embeddings
 
from
 
different
 
models),
 
embedding
 
projection
 
layers
 
can
 
help
 
align
 
the
 
output
 
dimensions
 
and
 
distribution
 
of
 
each
 
moda

---

dimensions
 
and
 
distribution
 
of
 
each
 
modality
 
for
 
smoother
 
downstream
 
tasks.
  2.  Hierarchical  and  Cross-Attention  Mechanisms  ➢  Cross-Attention  Layers:  These  can  be  integrated  into  the  model  to  attend  to  
speciﬁc
 
features
 
from
 
each
 
modality,
 
allowing
 
dynamic
 
focusing
 
on
 
pertinent
 
information
 
from
 
text,
 
images,
 
etc.
 
during
 
retrieval
 
and
 
generation.
 ➢  Hierarchical  Attention  Networks:  For  modalities  with  nested  structur

---

Networks:  For  modalities  with  nested  structure  (e.g.,  
text
 
or
 
video
 
frames),
 
hierarchical
 
attention
 
mechanisms
 
let
 
the
 
model
 
focus
 
on
 
higher-level
 
structures
 
while
 
retaining
 
modality-speciﬁc
 
granularity,
 
enhancing
 
contextual
 
relevance.
  3.  Prompt  and  Context  Engineering  ➢  Context  Fusion:  Concatenate  or  blend  features  from  different  modalities  into  
a
 
uniﬁed
 
prompt
 
or
 
context
 
block.
 
For
 
instance,
 
extracted
 
image

---

block.
 
For
 
instance,
 
extracted
 
image
 
descriptions
 
(via
 
object
 
detection
 
or
 
captioning
 
models)
 
can
 
be
 
injected
 
into
 
text
 
prompts,
 
helping
 
the
 
language
 
model
 
generate
 
more
 
contextually
 
informed
 
responses.
 ➢  Meta-Prompting:  Provide  modality-speciﬁc  prompts  that  direct  the  model  
on
 
how
 
to
 
interpret
 
and
 
use
 
each
 
modality.
 
For
 
instance,
 
text
 
prompts
 
can
 
specify
 
how
 
to
 
integrate
 
text
 
snippets,
 
images

---

how
 
to
 
integrate
 
text
 
snippets,
 
images,
 
or
 
tables
 
in
 
a
 
coherent
 
manner.
  4.  Modality-Speciﬁc  Retrieval  Models  ➢  Hybrid  Retrieval  Systems:  Use  modality-aware  retrieval  modules  that  can  
handle
 
different
 
types
 
of
 
inputs.
 
For
 
instance,
 
a
 
combination
 
of
 
dense
 
and
 
sparse
 
retrievers
 
can
 
work
 
with
 
both
 
text
 
and
 
image
 
embeddings.
 ➢  Adaptive  Retrieval  Reranking:  Rerank  retrieval  results  by  using  cross-modal  
relev

---

retrieval  results  by  using  cross-modal  
relevance
 
scoring,
 
which
 
takes
 
into
 
account
 
how
 
the
 
modalities
 
interact.
 
For
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   132 
 
instance,  an  image  may  be  reranked  higher  if  it  contains  visual  cues  that  
complement
 
or
 
clarify
 
text
 
content.
  5.  Consistency  and  Alignment  Losses  ➢  Contrastive  Learning  for  Modal  Alignment:  Trai

---

ontrastive  Learning  for  Modal  Alignment:  Train  with  a  contrastive  loss  
(e.g.,
 
InfoNCE)
 
to
 
ensure
 
that
 
similar
 
concepts
 
across
 
modalities
 
(e.g.,
 
a
 
caption
 
and
 
an
 
image)
 
are
 
closer
 
in
 
embedding
 
space,
 
while
 
dissimilar
 
ones
 
are
 
pushed
 
apart.
 
This
 
improves
 
the
 
retriever’s
 
ability
 
to
 
match
 
cross-modal
 
content.
 ➢  Consistency  Losses:  During  model  ﬁne-tuning,  impose  consistency  losses  
(like
 
KL
 
divergence)
 
bet

---

nsistency  losses  
(like
 
KL
 
divergence)
 
between
 
modalities
 
to
 
ensure
 
coherent
 
outputs.
 
For
 
instance,
 
when
 
using
 
text
 
and
 
image
 
inputs,
 
enforce
 
consistency
 
across
 
both
 
to
 
align
 
responses.
  6.  Memory  Augmentation  with  Modality-Speciﬁc  Context  ➢  Modality-Aware  Memory  Modules:  Build  memory  components  that  are  
aware
 
of
 
the
 
input
 
modality,
 
allowing
 
the
 
model
 
to
 
retrieve
 
relevant
 
past
 
information
 
in
 
a
 
modality

---

relevant
 
past
 
information
 
in
 
a
 
modality-appropriate
 
way.
 
For
 
instance,
 
in
 
conversation-based
 
RAG
 
pipelines,
 
memory
 
could
 
store
 
both
 
text
 
and
 
image
 
contexts
 
to
 
be
 
selectively
 
referenced.
 ➢  Cross-Modal  Memory:  Use  cross-modal  memory  mechanisms  that  allow  
recall
 
from
 
past
 
interactions
 
regardless
 
of
 
modality,
 
integrating
 
relevant
 
historical
 
context
 
into
 
the
 
prompt
 
when
 
needed.
  7.  Data  Augmentation  and  Pre

---

when
 
needed.
  7.  Data  Augmentation  and  Pretraining  on  Multimodal  Datasets  ➢  Multimodal  Pretraining:  Leverage  multimodal  datasets  (e.g.,  datasets  with  
aligned
 
text-image
 
pairs)
 
to
 
pretrain
 
your
 
model.
 
This
 
ensures
 
the
 
model
 
learns
 
the
 
relationship
 
between
 
different
 
modalities
 
early
 
on.
 ➢  Data  Augmentation  Techniques:  For  example,  use  synthetic  captioning  or  
audio
 
transcriptions
 
to
 
bridge
 
gaps
 
in
 
datasets
 
and
 
enh

---

s
 
to
 
bridge
 
gaps
 
in
 
datasets
 
and
 
enhance
 
modality
 
alignment
 
during
 
model
 
training.
  8.  Human-in-the-Loop  Evaluation  and  Iterative  Feedback  ➢  Feedback  Loops:  Periodically  incorporate  human  feedback  on  retrieval  and  
generation
 
accuracy
 
across
 
modalities
 
to
 
ﬁne-tune
 
the
 
alignment
 
strategies.
 
This
 
iterative
 
evaluation
 
is
 
particularly
 
useful
 
for
 
identifying
 
weak
 
points
 
in
 
cross-modal
 
understanding.
  
Q31.  In  a  mul

---

cross-modal
 
understanding.
  
Q31.  In  a  multimodal  RAG  setup,  how  would  you  evaluate  the  quality  and  
relevance
 
of
 
generated
 
content?
 
What
 
metrics
 
or
 
benchmarks
 
would
 
you
 
consider? 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   133 
 
To  evaluate  quality  and  relevance  in  a  multimodal  RAG  pipeline,  consider  these  key  metrics  
and
 
benchmarks:
 ❖  Relevance  &  Accuracy:  ➢

---

and
 
benchmarks:
 ❖  Relevance  &  Accuracy:  ➢  Precision@K,  Recall@K,  and  Mean  Reciprocal  Rank  (MRR)  assess  the  
relevance
 
of
 
retrieved
 
items.
 ➢  Multimodal  Relevance  Score  measures  similarity  between  retrieved  content  
across
 
modalities.
 ❖  Content  Quality:  ➢  BLEU,  ROUGE,  and  METEOR  measure  textual  relevance.  ➢  CIDEr  and  SPICE  evaluate  image-text  semantic  relevance.  ➢  BERTScore  assesses  deeper  semantic  alignment  between  generated  text  a

---

semantic  alignment  between  generated  text  and  
references.
 ❖  Alignment:  ➢  CLIP  Score  for  text-image  relevance,  and  VLP  Score  for  broader  modality  
alignment.
 ❖  Consistency  &  Coherence:  ➢  Self-BLEU  for  response  diversity,  Entity  Consistency  to  track  
object/attribute
 
alignment,
 
and
 
human
 
evaluations
 
for
 
coherence.
 ❖  Task-Speciﬁc  Metrics:  ➢  Exact  Match  (for  Q&A)  and  F1  Score  (for  classiﬁcation  tasks).  ❖  Human  Evaluation:  ➢  Relevan

---

cation  tasks).  ❖  Human  Evaluation:  ➢  Relevance,  coherence,  and  task  success  rates  judged  by  evaluators.   
Q32.  What  challenges  do  you  anticipate  when  scaling  a  multimodal  RAG  
system
 
to
 
handle
 
large
 
datasets,
 
and
 
how
 
would
 
you
 
address
 
them? 
Here
 
are
 
the
 
main
 
challenges
 
in
 
scaling
 
a
 
multimodal
 
RAG
 
system
 
and
 
their
 
solutions:
  ★  Computational  &  Storage  Demands:  High-dimensional  data  requires  efﬁcient  
storage.
 ○  S

---

nsional  data  requires  efﬁcient  
storage.
 ○  Solution:  Use  FAISS  or  vector  databases;  compress  embeddings  with  
quantization.
 ★  Efﬁcient  Retrieval:  Retrieving  across  modalities  can  slow  down  performance.  ○  Solution:  Use  hybrid  retrieval  with  sharding  and  hierarchical  search  for  
speed.
 ★  Cross-Modal  Alignment:  Maintaining  alignment  across  large,  diverse  data.  ○  Solution:  Use  joint  embeddings  (e.g.,  CLIP);  ﬁne-tune  with  contrastive  
learning.

---

,  CLIP);  ﬁne-tune  with  contrastive  
learning.
 ★  Latency  in  Generation:  Complex  multimodal  inputs  increase  response  time.  ○  Solution:  Cache  responses;  use  lightweight  models  for  simpler  queries.  ★  Data  Quality:  Inconsistent  data  can  reduce  relevance.  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   134 
 
○  Solution:  Curate  data  with  quality  checks;  retrain  with  recent,  high-value

---

ity  checks;  retrain  with  recent,  high-value  
data.
 ★  Memory  Management:  Keeping  relevant,  modality-speciﬁc  memory  can  be  
demanding.
 ○  Solution:  Use  modality-speciﬁc  memory  modules;  prune  less-used  data.  ★  System  Complexity:  Varied  pipelines  increase  maintenance  needs.  ○  Solution:  Use  modular  pipelines  and  containerization;  monitor  for  issues.   
Q33.  Can  you  provide  an  example  of  a  potential  ethical  concern  
associated
 
with
 
multimodal

---

hical  concern  
associated
 
with
 
multimodal
 
RAG
 
systems?
 
How
 
would
 
you
 
mitigate
 
this
 
issue? Ethical  Concern:   ●  Multimodal  RAG  systems  can  generate  biased  or  harmful  content  due  to  
biased
 
data
 
across
 
text,
 
images,
 
etc.
  Mitigation :  ➔  Bias  Detection:  Regularly  audit  data  for  biases;  use  ﬁlters  to  ﬂag  problematic  
content.
 ➔  Diverse  Data:  Curate  balanced  datasets  to  counter  stereotypes.  ➔  Human  Review:  Use  human-in-the-loop

---

otypes.  ➔  Human  Review:  Use  human-in-the-loop  monitoring  and  user  reporting  to  
catch
 
issues
 
early.
 ➔  Transparency:  Inform  users  of  data  limitations  and  potential  bias  risks.                     

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   135 
 
   
                   
Fine  Tuning 
                

MD  ARBAAZ  KHAN                                                                Generative  AI

---

Generative  AI  Interview  Questions   136 
 
  
Q1.  What  is  Fine-tuning?  Describe  the  Fine-tuning  process.  Fine-tuning  is  a  process  in  machine  learning  where  a  pre-trained  model  is  further  trained  
on
 
a
 
new
 
dataset
 
to
 
adapt
 
it
 
to
 
a
 
speciﬁc
 
task
 
or
 
improve
 
its
 
performance
 
in
 
a
 
certain
 
domain.
 
This
 
is
 
especially
 
common
 
in
 
natural
 
language
 
processing
 
(NLP)
 
and
 
computer
 
vision,
 
whe

---

rocessing
 
(NLP)
 
and
 
computer
 
vision,
 
where
 
large
 
models
 
are
 
ﬁrst
 
trained
 
on
 
massive
 
datasets
 
to
 
learn
 
general
 
patterns,
 
then
 
ﬁne-tuned
 
on
 
smaller,
 
more
 
specialized
 
datasets.
  
Fine-Tuning
 
Process ➢  Select  a  Pre-trained  Model:  The  process  begins  with  a  model  that  has  already  been  
trained
 
on
 
a
 
large,
 
generic
 
dataset.
 
These
 
models
 
are
 
generally
 
powerful
 
because
 
they
 
have
 
learned
 
a
 
wide
 
range
 
of

---

they
 
have
 
learned
 
a
 
wide
 
range
 
of
 
features
 
and
 
patterns.
 
For
 
instance,
 
in
 
NLP,
 
models
 
like
 
GPT,
 
BERT,
 
and
 
T5
 
are
 
pre-trained
 
on
 
vast
 
text
 
corpora.
 ➢  Prepare  the  Dataset:  Collect  and  preprocess  a  dataset  that  is  speciﬁc  to  the  task  or  
domain
 
you
 
want
 
the
 
model
 
to
 
specialize
 
in.
 
This
 
might
 
involve
 
labeling
 
data,
 
tokenizing
 
text
 
(for
 
NLP
 
tasks),
 
resizing
 
images,
 
or
 
standardizing
 
data

---

resizing
 
images,
 
or
 
standardizing
 
data
 
formats.
 ➢  Set  Up  the  Model  for  Fine-Tuning:  This  includes  conﬁguring  the  model’s  
architecture
 
(like
 
freezing
 
or
 
unfreezing
 
speciﬁc
 
layers).
 
Freezing
 
early
 
layers
 
allows
 
the
 
model
 
to
 
retain
 
its
 
general
 
knowledge
 
while
 
updating
 
higher
 
layers
 
to
 
focus
 
on
 
task-speciﬁc
 
information.
 ➢  Train  the  Model  on  the  New  Dataset:  The  model  is  trained  on  the  new,  smaller  
dataset

---

el  is  trained  on  the  new,  smaller  
dataset
 
for
 
a
 
few
 
epochs,
 
with
 
adjustments
 
made
 
to
 
the
 
learning
 
rate
 
and
 
other
 
hyperparameters
 
to
 
avoid
 
overﬁtting.
 
Fine-tuning
 
can
 
be
 
supervised
 
(using
 
labeled
 
data)
 
or
 
unsupervised
 
(using
 
unlabelled
 
data)
 
depending
 
on
 
the
 
task.
 ➢  Evaluate  and  Adjust:  After  ﬁne-tuning,  the  model  is  evaluated  to  ensure  it  performs  
well
 
on
 
the
 
specialized
 
task.
 
Adjustments
 
to
 
h

---

the
 
specialized
 
task.
 
Adjustments
 
to
 
hyperparameters
 
or
 
additional
 
ﬁne-tuning
 
may
 
be
 
done
 
to
 
improve
 
accuracy.
 ➢  Deploy  the  Fine-tuned  Model:  Once  the  model  performs  satisfactorily,  it’s  ready  for  
deployment
 
in
 
the
 
speciﬁc
 
application
 
or
 
task.
  
Q2.  What  are  the  different  Fine-tuning  methods? Fine-tuning  methods  vary  based  on  the  level  of  customization  and  computational  resources  
available.
 
  
Here
 
are
 
several
 
p

---

sources  
available.
 
  
Here
 
are
 
several
 
popular
 
ﬁne-tuning
 
methods: 1.  Full  Fine-Tuning  ➔  Description:  This  method  involves  updating  all  the  parameters  of  the  pre-trained  
model
 
during
 
ﬁne-tuning
 
on
 
the
 
new
 
dataset.
 ➔  Use  Case:  Suitable  for  cases  where  a  signiﬁcant  change  in  behavior  is  required,  such  
as
 
adapting
 
the
 
model
 
to
 
a
 
completely
 
different
 
domain.
 

MD  ARBAAZ  KHAN

---

N                                                                Generative  AI  Interview  Questions   137 
 
➔  Pros:  Allows  maximum  adaptation  to  the  new  task.  ➔  Cons:  Computationally  expensive  and  risks  overﬁtting,  especially  with  small  
datasets.
  2.  Partial  Fine-Tuning  (Layer  Freezing)  ➔  Description:  In  this  method,  only  the  later  (higher)  layers  of  the  model  are  
ﬁne-tuned,
 
while
 
the
 
earlier
 
(lower)
 
layers
 
are
 
"frozen"
 
and
 
left
 
unm

---

ower)
 
layers
 
are
 
"frozen"
 
and
 
left
 
unmodiﬁed.
 ➔  Use  Case:  Commonly  used  in  transfer  learning  where  the  lower  layers  capture  
general
 
features,
 
and
 
only
 
task-speciﬁc
 
patterns
 
need
 
reﬁnement
 
in
 
the
 
upper
 
layers.
 ➔  Pros:  Reduces  computational  cost  and  mitigates  overﬁtting  by  preserving  the  
general
 
features
 
learned
 
during
 
pre-training.
 ➔  Cons:  Less  adaptable  if  the  new  dataset  is  very  different  from  the  original  
pre

---

et  is  very  different  from  the  original  
pre-training
 
dataset.
  3.  Feature  Extraction  ➔  Description:  The  pre-trained  model  is  used  as  a  feature  extractor,  where  no  layers  
are
 
updated.
 
The
 
model
 
outputs
 
features
 
that
 
are
 
then
 
fed
 
into
 
a
 
separate
 
model
 
(like
 
a
 
classiﬁer)
 
trained
 
on
 
the
 
new
 
data.
 ➔  Use  Case:  Works  well  when  only  a  small  dataset  is  available,  and  the  goal  is  a  speciﬁc  
downstream
 
task
 
such

---

goal  is  a  speciﬁc  
downstream
 
task
 
such
 
as
 
classiﬁcation
 
or
 
clustering.
 ➔  Pros:  Extremely  efﬁcient;  no  retraining  of  the  model  layers  is  required.  ➔  Cons:  Limited  adaptability  to  the  new  task  since  the  model  parameters  aren’t  
updated.
  4.  Fine-Tuning  with  Regularization  ➔  Description:  In  this  method,  regularization  techniques  (such  as  dropout  or  weight  
decay)
 
are
 
applied
 
during
 
ﬁne-tuning
 
to
 
prevent
 
overﬁtting
 
to
 
the

---

ﬁne-tuning
 
to
 
prevent
 
overﬁtting
 
to
 
the
 
new
 
dataset.
 ➔  Use  Case:  Useful  when  ﬁne-tuning  with  a  small,  specialized  dataset  prone  to  
overﬁtting.
 ➔  Pros:  Balances  adaptation  and  generalization,  making  the  model  perform  well  on  
unseen
 
data.
 ➔  Cons:  Requires  careful  tuning  of  regularization  hyperparameters.   5.  Domain-Adaptive  Fine-Tuning  ➔  Description:  The  model  is  ﬁne-tuned  on  data  that  closely  matches  the  target  domain  
before

---

t  closely  matches  the  target  domain  
before
 
ﬁne-tuning
 
on
 
the
 
speciﬁc
 
task
 
data.
 ➔  Use  Case:  Used  in  scenarios  where  the  new  dataset  is  small  but  similar  domain  data  
is
 
available,
 
which
 
can
 
help
 
improve
 
performance
 
without
 
requiring
 
large
 
task-speciﬁc
 
data.
 ➔  Pros:  Boosts  performance  by  allowing  the  model  to  adjust  to  domain-speciﬁc  
features
 
ﬁrst.
 

MD  ARBAAZ  KHAN

---

Generative  AI  Interview  Questions   138 
 
➔  Cons:  Adds  an  intermediate  ﬁne-tuning  step  and  requires  additional  
domain-speciﬁc
 
data.
  6.  Parameter-Efﬁcient  Fine-Tuning  ➔  Description:  Only  a  small  subset  of  parameters  is  ﬁne-tuned  while  the  rest  are  kept  
frozen.
 
Techniques
 
like
 
Adapter
 
layers,
 
LoRA
 
(Low-Rank
 
Adaptation),
 
or
 
BitFit
 
are
 
commonly
 
used
 
here.
 ➔  Use  Case:  Useful  w

---

commonly
 
used
 
here.
 ➔  Use  Case:  Useful  when  working  with  large  models  where  full  ﬁne-tuning  is  
impractical
 
due
 
to
 
memory
 
or
 
computational
 
constraints.
 ➔  Pros:  Highly  efﬁcient  and  retains  most  of  the  model’s  general  knowledge  while  
learning
 
speciﬁc
 
patterns.
 ➔  Cons:  May  be  less  effective  on  very  task-speciﬁc  or  complex  adaptations.   7.  Prompt  Tuning  ➔  Description:  Fine-tuning  involves  learning  only  the  prompt  embeddings  o

---

volves  learning  only  the  prompt  embeddings  or  small  
prompt
 
parameters
 
rather
 
than
 
modifying
 
the
 
main
 
model’s
 
weights.
 
Prompts
 
guide
 
the
 
model
 
to
 
perform
 
speciﬁc
 
tasks.
 ➔  Use  Case:  Primarily  used  in  language  models  to  adapt  to  speciﬁc  language  tasks  
without
 
modifying
 
the
 
model
 
architecture.
 ➔  Pros:  Lightweight,  requires  minimal  additional  computation.  ➔  Cons:  Limited  to  prompt-based  models,  and  performance  may  be  l

---

rompt-based  models,  and  performance  may  be  lower  than  full  
ﬁne-tuning
 
on
 
complex
 
tasks.
  8.  Knowledge  Distillation  ➔  Description:  A  smaller  "student"  model  is  ﬁne-tuned  using  the  outputs  or  
"knowledge"
 
from
 
a
 
larger,
 
pre-trained
 
"teacher"
 
model.
 ➔  Use  Case:  Useful  when  deploying  large  models  is  infeasible,  but  their  knowledge  can  
be
 
distilled
 
into
 
smaller,
 
efﬁcient
 
models.
 ➔  Pros:  Results  in  a  compact  model  with  impr

---

Pros:  Results  in  a  compact  model  with  improved  task-speciﬁc  performance.  ➔  Cons:  Distillation  process  can  be  complex  and  might  require  ﬁne-tuning  both  
teacher
 
and
 
student
 
models.
  
Q3.  When  should  you  go  for  ﬁne-tuning?  
Fine-tuning
 
is
 
best
 
when
 
you: ➢  Have  Limited  Task-Speciﬁc  Data:  Fine-tuning  adapts  a  pre-trained  model  to  your  
data
 
without
 
needing
 
massive
 
datasets.
 ➢  Need  High  Accuracy:  It  improves  precision  on  specia

---

gh  Accuracy:  It  improves  precision  on  specialized  tasks,  crucial  for  
applications
 
requiring
 
high
 
performance.
 ➢  Have  Related  Domains:  If  your  task  resembles  the  pre-trained  model's  domain,  
ﬁne-tuning
 
can
 
be
 
highly
 
effective.
 ➢  Face  Computational  Limits:  It’s  more  efﬁcient  than  training  from  scratch,  especially  
for
 
large
 
models.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Que

---

Generative  AI  Interview  Questions   139 
 
➢  Want  Faster  Development:  Fine-tuning  speeds  up  customization  for  new  tasks.  ➢  Need  Regular  Updates:  It  allows  models  to  quickly  adapt  to  new  trends  or  
terminology.
  
Q4.  What  is  the  difference  between  Fine-tuning  and  Transfer  Learning? Fine-tuning  and  transfer  learning  are  closely  related  concepts,  but  they  differ  in  scope  and  
approach:
 1.  Scope  and  Deﬁnition  ➢  Transfer  L

---

proach:
 1.  Scope  and  Deﬁnition  ➢  Transfer  Learning:  Broadly  refers  to  the  concept  of  leveraging  knowledge  from  one  
task
 
(or
 
domain)
 
to
 
improve
 
performance
 
on
 
another
 
related
 
task.
 
It
 
includes
 
various
 
techniques,
 
such
 
as
 
using
 
pre-trained
 
models
 
as
 
feature
 
extractors
 
or
 
initializing
 
with
 
pre-trained
 
weights.
 ➢  Fine-Tuning:  A  type  of  transfer  learning  where  a  pre-trained  model  is  further  
trained
 
(ﬁne-tuned)
 
o

---

ed  model  is  further  
trained
 
(ﬁne-tuned)
 
on
 
a
 
speciﬁc
 
new
 
dataset,
 
often
 
with
 
adjustments
 
to
 
model
 
parameters
 
to
 
adapt
 
it
 
closely
 
to
 
the
 
new
 
task.
  2.  Usage  ➢  Transfer  Learning:  Includes  both  feature  extraction  (using  the  model’s  learned  
features
 
without
 
additional
 
training)
 
and
 
ﬁne-tuning.
 
It’s
 
a
 
broader
 
strategy,
 
where
 
the
 
model
 
may
 
remain
 
frozen,
 
partially
 
trainable,
 
or
 
fully
 
trainable
 
based

---

ly
 
trainable,
 
or
 
fully
 
trainable
 
based
 
on
 
the
 
task’s
 
needs.
 ➢  Fine-Tuning:  Speciﬁcally  refers  to  training  (usually  a  subset  of)  model  layers  on  a  
new
 
dataset.
 
It’s
 
more
 
focused
 
and
 
typically
 
implies
 
modifying
 
or
 
updating
 
some
 
of
 
the
 
model’s
 
weights.
  3.  Level  of  Adaptation  ➢  Transfer  Learning:  Often  preserves  much  of  the  original  knowledge  of  the  model,  
making
 
minimal
 
changes
 
(especially
 
in
 
feature
 
ext

---

nimal
 
changes
 
(especially
 
in
 
feature
 
extraction).
 ➢  Fine-Tuning:  Provides  a  deeper  adaptation  to  the  new  task  by  updating  model  
weights,
 
resulting
 
in
 
a
 
model
 
that
 
can
 
learn
 
more
 
speciﬁc
 
patterns
 
from
 
the
 
new
 
data.
  4.  Example  ➢  Transfer  Learning:  Using  a  pre-trained  model’s  features  (like  image  embeddings)  
directly
 
for
 
a
 
new
 
classiﬁcation
 
task.
 ➢  Fine-Tuning:  Starting  with  a  pre-trained  language  model  and  fur

---

g  with  a  pre-trained  language  model  and  further  training  it  on  a  
dataset
 
of
 
ﬁnancial
 
documents
 
to
 
improve
 
performance
 
in
 
ﬁnancial
 
sentiment
 
analysis.
  
Q5.  Write  about  the  instruction  ﬁnetune  and  explain  how  does  it  work Instruction  ﬁne-tuning  is  a  technique  to  improve  a  model’s  ability  to  follow  prompts  or  
instructions.
 
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Quest

---

Generative  AI  Interview  Questions   140 
 
Here’s
 
how
 
it
 
works: ➢  Collect  Instruction  Data:  Create  a  dataset  with  various  instruction-response  pairs.  ➢  Adapt  Model  for  Prompts:  Adjust  the  model’s  architecture  to  better  interpret  and  
respond
 
to
 
instructional
 
input.
 ➢  Fine-Tune  with  Instructions:  Train  the  model  on  the  instruction  dataset,  teaching  it  
to
 
respond
 
accurately
 
to
 
different
 
prompts.
 ➢  Evaluate  and  Ad

---

to
 
different
 
prompts.
 ➢  Evaluate  and  Adjust:  Test  and  reﬁne  the  model  to  ensure  it  generalizes  well  across  
various
 
instructions.
  
Q6.  Explaining  RLHF  in  Detail. Reinforcement  Learning  from  Human  Feedback  (RLHF)  is  a  technique  to  improve  AI  by  
aligning
 
it
 
with
 
human
 
preferences.
 
 
 
Here’s
 
how
 
it
 
works: ❖  Pre-Train  a  Base  Model:  A  language  model  is  trained  on  general  data  to  establish  
basic
 
understanding.
 ❖  Collect

---

o  establish  
basic
 
understanding.
 ❖  Collect  Human  Feedback:  Humans  rank  or  rate  the  model's  responses  for  qualities  
like
 
helpfulness
 
and
 
safety.
 ❖  Train  a  Reward  Model:  This  model  learns  from  the  feedback,  assigning  high  rewards  
to
 
preferred
 
responses.
 ❖  Optimize  with  Reinforcement  Learning:  The  model  then  uses  reinforcement  
learning
 
to
 
maximize
 
the
 
reward
 
model’s
 
scores,
 
reﬁning
 
responses
 
to
 
align
 
with
 
human
 
pref

---

ing
 
responses
 
to
 
align
 
with
 
human
 
preferences.
 RLHF  makes  AI  safer,  more  responsive,  and  better  suited  to  real-world  tasks  by  
continuously
 
learning
 
from
 
human-guided
 
feedback.
 
 
Q7.  Write  the  different  RLHF  techniques 
RLHF
 
Techniques  ➢  Reward  Modeling:  Train  a  reward  model  from  human  feedback  to  guide  
reinforcement
 
learning.
 ➢  Preference  Ranking:  Collect  human  rankings  of  outputs  to  reﬁne  the  model.  ➢  Reinforcement  Learn

---

s  to  reﬁne  the  model.  ➢  Reinforcement  Learning:  Use  algorithms  like  PPO  to  align  the  model  with  the  
reward
 
model.
 ➢  Imitation  Learning:  Learn  from  human  demonstrations  via  behavior  cloning  or  IRL.  ➢  Interactive  Feedback:  Iteratively  improve  the  model  with  real-time  human  input.  ➢  Human-in-the-Loop  RL:  Actively  involve  humans  to  review  and  reﬁne  outputs.  ➢  Multimodal  Feedback:  Incorporate  feedback  from  text,  images,  or  speech.  ➢  A

---

feedback  from  text,  images,  or  speech.  ➢  Active  Learning:  Prioritize  feedback  on  uncertain  or  ambiguous  outputs.  ➢  Penalizing  Undesired  Behavior:  Train  to  avoid  harmful  or  undesirable  outputs.  ➢  Comparative  RLHF:  Use  nuanced  comparisons  instead  of  binary  feedback.  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   141 
 
➢  Curriculum  Learning:  Gradually  introduce  complex  tasks  for

---

ning:  Gradually  introduce  complex  tasks  for  step-by-step  
improvement.
 ➢  Co-Training:  Combine  RLHF  with  unsupervised  pre-training  for  efﬁciency.  
 
Q8.  Explaining  PEFT  in  Detail. Parameter-Efﬁcient  Fine-Tuning  (PEFT)  is  a  method  to  ﬁne-tune  large  pre-trained  models  
efﬁciently
 
by
 
updating
 
only
 
a
 
small
 
subset
 
of
 
parameters
 
or
 
adding
 
lightweight
 
trainable
 
components,
 
instead
 
of
 
ﬁne-tuning
 
the
 
entire
 
model.
 
PEFT
 
is
 
designed

---

ing
 
the
 
entire
 
model.
 
PEFT
 
is
 
designed
 
to
 
save
 
computational
 
resources,
 
reduce
 
memory
 
requirements,
 
and
 
maintain
 
performance
 
close
 
to
 
full
 
ﬁne-tuning.
 
 
 
Key
 
Techniques LoRA:  Adds  low-rank  trainable  matrices  to  speciﬁc  layers.  Adapters:  Inserts  small,  task-speciﬁc  modules  in  transformer  layers.  Preﬁx  Tuning:  Adds  trainable  preﬁx  tokens  to  input  embeddings.  Prompt  Tuning:  Learns  task-speciﬁc  prompts  prepended  to  inputs.

---

ns  task-speciﬁc  prompts  prepended  to  inputs.  BitFit:  Updates  only  bias  terms  in  the  model.  Hybrid  Methods:  Combines  techniques  like  LoRA  and  Adapters.  
 
Advantages Efﬁciency:  Reduces  memory,  compute,  and  storage.  Scalability:  Works  with  very  large  models.  Flexibility:  Enables  multi-task  learning  with  minimal  updates.  
 
Q9.  What  is  LoRA  and  QLoRA? 
LoRA
 
(Low-Rank
 
Adaptation) LoRA  is  a  Parameter-Efﬁcient  Fine-Tuning  (PEFT)  technique  for  a

---

r-Efﬁcient  Fine-Tuning  (PEFT)  technique  for  adapting  large  pre-trained  
models
 
to
 
speciﬁc
 
tasks
 
by
 
introducing
 
small,
 
low-rank
 
trainable
 
matrices.
 
Instead
 
of
 
ﬁne-tuning
 
all
 
model
 
parameters,
 
LoRA
 
updates
 
only
 
these
 
additional
 
matrices,
 
signiﬁcantly
 
reducing
 
memory
 
and
 
compute
 
costs.
 
How
 
LoRA
 
Works: ➢  Insert  Trainable  Low-Rank  Matrices:  Decomposes  weight  updates  into  low-rank  
matrices
   ,  where   Matrices  A  and   a

---

rank  
matrices
   ,  where   Matrices  A  and   are  trainable  but  much  smaller  in  size  than  the  original  weight  .   ➢  Freeze  the  Original  Model: The  base  model  parameters  remain  unchanged.  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   142 
 
 ➢  Efﬁcient  Fine-Tuning: Train  only  the  A  and   matrices,  saving  memory  and  compute.  
 
Advantages
 
of
 
LoRA: ➢  Efﬁciency:  Drastically  reduces  t

---

of
 
LoRA: ➢  Efﬁciency:  Drastically  reduces  the  number  of  trainable  parameters.  ➢  Reusability:  Keeps  the  base  model  intact,  making  it  reusable  for  multiple  tasks.  ➢  Cost-Effectiveness:  Enables  ﬁne-tuning  even  on  resource-constrained  hardware.  
 
QLoRA
 
(Quantized
 
LoRA) QLoRA  is  designed  to  further  reduce  memory  usage  by  quantizing  the  base  model's  
parameters
 
during
 
ﬁne-tuning.
 
It
 
combines
 
low-rank
 
adaptation
 
with
 
4-bit
 
quantizatio

---

low-rank
 
adaptation
 
with
 
4-bit
 
quantization,
 
allowing
 
ﬁne-tuning
 
of
 
very
 
large
 
models
 
on
 
consumer-grade
 
hardware.
 
How
 
QLoRA
 
Works: Quantize  the  Base  Model:  Converts  the  base  model  weights  into  a  4-bit  integer  format,  
reducing
 
memory
 
without
 
signiﬁcant
 
loss
 
in
 
performance.
 Use  LoRA  for  Fine-Tuning:  Adds  low-rank  trainable  matrices  (like  standard  LoRA)  while  
keeping
 
the
 
quantized
 
weights
 
frozen.
 Dequantization  for

---

antized
 
weights
 
frozen.
 Dequantization  for  Operations:  During  computation,  de-quantized  the  4-bit  weights  only  
temporarily
 
to
 
perform
 
matrix
 
multiplications.
 
Advantages
 
of
 
QLoRA: Extreme  Efﬁciency:  Reduces  memory  usage  by  both  quantizing  weights  and  using  
low-rank
 
updates.
 Scalable  Fine-Tuning:  Enables  ﬁne-tuning  of  models  with  over  100  billion  parameters  on  a  
single
 
GPU.
 
High
 
Performance:
 
Maintains
 
task
 
performance
 
close

---

rmance:
 
Maintains
 
task
 
performance
 
close
 
to
 
full-precision
 
ﬁne-tuning
.   
Q10.  Deﬁne  “pre-training”  vs.  “ﬁne-tuning”  in  LLMs. 
Pre-training The  initial  training  phase  where  the  model  learns  general  language  patterns  from  large,  
unlabeled
 
datasets.
 The  main  objective  is  to  develop  broad  language  understanding.  Example:  GPT-3  learning  from  billions  of  web  pages.  
 
Fine-tuning A  subsequent  training  phase  where  the  model  is  specialized

---

aining  phase  where  the  model  is  specialized  for  speciﬁc  tasks  using  smaller,  
labeled
 
datasets.
 The  main  objective  is  to  customize  for  a  speciﬁc  use  case  (e.g.,  sentiment  analysis).  Example:  Adapting  GPT  for  customer  support  chat.  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   143 
 
 
Q11.  How  do  you  train  LLM  models  with  billions  of  parameters?(training  
pipeline
 
of
 
llm)

---

s  of  parameters?(training  
pipeline
 
of
 
llm) 
Techniques
 
for
 
Fine-tuning
 
Billion-Parameter
 
LLMs Full  Fine-tuning:  ➢  Update  all  model  parameters.  ➢  Expensive;  used  when  large  labeled  datasets  are  available.   Parameter-Efﬁcient  Fine-tuning  (PEFT):  ➢  LoRA:  Adds  trainable  low-rank  matrices.  ➢  Adapters:  Inserts  small  trainable  modules.  ➢  Preﬁx  Tuning:  Optimizes  prepended  task-speciﬁc  prompts.   Instruction  Tuning:  ➢  Fine-tune  with  instruction-re

---

ction  Tuning:  ➢  Fine-tune  with  instruction-response  datasets  for  task  generalization.   Reinforcement  Learning  with  Human  Feedback  (RLHF):  ➢  Aligns  models  to  user  preferences  using  human-labeled  feedback  and  
rewards.
 Few-Shot  Fine-tuning:  ➢  Fine-tune  with  minimal  task-speciﬁc  data.   Multitask  Fine-tuning:  ➢  Train  on  multiple  tasks  for  better  generalization.  
 
Q11.  How  does  LoRA  work? 
Here’s,
 
how
 
LoRA
 
works
  1.  Low-Rank  Update:  Adds  tw

---

w
 
LoRA
 
works
  1.  Low-Rank  Update:  Adds  two  small  trainable  matrices   (low-rank)  and   to  existing  weights  :   2.  Frozen  Base  Weights:  Original  weights   are  frozen;  only   and    are  updated.   3.  Parameter  Efﬁciency:  Signiﬁcantly  reduces  trainable  parameters  (low  memory  and  compute  cost).   4.  Task-Speciﬁc  Adaptation:  Trains   and    for  speciﬁc  tasks,  preserving  general  knowledge  in  .   5.  Easy  Deployment:  

MD  ARBAAZ  KHAN

---

ployment:  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   144 
 
Modular;  task-speciﬁc  matrices  can  be  swapped  without  retraining.    
 
Q12.  How  do  you  train  an  LLM  model  that  prevents  prompt  
hallucinations? 
To
 
prevent
 
hallucinations
 
in
 
an
 
LLM: ➢  High-Quality  Data:  Use  curated,  factual  datasets  and  ﬁlter  noisy  or  
speculative
 
data.
 ➢  Retrieval-Augmented  Generation  (RAG):  Lin

---

.
 ➢  Retrieval-Augmented  Generation  (RAG):  Link  the  model  to  external  
knowledge
 
bases
 
for
 
fact-checking.
 ➢  Reinforcement  Learning  with  Human  Feedback  (RLHF):  Fine-tune  the  model  
with
 
human
 
evaluation
 
and
 
reward
 
systems.
 ➢  Instruction  Tuning:  Train  the  model  to  follow  clear,  factual  instructions.  ➢  Fact-Checking  Safeguards:  Integrate  mechanisms  to  validate  or  ﬂag  
uncertain
 
outputs.
 
Q13.  How  do  you  prevent  bias  and  harmful  pro

---

3.  How  do  you  prevent  bias  and  harmful  prompt  generation? 
To
 
prevent
 
bias
 
and
 
harmful
 
prompt
 
generation
 
in
 
LLMs: 1.  Diverse  and  Representative  Training  Data:   ➢  Use  datasets  that  reﬂect  a  broad  range  of  perspectives  and  avoid  
over-representation
 
of
 
speciﬁc
 
viewpoints
 
or
 
stereotypes.
  2.  Bias  Detection  and  Mitigation:  ➢  Analyze  training  data  and  outputs  for  biases  using  automated  and  human  
evaluations.
 ➢  Apply  debiasing

---

d  and  human  
evaluations.
 ➢  Apply  debiasing  techniques  like  adversarial  training  or  reweighting.   3.  Reinforcement  Learning  with  Human  Feedback  (RLHF):   ➢  Incorporate  human  reviewers  trained  to  detect  and  penalize  harmful  or  
biased
 
outputs
 
during
 
ﬁne-tuning.
  4.  Ethical  Fine-Tuning:   ➢  Train  the  model  on  examples  of  ethical,  inclusive,  and  neutral  language  to  
promote
 
responsible
 
behavior.
  5.  Guardrails  and  Filters:  ➢  Implement  p

---

.
  5.  Guardrails  and  Filters:  ➢  Implement  prompt  and  output  moderation  systems  to  identify  and  block  
harmful
 
content.
 ➢  Use  rule-based  or  machine  learning  classiﬁers  to  ﬂag  unsafe  responses.   6.  Explainability  and  Transparency:   

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   145 
 
➢  Teach  the  model  to  explain  reasoning  for  outputs,  aiding  in  bias  detection  
and
 
correction.

---

,  aiding  in  bias  detection  
and
 
correction.
  7.  Continuous  Monitoring  and  Iteration:   ➢  Collect  feedback  on  harmful  outputs  post-deployment  and  use  it  to  improve  
the
 
model
 
through
 
regular
 
updates.
  8.  Uncertainty  Handling:   ➢  Program  the  model  to  acknowledge  when  it  lacks  enough  context,  avoiding  
speculation
 
or
 
harmful
 
assumptions.
 
 
Q14.  How  does  proximal  policy  gradient  work  in  a  prompt  generation?  Proximal  Policy  Gradient

---

a  prompt  generation?  Proximal  Policy  Gradient  (PPO)  is  a  reinforcement  learning  algorithm  often  used  in  training  
large
 
language
 
models
 
(LLMs)
 
via
 
Reinforcement
 
Learning
 
with
 
Human
 
Feedback
 
(RLHF)
 
to
 
reﬁne
 
prompt
 
generation.
 
Here's
 
a
 
concise
 
explanation
 
of
 
how
 
PPO
 
works
 
in
 
this
 
context:
 
 
Steps
 
in
 
PPO
 
for
 
Prompt
 
Generation: Model  as  a  Policy:  The  language  model  (LLM)  acts  as  a  stochastic  policy   ,  where:

---

LLM)  acts  as  a  stochastic  policy   ,  where:  ➢   is  the  input  prompt  (state).  ➢   is  the  generated  text  (action).  ➢   are  the  model  parameters.   Reward  Signal:  A  reward  function  R(a ∣ s)  evaluates  the  quality  of  the  generated  output.  Rewards  can  be  
based
 
on:
 ➢  Alignment  with  user  intent.  ➢  Factual  accuracy,  ethicality,  or  lack  of  harmful  content.  ➢  Ratings  from  human  feedback  or  learned  reward  models.   Optimization  Objective:  PPO

---

reward  models.   Optimization  Objective:  PPO  optimizes  a  clipped  surrogate  objective:   
➢  :  The  probability  ratio  of  the  new  and  old  policies.  ➢   :  Advantage  function,  estimating  the  improvement  of  an  action  over  baseline  performance.  ➢  :  Clipping  parameters  to  limit  policy  updates,  ensuring  stability  and  preventing  large  changes.  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions

---

Generative  AI  Interview  Questions   146 
 
 Clipping:    ➢  By  restricting  updates  to  be  within  ,  PPO  avoids  excessive  changes  that  can  destabilize  learning,  ensuring  smooth  and  gradual  policy  improvement.   Iterative  Training:  ➢  The  model  generates  outputs  based  on  the  current  policy.  ➢  Rewards  guide  updates  to  the  policy  using  the  PPO  objective.  ➢  This  process  repeats  until  the  model  consistently  produces  high-quality,  aligne

---

del  consistently  produces  high-quality,  aligned  
prompts.
 
 
Q15.  How  does  knowledge  distillation  beneﬁt  LLMs? Knowledge  distillation  beneﬁts  LLMs  by  transferring  knowledge  from  a  large,  
computationally
 
expensive
 
teacher
 
model
 
to
 
a
 
smaller,
 
faster
 
student
 
model,
 
enabling:
 ➢  Efﬁciency:  Reduces  inference  time  and  resource  usage.  ➢  Scalability:  Makes  deployment  on  edge  devices  or  low-power  systems  feasible.  ➢  Performance  Retention:  P

---

systems  feasible.  ➢  Performance  Retention:  Preserves  key  capabilities  of  the  teacher  while  reducing  
complexity.
 ➢  Specialization:  Allows  ﬁne-tuning  the  student  for  speciﬁc  tasks  or  domains.  
 
Q16.  What’s  “few-shot”  learning  in  LLMs?(RAG) Few-shot  learning  in  Retrieval-Augmented  Generation  (RAG)  involves  providing  the  LLM  
with
 
a
 
small
 
number
 
of
 
examples
 
(few
 
shots)
 
of
 
the
 
task
 
in
 
the
 
input
 
prompt.
 
 These  examples  guide  t

---

the
 
input
 
prompt.
 
 These  examples  guide  the  model's  behavior,  helping  it  generate  better,  task-speciﬁc  
outputs
 
without
 
requiring
 
additional
 
ﬁne-tuning.
 
 The  retrieval  mechanism  enhances  this  by  supplying  relevant  external  knowledge  to  
improve
 
accuracy
 
and
 
context.
 
 
Q17.  Evaluating  LLM  performance  metrics? 
Key
 
metrics
 
for
 
evaluating
 
LLM
 
performance
 
include: 1.  Perplexity:  Measures  how  well  the  model  predicts  text;  lower  i

---

how  well  the  model  predicts  text;  lower  is  better.  2.  BLEU/ROUGE:  Evaluate  similarity  to  reference  text  for  translation/summarization  
tasks.
 3.  Accuracy:  Assesses  correctness  in  classiﬁcation  or  reasoning  tasks.  4.  Factuality:  Measures  alignment  with  ground  truth  knowledge.  5.  Human  Ratings:  Evaluates  ﬂuency,  relevance,  and  coherence  via  human  feedback.  6.  Bias  and  Safety  Metrics:  Tests  for  fairness  and  harmful  content  generation.  7.

---

fairness  and  harmful  content  generation.  7.  Latency:  Measures  response  speed  for  real-time  usability.  8.  Calibration:  Assesses  how  well  conﬁdence  scores  match  actual  accuracy.  
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   147 
 
Q18.  How  would  you  use  RLHF  to  train  an  LLM  model?(RLHF)  
To
 
train
 
an
 
LLM
 
using
 
Reinforcement
 
Learning
 
with
 
Human
 
Feedback
 
(RLHF):
 ➢  Pret

---

rning
 
with
 
Human
 
Feedback
 
(RLHF):
 ➢  Pretrain  the  Model:  Train  the  LLM  on  a  large  corpus  of  text  to  generate  coherent  
and
 
diverse
 
outputs.
 ➢  Collect  Human  Feedback:  Gather  preferences  or  ratings  for  model-generated  
responses
 
to
 
deﬁne
 
desirable
 
behavior.
 ➢  Train  a  Reward  Model:  Use  the  feedback  to  train  a  model  that  scores  outputs  based  
on
 
alignment
 
with
 
human
 
preferences.
 ➢  Optimize  with  RL:  Fine-tune  the  LLM  usin

---

➢  Optimize  with  RL:  Fine-tune  the  LLM  using  reinforcement  learning  (e.g.,  PPO),  
maximizing
 
the
 
reward
 
signal
 
from
 
the
 
reward
 
model.
 ➢  Iterate:  Repeat  the  process  with  updated  data  and  reward  models  to  reﬁne  
performance.
 
 
Q19.  What  techniques  can  be  employed  to  improve  the  factual  accuracy  
of
 
text
 
generated
 
by
 
LLMs?(RAG) 
To
 
improve
 
the
 
factual
 
accuracy
 
of
 
text
 
generated
 
by
 
LLMs,
 
particularly
 
in
 
Retrieval-Au

---

ed
 
by
 
LLMs,
 
particularly
 
in
 
Retrieval-Augmented
 
Generation
 
(RAG): ➢  Integrate  External  Knowledge  Bases:  Use  retrieval  mechanisms  to  fetch  and  ground  
outputs
 
in
 
up-to-date,
 
factual
 
information.
 ➢  Fine-Tune  on  Reliable  Data:  Train  on  datasets  veriﬁed  for  factual  accuracy  and  
relevance.
 ➢  Fact-Checking  Pipelines:  Incorporate  tools  to  validate  generated  text  against  trusted  
sources.
 ➢  Prompt  Engineering:  Design  prompts  to  guide  t

---

rompt  Engineering:  Design  prompts  to  guide  the  model  toward  more  fact-based  
responses.
 ➢  Reinforcement  Learning:  Use  RLHF  with  rewards  focused  on  factual  correctness.  ➢  Calibration  Mechanisms:  Enable  the  model  to  express  uncertainty  when  data  is  
insufﬁcient.
 
 
Q20.  How  would  you  detect  drift  in  LLM  performance  over  time,  
especially
 
in
 
real-world
 
production
 
settings?(monitoring
 
and
 
evaluation
 
metrics) 
To
 
detect
 
drift
 
in
 
LLM

---

luation
 
metrics) 
To
 
detect
 
drift
 
in
 
LLM
 
performance
 
in
 
production,
 
focus
 
on: ➢  Baseline  Metrics:  Regularly  evaluate  model  outputs  against  benchmarks  like  BLEU,  
ROUGE,
 
or
 
task-speciﬁc
 
accuracy.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   148 
 
➢  Input  Drift:  Monitor  changes  in  input  distributions  (e.g.,  token  frequencies,  
embeddings)
 
using
 
Jensen-Shannon
 
Divergen

---

embeddings)
 
using
 
Jensen-Shannon
 
Divergence
 
or
 
Cosine
 
Similarity.
 ➢  Output  Quality:  Track  prediction  relevance  (e.g.,  BLEU,  perplexity)  and  user  
feedback
 
(e.g.,
 
satisfaction
 
scores,
 
error
 
corrections).
 ➢  User  Interaction  Metrics:  Analyze  session  abandonment,  click-through  rates,  and  
escalations
 
for
 
signs
 
of
 
performance
 
issues.
 ➢  Automated  Alerts:  Set  thresholds  for  anomalies  in  performance  metrics  using  
statistical
 
method

---

performance  metrics  using  
statistical
 
methods
 
or
 
ML-based
 
anomaly
 
detectors.
 ➢  Regular  Validation:  Periodically  retrain  and  test  the  model  on  updated  data  to  
address
 
observed
 
drift.
 ➢  Domain-Speciﬁc  Metrics:  Tailor  KPIs  like  factual  accuracy  or  adherence  to  
guidelines
 
as
 
needed.
 ➢  Canary  Models:  Use  A/B  testing  or  shadow  deployments  to  compare  model  
versions
 
on
 
live
 
data.
 
 
Q21.   Describe  strategies  for  curating  a  high

---

21.   Describe  strategies  for  curating  a  high-quality  dataset  tailored  for  
training
 
a
 
generative
 
AI
 
model.
 
To
 
curate
 
a
 
high-quality
 
dataset
 
for
 
a
 
generative
 
AI
 
model:  ➢  Deﬁne  Objectives:  Specify  the  model's  purpose,  domain,  and  output  requirements.  ➢  Source  Data:  Collect  diverse,  domain-relevant  data  from  public  datasets,  
proprietary
 
sources,
 
or
 
custom
 
collection.
 ➢  Clean  Data:  Remove  noise,  duplicates,  and  bias  while

---

a:  Remove  noise,  duplicates,  and  bias  while  standardizing  formats.  ➢  Annotate:  Add  metadata  and  labels  for  context  and  conditioning.  ➢  Balance  Data:  Ensure  diversity  and  avoid  overrepresentation  of  certain  categories.  ➢  Augment  Data:  Use  paraphrasing,  synthetic  data,  and  controlled  noise  for  
robustness.
 ➢  Validate  Quality:  Review  samples  and  monitor  quality  metrics  like  diversity  and  
entropy.
 ➢  Document  and  Update:  Version  datasets,

---

.
 ➢  Document  and  Update:  Version  datasets,  document  sources,  and  refresh  regularly.  ➢  Ensure  Ethics:  Source  data  legally  and  exclude  harmful  or  sensitive  content.  
  
Q22.  What  methods  exist  to  identify  and  address  biases  within  training  
data
 
that
 
might
 
impact
 
the
 
generated
 
output? 
To
 
identify
 
and
 
address
 
biases
 
in
 
training
 
data: ❖  Analysis  Methods:  ➢  Statistical  Bias  Detection:  Analyze  word/token  frequencies,  demographic

---

Analyze  word/token  frequencies,  demographic  
representation,
 
and
 
co-occurrence
 
patterns.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   149 
 
➢  Embedding  Analysis:  Use  tools  like  Word  Embedding  Association  Tests  
(WEAT)
 
to
 
uncover
 
biases
 
in
 
word
 
associations.
 ➢  Counterfactual  Testing:  Test  model  outputs  with  diverse  inputs  (e.g.,  gender  
or
 
race
 
variations)
 
to
 
spot

---

,  gender  
or
 
race
 
variations)
 
to
 
spot
 
disparities.
 ❖  Evaluation  Metrics:  ➢  Demographic  Parity:  Measure  output  consistency  across  groups.  ➢  Equality  of  Opportunity:  Ensure  fair  performance  for  all  subgroups.  ➢  Bias  Ampliﬁcation:  Track  the  extent  to  which  model  output  reﬂects  biases  in  
training
 
data.
 ❖  Mitigation  Techniques:  ➢  Balanced  Dataset:  Ensure  diverse  and  representative  training  data.  ➢  Adversarial  Debiasing:  Train  models

---

data.  ➢  Adversarial  Debiasing:  Train  models  to  minimize  bias  through  adversarial  
loss.
 ➢  Post-Processing:  Adjust  outputs  to  reduce  bias  after  generation.  
 
Q23.  How  would  you  ﬁne-tune  LLM  for  domain-speciﬁc  purposes  like  
ﬁnancial
 
and
 
medical
 
applications? 
To
 
ﬁne-tune
 
an
 
LLM
 
for
 
domains
 
like
 
ﬁnance
 
or
 
medicine: ➢  Data  Preparation:  Gather  high-quality,  domain-speciﬁc  data,  clean,  and  annotate  it.  ➢  Fine-Tuning  Strategy:  Use

---

d  annotate  it.  ➢  Fine-Tuning  Strategy:  Use  full  ﬁne-tuning,  LoRA,  or  adapters  depending  on  
resources.
 ➢  Training:  Employ  frameworks  like  Hugging  Face  with  domain-speciﬁc  loss  and  
validation
 
metrics.
 ➢  Evaluation:  Test  outputs  for  domain  accuracy,  generalization,  and  factual  
consistency.
 ➢  Ethical  Compliance:  Ensure  privacy,  regulatory  compliance  (e.g.,  HIPAA),  and  bias  
mitigation.
 ➢  Deployment:  Monitor  performance  and  continuously  upd

---

ent:  Monitor  performance  and  continuously  update  with  new  data.  
 
Q24.  Explain  the  algorithm  architecture  for  LLAMA  and  other  LLMs  alike.  
LLAMA
 
and
 
LLM
 
Algorithm
 
Architecture ➢  Base  Architecture:  ○  Transformer  Decoder-Only:  Optimized  for  autoregressive  text  generation.   ➢  Key  Components:  ○  Token  Embeddings:  Converts  text  into  dense  vectors.  ○  Positional  Encoding:  Adds  sequence  order  information.  ○  Multi-Head  Self-Attention:  Captures

---

ation.  ○  Multi-Head  Self-Attention:  Captures  relationships  between  tokens.  ○  Feedforward  Networks:  Processes  attention  outputs.  ○  Layer  Normalization  &  Residual  Connections:  Improves  training  stability.  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   150 
 
 ➢  Output  Layer:  ○  A  linear  layer  maps  outputs  to  vocabulary  probabilities  via  softmax.   ➢  Training  Objective:  ○  Causal  Languag

---

max.   ➢  Training  Objective:  ○  Causal  Language  Modeling:  Predicts  the  next  token  based  on  previous  
tokens.
 
                    
             

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   151 
 
                   
Vector  Database                  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   152 
 
Q1.  What  are  vector  database

---

uestions   152 
 
Q1.  What  are  vector  databases,  and  how  do  they  differ  from  traditional  
relational
 
databases? Vector  databases  are  specialized  storage  systems  designed  to  store,  manage,  and  retrieve  
high-dimensional
 
vectors
 
(often
 
used
 
for
 
machine
 
learning
 
and
 
AI
 
tasks).
 
  Aspect  Vector  Databases  Relational  Databases  (RDBMS)  
Data  Format  High-dimensional  vectors  (e.g.,  embeddings)  
Structured  tables  with  rows  and  columns  
Query

---

ctured  tables  with  rows  and  columns  
Query  Type  Nearest  neighbor  search,  similarity-based  queries  
SQL  queries  for  ﬁltering,  joins,  and  aggregations  
Indexing  Specialized  indexing  methods  like  HNSW  (Hierarchical  Navigable  Small  World)  for  fast  similarity  search  
B-tree,  hash  indexes  for  fast  lookups  and  joins  
Use  Case  AI,  machine  learning,  NLP,  image  search  
Transactional  systems,  analytics,  reporting  
Data  Structure  Unstructured  or  semi

---

eporting  
Data  Structure  Unstructured  or  semi-structured  data  represented  as  vectors  
Structured  data  with  predeﬁned  schemas  
Scalability  Optimized  for  high-dimensional  and  large  datasets  
Optimized  for  structured,  relational  data  with  fewer  dimensions  
 
Q2.  Explain  how  vector  embeddings  are  generated  and  their  role  in  
vector
 
databases. Vector  embeddings  are  numerical  representations  of  data  (e.g.,  text,  images,  or  audio)  
generated
 
usin

---

g.,  text,  images,  or  audio)  
generated
 
using
 
machine
 
learning
 
models,
 
such
 
as
 
transformers
 
or
 
neural
 
networks.
 
These
 
embeddings
 
capture
 
the
 
semantic
 
or
 
contextual
 
meaning
 
of
 
the
 
input
 
in
 
a
 
high-dimensional
 
space.
  In  vector  databases ,  embeddings  are  stored  and  indexed  to  enable  efﬁcient  similarity  
searches.
 
By
 
comparing
 
embeddings
 
using
 
metrics
 
like
 
cosine
 
similarity
 
or
 
Euclidean
 
distance,
 
vector
 
data

---

arity
 
or
 
Euclidean
 
distance,
 
vector
 
databases
 
can
 
quickly
 
retrieve
 
relevant
 
items
 
based
 
on
 
their
 
semantic
 
relationships.
 
This
 
is
 
crucial
 
for
 
applications
 
like
 
recommendation
 
systems,
 
natural
 
language
 
search,
 
and
 
image
 
retrieval.
 
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   153 
 
Q3.  What  are  the  key  challenges  in  indexing  and  searching  through  
high

---

nges  in  indexing  and  searching  through  
high-dimensional
 
vector
 
spaces? Indexing  and  searching  in  high-dimensional  vector  spaces  face  several  key  challenges:  ➢  Curse  of  Dimensionality:  As  dimensionality  increases,  the  data  points  tend  to  
become
 
equidistant,
 
making
 
traditional
 
distance
 
metrics
 
less
 
effective
 
and
 
increasing
 
computational
 
complexity.
 ➢  Scalability:  Managing  and  querying  millions  or  billions  of  high-dimensional  vecto

---

illions  or  billions  of  high-dimensional  vectors  
requires
 
signiﬁcant
 
computational
 
and
 
storage
 
resources.
 ➢  Trade-off  Between  Speed  and  Accuracy:  Exact  nearest-neighbor  search  is  
computationally
 
expensive,
 
leading
 
to
 
the
 
use
 
of
 
approximate
 
methods
 
(ANN),
 
which
 
may
 
compromise
 
result
 
accuracy
 
for
 
faster
 
performance.
 ➢  Efﬁcient  Index  Structures:  Designing  optimal  index  structures  (e.g.,  tree-based,  
hashing,
 
or
 
graph-based

---

(e.g.,  tree-based,  
hashing,
 
or
 
graph-based
 
methods)
 
that
 
balance
 
query
 
speed,
 
memory
 
usage,
 
and
 
dynamic
 
updates
 
is
 
challenging.
 ➢  Memory  and  Storage  Limitations:  High-dimensional  vectors  require  large  amounts  
of
 
memory
 
and
 
storage,
 
especially
 
when
 
working
 
with
 
dense
 
embeddings
 
and
 
large
 
datasets.
 ➢  Dynamic  Updates:  Maintaining  efﬁcient  indexing  while  continuously  adding,  
removing,
 
or
 
updating
 
vectors
 
without

---

removing,
 
or
 
updating
 
vectors
 
without
 
signiﬁcant
 
performance
 
degradation.
 ➢  Handling  Noise  and  Variability:  High-dimensional  spaces  are  sensitive  to  noise  in  
the
 
data
 
or
 
embeddings,
 
which
 
can
 
degrade
 
search
 
accuracy.
 ➢  Distance  Metric  Selection:  Choosing  or  tuning  the  appropriate  similarity  metric  
(e.g.,
 
cosine
 
similarity,
 
Euclidean
 
distance)
 
for
 
speciﬁc
 
data
 
types
 
and
 
use
 
cases
 
is
 
non-trivial.
 
 
Q4.  How  do

---

use
 
cases
 
is
 
non-trivial.
 
 
Q4.  How  do  you  evaluate  the  performance  of  a  vector  database  in  terms  
of
 
search
 
efﬁciency
 
and
 
accuracy? 
Search
 
Eﬃciency
 
Metrics Query  Latency:  Time  to  retrieve  results.  Throughput:  Queries  processed  per  second.  Memory/Storage  Usage:  Resource  efﬁciency.  Indexing  Speed:  Time  to  build/update  the  index.  
 
Search
 
Accuracy
 
Metrics Recall@K:  Fraction  of  true  neighbors  in  top  K  results.  Precision@K:  Fra

---

neighbors  in  top  K  results.  Precision@K:  Fraction  of  top  K  results  that  are  true  neighbors.  F1-Score:  Balance  of  recall  and  precision.  Mean  Reciprocal  Rank  (MRR):  Rank  of  the  ﬁrst  correct  result.  
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   154 
 
Q5.  Can  you  describe  a  scenario  where  you  would  prefer  using  a  vector  
database
 
over
 
a
 
traditional
 
database? A  vector  da

---

over
 
a
 
traditional
 
database? A  vector  database  is  preferred  when  working  with  unstructured  data  like  text,  images,  or  
audio,
 
and
 
the
 
goal
 
is
 
semantic
 
search
 
or
 
similarity-based
 
queries.
 
For
 
example:
  In  an  image  search  application,  users  upload  an  image  to  ﬁnd  visually  similar  ones.  A  vector  
database
 
can
 
store
 
and
 
search
 
high-dimensional
 
image
 
embeddings,
 
enabling
 
fast,
 
accurate
 
results,
 
unlike
 
traditional

---

st,
 
accurate
 
results,
 
unlike
 
traditional
 
databases
 
that
 
rely
 
on
 
exact
 
matching
 
of
 
predeﬁned
 
attributes.
 
 
Q6.  What  are  some  popular  vector  databases  available  today,  and  what  
unique
 
features
 
do
 
they
 
offer? 
Popular
 
Vector
 
Databases
 
and
 
Their
 
Features ➢  Pinecone  ○  Fully  managed  service.  ○  Scalable  and  optimized  for  real-time  updates.  ○  Built-in  metrics  like  cosine  similarity.  ➢  Milvus  ○  Open-source  and  highly  scala

---

ty.  ➢  Milvus  ○  Open-source  and  highly  scalable.  ○  Supports  hybrid  queries  (structured  +  vector).  ○  Integrates  with  popular  ML  tools  like  TensorFlow.  ➢  Weaviate  ○  Schema-based  with  hybrid  search.  ○  Offers  semantic  search  and  context-based  reasoning.  ○  Extensible  with  custom  modules.  ➢  Vespa  ○  Focused  on  real-time  applications.  ○  Combines  vectors  with  structured  data.  ○  Supports  advanced  ranking  and  ﬁltering.  ➢  Qdrant  ○  Open-source  w

---

king  and  ﬁltering.  ➢  Qdrant  ○  Open-source  with  high-performance  ANN  search.  ○  Offers  REST  and  gRPC  APIs  for  integration.  ○  Optimized  for  recall  and  relevance.  
 
Q7.  How  do  vector  databases  support  machine  learning  workﬂows,  
particularly
 
in
 
deploying
 
AI
 
models?
 
Vector
 
databases
 
support
 
machine
 
learning
 
workﬂows
 
by: ➢  Embedding  Storage:  Storing  vector  embeddings  generated  by  AI  models  for  efﬁcient  
retrieval.
 

MD  ARBAAZ  KHAN

---

ls  for  efﬁcient  
retrieval.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   155 
 
➢  Semantic  Search:  Enabling  similarity-based  searches  for  recommendations  or  
content
 
matching.
 ➢  Real-Time  Inference:  Serving  as  a  backend  for  AI  models  to  perform  fast  searches  
on
 
live
 
data.
 ➢  Model  Fine-Tuning:  Facilitating  feedback  loops  by  analyzing  search  results  to  
improve
 
model
 
accur

---

ing  search  results  to  
improve
 
model
 
accuracy.
 ➢  Hybrid  Queries:  Combining  vector  searches  with  structured  data  for  context-aware  
results.
 
 
Q8.  What  techniques  can  be  employed  to  ensure  the  scalability  of  a  
vector
 
database
 
as
 
the
 
dataset
 
grows? 
To
 
ensure
 
scalability
 
of
 
a
 
vector
 
database
 
as
 
the
 
dataset
 
grows: ➢  Sharding:  Distribute  data  across  multiple  nodes  to  balance  load.  ➢  Approximate  Nearest  Neighbor  (ANN):  Us

---

ad.  ➢  Approximate  Nearest  Neighbor  (ANN):  Use  ANN  algorithms  to  reduce  query  
complexity.
 ➢  Dynamic  Indexing:  Employ  index  structures  (like  HNSW  or  IVF)  that  adapt  to  
growing
 
data.
 ➢  Compression:  Use  techniques  like  PQ  or  binary  embeddings  to  reduce  storage  
needs.
 ➢  Caching:  Cache  frequent  queries  to  improve  response  times.  ➢  Horizontal  Scaling:  Add  more  nodes  to  handle  increased  data  and  queries.  ➢  Batch  Updates:  Optimize  data

---

and  queries.  ➢  Batch  Updates:  Optimize  data  ingestion  by  batching  updates  to  the  index.  
 
Q9.  How  can  you  handle  vector  data  that  may  have  different  
dimensionalities
 
or
 
representations? 
To
 
handle
 
vector
 
data
 
with
 
diﬀerent
 
dimensionalities
 
or
 
representations: ➢  Dimensionality  Reduction:  Use  techniques  like  PCA  or  t-SNE  to  standardize  
dimensions.
 ➢  Padding  or  Truncation:  Adjust  vectors  to  a  ﬁxed  size  by  padding  with  zeros

---

ors  to  a  ﬁxed  size  by  padding  with  zeros  or  
truncating.
 ➢  Projection:  Map  vectors  to  a  common  space  using  transformation  models.  ➢  Separate  Indexes:  Maintain  separate  indexes  for  different  dimensionalities.  ➢  Preprocessing  Pipelines:  Normalize  or  encode  data  consistently  during  
preprocessing.
 
  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   156 
 
Q10.  What  role  does  vector

---

uestions   156 
 
Q10.  What  role  does  vector  similarity  play  in  applications  like  
recommendation
 
systems
 
or
 
natural
 
language
 
processing? Vector  similarity  identiﬁes  relationships  between  items  by  measuring  their  closeness  in  a  
high-dimensional
 
space.
 
 
Recommendation
 
Systems:
 
 Finds  items  (e.g.,  products,  movies)  similar  to  user  preferences  or  behaviors  based  on  
embedding
 
proximity.
 
Natural
 
Language
 
Processing
 
(NLP):  Enables  sem

---

ral
 
Language
 
Processing
 
(NLP):  Enables  semantic  search,  text  clustering,  and  context-aware  matching  by  comparing  text  
embeddings
 
for
 
similarity.
 
              
             

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   157 
 
                  
LLMOPs  &  system  design                    

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Q

---

Generative  AI  Interview  Questions   158 
 
Q1.  You  need  to  design  a  system  that  uses  an  LLM  to  generate  responses  
to
 
a
 
massive
 
inﬂux
 
of
 
user
 
queries
 
in
 
near
 
real-time.
 
Discuss
 
strategies
 
for
 
scaling,
 
load
 
balancing,
 
and
 
optimizing
 
for
 
rapid
 
response
 
times. To  handle  a  massive  inﬂux  of  user  queries  in  real-time  using  an  LLM,  focus  on  scaling,  load  
balancing,
 
and
 
optimization:
 1.  Scaling  ➢  H

---

alancing,
 
and
 
optimization:
 1.  Scaling  ➢  Horizontal  Scaling:  Deploy  multiple  LLM  instances  across  servers  or  containers.  ➢  Autoscaling:  Use  Kubernetes  or  cloud-based  autoscaling  to  match  trafﬁc  demand.  ➢  Model  Sharding:  Distribute  large  models  across  multiple  GPUs  for  efﬁciency.  2.  Load  Balancing  ➢  Use  latency-aware  load  balancers  to  route  queries  to  the  least-busy  servers.  ➢  Implement  query-based  routing  (e.g.,  based  on  token  count

---

y-based  routing  (e.g.,  based  on  token  count  or  complexity).  ➢  Tools:  AWS  ALB,  GCP  Load  Balancing,  or  custom  Envoy/HAProxy  setups.  3.  Optimization  ➢  Model  Techniques:   ○  Quantization  (e.g.,  INT8/FP16)  to  boost  speed.   ○  Use  smaller  distilled  models  for  simpler  queries.  ➢  Batching:  Process  multiple  queries  in  one  forward  pass  for  better  GPU  utilization.  ➢  Caching:  Store  frequent  responses  using  Redis  or  Memcached.  4.  Cost-Efﬁcient  Sca

---

ing  Redis  or  Memcached.  4.  Cost-Efﬁcient  Scaling  ➢  Use  cloud  spot/preemptible  instances  for  cost  savings.  ➢  Combine  cloud  infrastructure  with  on-premise  systems  for  hybrid  scaling.  5.  Rapid  Response  ➢  Deploy  models  in  multiple  geographic  regions  to  reduce  latency.  ➢  Preload  and  keep  models  warm  to  prevent  cold  start  delays.  6.  Monitoring  ➢  Track  query  latency,  GPU  utilization,  and  throughput  with  tools  like  
Prometheus/Grafana.
 ➢  Us

---

ut  with  tools  like  
Prometheus/Grafana.
 ➢  Use  distributed  tracing  (e.g.,  Jaeger)  for  debugging  performance  bottlenecks.  
 
Q2.  How  would  you  incorporate  caching  mechanisms  into  an  LLM-based  
system
 
to
 
improve
 
performance
 
and
 
reduce
 
computational
 
costs?
 
What
 
kinds
 
of
 
information
 
would
 
be
 
best
 
suited
 
for
 
caching? Incorporating  caching  mechanisms  into  an  LLM-based  system  can  signiﬁcantly  improve  
performance
 
and
 
reduce
 
compu

---

ntly  improve  
performance
 
and
 
reduce
 
computational
 
costs:
  1.  Types  of  Information  to  Cache  ➢  Frequently  Asked  Questions  (FAQs):  Store  responses  for  common  queries.  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   159 
 
➢  Static  Prompts  and  Responses:  Predeﬁned  outputs  like  summaries  or  
template-based
 
answers.
 ➢  Intermediate  Computations:  Cache  embeddings  or  partially  processe

---

tions:  Cache  embeddings  or  partially  processed  results  for  
reusable
 
components
 
in
 
pipelines
 
(e.g.,
 
semantic
 
search).
 ➢  Short-Term  Popular  Queries:  Use  a  time-based  cache  for  trending  or  high-trafﬁc  
queries.
 2.  Caching  Strategies  ➢  Result  Caching:  Save  complete  responses  for  exact-match  queries.  ➢  Approximate  Matching:  Use  semantic  similarity  (e.g.,  cosine  similarity  of  
embeddings)
 
to
 
match
 
and
 
reuse
 
responses
 
for
 
similar

---

match
 
and
 
reuse
 
responses
 
for
 
similar
 
queries.
 ➢  Token-Level  Caching:  Store  output  token  probabilities  to  accelerate  next-token  
predictions
 
in
 
partial
 
queries.
 3.  Implementation  ➢  Use  in-memory  caches  like  Redis  or  Memcached  for  fast  retrieval.  ➢  TTL  (Time-to-Live):  Set  expiration  times  for  dynamic  data  to  maintain  relevance.  ➢  Cache  Invalidation:  Update  or  purge  stale  entries  when  underlying  data  changes.  4.  Workﬂow  Integrati

---

underlying  data  changes.  4.  Workﬂow  Integration  ➢  Check  the  cache  for  a  matching  query  or  embedding.  ○  If  found,  return  the  cached  response.  ○  If  not,  process  the  query  via  the  LLM  and  store  the  result  for  future  use.  
 
Q3.  How  would  you  reduce  model  size  and  optimize  for  deployment  on  
resource-constrained
 
devices
 
(e.g.,
 
smartphones)? To  optimize  LLMs  for  deployment  on  resource-constrained  devices  like  smartphones:  1.  Compress

---

trained  devices  like  smartphones:  1.  Compression:  ➢  Quantization:  Convert  weights  to  INT8/FP16.  ➢  Pruning:  Remove  redundant  weights  or  neurons.  ➢  Distillation:  Use  smaller,  distilled  models  (e.g.,  TinyBERT).   2.  Efﬁcient  Architectures:  ➢  Use  lightweight  models  like  MobileBERT  or  DistilBERT.  ➢  Share  weights  across  layers.  3.  On-Device  Frameworks:  ➢  Use  TensorFlow  Lite,  PyTorch  Mobile,  or  ONNX  Runtime  with  hardware  
acceleration.
 4.  Optimi

---

untime  with  hardware  
acceleration.
 4.  Optimization:  ➢  Reduce  sequence  length  and  implement  early  stopping  during  inference.  ➢  Batch  small  queries  dynamically.  ➢  Hybrid  Approach:  Ofﬂoad  complex  queries  to  the  cloud  if  needed.  
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   160 
 
Q4.  Discuss  the  trade-offs  of  using  GPUs  vs.  TPUs  vs.  other  specialized  
hardware
 
when
 
deploying

---

other  specialized  
hardware
 
when
 
deploying
 
large
 
language
 
models. GPUs:  for  ﬂexibility  ➔  Pros:  Versatile,  strong  software  support,  great  for  dynamic  workloads.  ➔  Cons:  High  power  consumption,  costly  for  large-scale  use.   TPUs:  for  large-scale  efﬁciency  ➔  Pros:  High  throughput,  energy-efﬁcient,  ideal  for  large-batch  training.  ➔  Cons:  Limited  ﬂexibility,  TensorFlow-centric,  cloud-dependent.   Specialized  Hardware  (FPGAs,  NPUs,  etc.):  for

---

ecialized  Hardware  (FPGAs,  NPUs,  etc.):  for  edge-speciﬁc  needs  ➔  Pros:  Task-speciﬁc  efﬁciency,  low  power  for  edge  devices.  ➔  Cons:  Custom  development  required,  less  general-purpose.  
 
Q5.  How  would  you  build  a  ChatGPT-like  system?  
To
 
build
 
a
 
ChatGPT-like
 
system:  ❖  Core  Components:  ➢  Language  Model:  Use  or  ﬁne-tune  a  pre-trained  model  (e.g.,  GPT,  LLaMA).  ➢  Frontend:  Web  or  app  interface  for  user  interaction.  ➢  Backend:  API  to

---

e  for  user  interaction.  ➢  Backend:  API  to  handle  requests  and  model  inference.  ➢  Database:  Store  conversation  history  and  user  data.   ❖  Model  Training:  ➢  Fine-tune  on  conversational  datasets.  ➢  Use  RLHF  for  better  response  quality.  ➢  Optimize  model  size  with  quantization  or  distillation.   ❖  Deployment:  ➢  Deploy  on  GPUs/TPUs  with  tools  like  TensorFlow  Serving  or  Hugging  Face  
Inference.
 ➢  Use  Kubernetes  and  cloud  platforms  (AWS/GCP)

---

Use  Kubernetes  and  cloud  platforms  (AWS/GCP)  for  scaling.   ❖  Enhancements:  ➢  Add  memory  via  embedding-based  retrieval  (e.g.,  FAISS).  ➢  Use  a  RAG  system  for  factual  answers.  ➢  Customize  responses  for  domain-speciﬁc  use  cases.   ❖  Monitoring:  ➢  Track  performance  with  tools  like  Prometheus  and  reﬁne  via  user  feedback.  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   161 
 
 
Q6.  S

---

rative  AI  Interview  Questions   161 
 
 
Q6.  System  design  an  LLM  for  code  generation  tasks.  Discuss  potential  
challenges. 
LLM
 
for
 
Code
 
Generation
 
System
 
Design  Core  Components:  ➢  Model:  Use  code-specialized  LLMs  (e.g.,  Codex,  CodeGen).  ➢  Frontend:  IDE  plugin  or  web  interface  for  seamless  interaction.  ➢  Backend:  API  for  code  generation  with  GPUs/TPUs.  ➢  Database:  Store  code,  prompts,  and  user  preferences.   Challenges  &  Mitigations:

---

user  preferences.   Challenges  &  Mitigations:  ➢  Quality:  Validate  with  unit  tests  to  ensure  correctness.  ➢  Bias/Security:  Filter  datasets  to  avoid  insecure  patterns.  ➢  Scalability:  Autoscale  and  cache  frequent  queries.  ➢  Multi-Language:  Use  diverse  datasets  and  ﬁne-tuning.  ➢  Context:  Use  RAG  for  larger  codebases.  ➢  Ethical/Legal:  Filter  out  direct  code  replication,  add  disclaimers.  ➢  Edge  Cases:  User  feedback  to  improve  rare  scenarios.

---

es:  User  feedback  to  improve  rare  scenarios.   
Q7.  Describe  an  approach  to  using  generative  AI  models  for  creating  
original
 
music
 
compositions.
 ➢  Idea  Generation:  Use  AI  models  like  ChatGPT  for  brainstorming  musical  themes,  
styles,
 
or
 
lyrics.
 ➢  Melody  Creation:  Employ  models  like  OpenAI's  MuseNet  or  Google's  Magenta  to  
generate
 
melodies
 
or
 
harmonies.
 ➢  Arrangement:  Utilize  AI  for  instrument  selection,  chord  progressions,  and

---

instrument  selection,  chord  progressions,  and  
orchestration
 
ideas.
 ➢  Iteration:  Collaborate  interactively  with  the  AI  by  reﬁning  outputs  and  combining  
human
 
creativity
 
with
 
AI
 
suggestions.
 ➢  Final  Touches:  Use  AI-assisted  tools  for  sound  design,  mixing,  or  mastering  to  polish  
the
 
composition.
 
 
Q8.  How  would  you  build  an  LLM-based  question-answering  system  for  a  
speciﬁc
 
domain
 
or
 
complex
 
dataset? Building  an  LLM-Based  Quest

---

complex
 
dataset? Building  an  LLM-Based  Question-Answering  System  for  a  Speciﬁc  Domain  ➢  Data  Collection:  Gather  domain-speciﬁc  datasets  (e.g.,  research  papers,  manuals,  or  
knowledge
 
bases).
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   162 
 
➢  Model  Selection:  Choose  a  pre-trained  LLM  (e.g.,  GPT,  BERT)  and  ﬁne-tune  it  on  the  
domain-speciﬁc
 
data.
 ➢  Document  Retrieval:  Imple

---

in-speciﬁc
 
data.
 ➢  Document  Retrieval:  Implement  a  Retrieval-Augmented  Generation  (RAG)  system,  
using
 
tools
 
like
 
FAISS
 
or
 
ElasticSearch
 
to
 
fetch
 
relevant
 
documents.
 ➢  Question  Processing:  Use  natural  language  processing  (NLP)  techniques  to  
understand
 
and
 
structure
 
the
 
user
 
query.
 ➢  Answer  Generation:  The  LLM  generates  an  answer  by  referencing  the  retrieved  
documents
 
or
 
knowledge
 
base.
 ➢  Evaluation:  Continuously  evaluate

---

ge
 
base.
 ➢  Evaluation:  Continuously  evaluate  and  ﬁne-tune  the  model  based  on  
domain-speciﬁc
 
metrics
 
(e.g.,
 
precision,
 
recall).
 ➢  Deployment:  Build  an  API  for  querying  the  model  and  deploy  with  scalability  in  mind  
(e.g.,
 
via
 
Kubernetes,
 
cloud
 
services).
 
 
Q9.  What  design  considerations  are  important  when  building  a  
multi-turn
 
conversational
 
AI
 
system
 
powered
 
by
 
an
 
LLM?
 
Key
 
considerations
 
for
 
multi-turn
 
conversation

---

considerations
 
for
 
multi-turn
 
conversational
 
AI
 
with
 
LLMs:
 
 ➢  Context  Management:  Efﬁciently  track  and  maintain  conversation  history.  ➢  Intent  Understanding:  Accurately  interpret  and  respond  to  user  inputs.  ➢  Personalization:  Adapt  responses  based  on  user  preferences.  ➢  Error  Handling:  Manage  ambiguity  with  clariﬁcations  or  alternatives.  ➢  Memory  Limits:  Condense  or  prioritize  context  to  ﬁt  token  limits.  ➢  Natural  Flow:  Ensure  sm

---

ﬁt  token  limits.  ➢  Natural  Flow:  Ensure  smooth  turn-taking  and  topic  transitions.  ➢  Safety:  Filter  harmful  or  biased  outputs.  ➢  Integration:  Connect  with  APIs  and  external  systems.  ➢  Response  Quality:  Ensure  diversity  and  align  tone  with  purpose.  ➢  Improvement:  Use  logs  and  feedback  for  ongoing  reﬁnement.  
 
Q10.  How  can  you  control  and  guide  the  creative  output  of  generative  
models
 
for
 
speciﬁc
 
styles
 
or
 
purposes?
 
To
 
contr

---

r
 
speciﬁc
 
styles
 
or
 
purposes?
 
To
 
control
 
and
 
guide
 
generative
 
models
 
for
 
speciﬁc
 
styles
 
or
 
purposes:
 ➢  Prompt  Engineering:  Design  detailed,  context-rich  prompts  to  steer  the  output  
toward
 
the
 
desired
 
style
 
or
 
purpose.
 ➢  Fine-Tuning:  Train  the  model  on  domain-speciﬁc  data  or  examples  to  align  its  
behavior
 
with
 
the
 
intended
 
output.
 ➢  Few-Shot  Learning:  Provide  examples  of  the  desired  style  or  purpose  within  th

---

of  the  desired  style  or  purpose  within  the  
prompt
 
to
 
guide
 
the
 
model.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   163 
 
➢  Constraints  and  Rules:  Apply  post-processing  or  incorporate  rules  to  ﬁlter  and  
reﬁne
 
outputs.
 ➢  Feedback  Loops:  Use  iterative  feedback  to  reﬁne  and  improve  outputs  over  multiple  
generations.
 ➢  Preprocessing  Inputs:  Frame  inputs  to  align  with

---

ocessing  Inputs:  Frame  inputs  to  align  with  the  context  or  characteristics  of  
the
 
desired
 
outcome.
 ➢  Style  Tokens  or  Tags:  Utilize  special  tokens  or  tags  if  supported  by  the  model  to  
signal
 
a
 
speciﬁc
 
tone,
 
style,
 
or
 
format.
 ➢  Hybrid  Approaches:  Combine  AI  outputs  with  human  editing  or  use  AI  tools  for  
speciﬁc
 
components
 
within
 
a
 
broader
 
creative
 
process.
 
 
Q11.  How  do  you  monitor  LLM  systems  once  productionized?

---

you  monitor  LLM  systems  once  productionized? 
To
 
monitor
 
LLM
 
systems
 
in
 
production: ➢  Performance  Metrics:  Track  key  metrics  like  response  latency,  accuracy,  relevance,  
and
 
token
 
usage.
 ➢  User  Feedback:  Collect  and  analyze  user  ratings,  comments,  and  complaints  to  
identify
 
improvement
 
areas.
 ➢  Content  Quality:  Regularly  review  outputs  for  coherence,  safety,  and  adherence  to  
guidelines.
 ➢  Error  Logging:  Log  interactions  with  e

---

s.
 ➢  Error  Logging:  Log  interactions  with  errors  or  failures,  including  out-of-scope  or  
ambiguous
 
responses.
 ➢  Model  Drift:  Monitor  for  performance  degradation  due  to  changing  user  behavior  or  
data
 
shifts.
 ➢  Safety  Filters:  Ensure  real-time  monitoring  for  harmful,  biased,  or  inappropriate  
outputs.
 ➢  Scalability  Metrics:  Monitor  system  load,  API  call  volumes,  and  infrastructure  
performance.
 ➢  A/B  Testing:  Compare  different  versions

---

.
 ➢  A/B  Testing:  Compare  different  versions  or  conﬁgurations  to  optimize  
performance.
 ➢  Usage  Analytics:  Analyze  patterns  in  user  queries  to  understand  trends  and  
emerging
 
needs.
 ➢  Automated  Alerts:  Set  up  alerts  for  anomalies  or  thresholds  being  breached  in  
metrics
 
like
 
response
 
times
 
or
 
error
 
rates.
 
 
    

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   164

---

nerative  AI  Interview  Questions   164 
 
            
Evaluation  Methods
             

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   165 
 
Q1.  What  are  some  common  evaluation  metrics  used  in  NLP,  and  how  do  
you
 
decide
 
which
 
one
 
to
 
use?
 
Common
 
NLP
 
Evaluation
 
Metrics: Accuracy:  Measures  correctness,  used  for  classiﬁcation  tasks  (e.g.,  sentiment  analysis).  Precision,  Recall,  F1

---

.,  sentiment  analysis).  Precision,  Recall,  F1-Score:  Evaluate  true  positives;  used  for  imbalanced  datasets  or  tasks  
like
 
entity
 
recognition.
 BLEU/ROUGE:  Compare  generated  text  with  references,  used  for  translation  or  
summarization.
 Perplexity:  Measures  model  uncertainty;  lower  is  better  for  language  modeling.  Exact  Match  (EM):  Checks  if  the  output  matches  the  reference  exactly,  used  in  question  
answering.
 Edit  Distance:  Measures  simil

---

ion  
answering.
 Edit  Distance:  Measures  similarity  by  counting  edit  operations;  used  for  spelling  
correction.
 
 
Choosing
 
Metrics: Task-Speciﬁc  Goals:  Match  metrics  to  the  task  (e.g.,  F1  for  imbalanced  data,  BLEU  for  
translation).
 Dataset  Characteristics:  Use  metrics  like  recall  for  incomplete  annotations  or  imbalanced  
data.
 User  Expectations:  Select  metrics  aligned  with  end-user  priorities,  such  as  ﬂuency  or  
accuracy.
 Interpretability:

---

uch  as  ﬂuency  or  
accuracy.
 Interpretability:  Favor  easily  interpretable  metrics  when  communicating  results  to  
stakeholders.
 
 
Q2.   How  do  you  approach  model  evaluation  differently  for  generative  AI  
tasks
 
like
 
text
 
generation
 
versus
 
classiﬁcation
 
tasks? Text  Generation:  Generative  tasks  prioritize  creativity  and  contextual  ﬁt.  ●  Use  qualitative  metrics  (e.g.,  ﬂuency,  coherence).  ●  Employ  BLEU ,  ROUGE ,  or  METEOR  for  comparing  outpu

---

BLEU ,  ROUGE ,  or  METEOR  for  comparing  outputs  to  references.  ●  Consider  human  evaluations  for  creativity  or  contextual  relevance.  ●  Measure  diversity  and  avoidance  of  repetition.  Classiﬁcation:  Classiﬁcation  focuses  on  discrete  correctness.  ●  Focus  on  quantitative  metrics  like  accuracy ,  precision ,  recall ,  and  F1-score.  ●  Handle  imbalanced  datasets  with  weighted  metrics.  ●  Evaluate  confusion  matrix  for  error  analysis.  
 
Q3.  What  is  t

---

matrix  for  error  analysis.  
 
Q3.  What  is  the  importance  of  human  evaluation  in  NLP,  especially  for  
generative
 
AI?
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   166 
 
Human
 
evaluation
 
is
 
crucial
 
in
 
NLP,
 
especially
 
for
 
generative
 
AI,
 
because: ➢  Subjective  Quality:  Measures  ﬂuency,  coherence,  relevance,  and  creativity—areas  
where
 
automated
 
metrics
 
fall
 
short.
 ➢  Co

---

where
 
automated
 
metrics
 
fall
 
short.
 ➢  Context  Understanding:  Humans  can  assess  outputs  in  nuanced  or  domain-speciﬁc  
contexts
 
that
 
models
 
may
 
not
 
fully
 
grasp.
 ➢  Ethical  and  Cultural  Sensitivity:  Detects  subtle  biases,  offensive  content,  or  
culturally
 
inappropriate
 
outputs.
 ➢  Usability  Insights:  Provides  feedback  on  user  satisfaction  and  practical  applicability  
of
 
generated
 
outputs.
 ➢  Benchmark  Validation:  Supplements  automate

---

➢  Benchmark  Validation:  Supplements  automated  metrics  to  ensure  alignment  with  
real-world
 
expectations.
 
 
Q4.  How  do  you  evaluate  models  for  bias  and  fairness,  especially  in  NLP  
tasks? 
To
 
evaluate
 
bias
 
and
 
fairness
 
in
 
NLP
 
models: ➢  Bias  Tests:  Use  datasets  designed  to  reveal  biases  (e.g.,  stereotype  prompts).  ➢  Demographic  Parity:  Check  performance  consistency  across  groups  (e.g.,  accuracy  
by
 
gender,
 
ethnicity).
 ➢  Counter

---

accuracy  
by
 
gender,
 
ethnicity).
 ➢  Counterfactual  Analysis:  Test  with  inputs  altered  for  sensitive  attributes  (e.g.,  
swapping
 
names
 
or
 
pronouns).
 ➢  Error  Analysis:  Identify  disproportionate  errors  for  speciﬁc  groups.  ➢  Human  Review:  Assess  outputs  for  subtle  biases  missed  by  automated  metrics.  
 
Q5.  What  is  perplexity,  and  why  is  it  used  to  evaluate  language  models?  Perplexity  is  a  metric  used  to  evaluate  language  models,  meas

---

etric  used  to  evaluate  language  models,  measuring  how  well  a  model  
predicts
 
a
 
sample
 
of
 
text.
 
It
 
is
 
deﬁned
 
as
 
the
 
exponentiation
 
of
 
the
 
average
 
negative
 
log-likelihood
 
of
 
the
 
predicted
 
probabilities
 
for
 
the
 
true
 
words
 
in
 
the
 
text.
 
Formula:
       Where   is  the  model's  predicted  probability  for  the  -th  word.  
 
Why
 
it's
 
used:
   ➢  Lower  perplexity  indicates  better  performance,  meaning  the  model  assigns  highe

---

performance,  meaning  the  model  assigns  higher  
probabilities
 
to
 
the
 
actual
 
words
 
in
 
the
 
text.
 ➢  It  reﬂects  how  "conﬁdent"  and  accurate  the  model  is  in  its  predictions .  ➢  It  is  particularly  useful  for  comparing  different  language  models  or  tuning  them.  
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   167 
 
Q6.  How  do  you  evaluate  the  coherence  and  relevance  of  text

---

evaluate  the  coherence  and  relevance  of  text  generated  
by
 
an
 
NLP
 
model? 
To
 
evaluate
 
the
 
coherence
 
and
 
relevance
 
of
 
text
 
generated
 
by
 
an
 
NLP
 
model,
 
you
 
can: ➢  Human  Evaluation:  Manually  assess  the  generated  text  for  logical  ﬂow,  clarity,  and  
alignment
 
with
 
the
 
context
 
or
 
prompt.
 ➢  Automated  Metrics:  ○  Perplexity:  Measures  how  well  the  model  predicts  the  next  word,  reﬂecting  
ﬂuency.
 ○  BLEU/ROUGE:  Compare  gener

---

eﬂecting  
ﬂuency.
 ○  BLEU/ROUGE:  Compare  generated  text  to  reference  text  for  similarity  
(useful
 
for
 
tasks
 
with
 
ground
 
truth).
 ○  BERTScore:  Evaluates  semantic  similarity  using  embeddings.  ○  Task-Speciﬁc  Evaluation:  Check  if  the  output  satisﬁes  the  speciﬁc  task  
requirements
 
(e.g.,
 
accuracy
 
for
 
summarization
 
or
 
QA).
 ➢  Adversarial  Testing:  Input  challenging  or  edge-case  prompts  to  see  how  robustly  
the
 
model
 
generates
 
coherent

---

how  robustly  
the
 
model
 
generates
 
coherent
 
and
 
relevant
 
responses.
 ➢  User  Feedback:  Collect  input  from  end  users  about  how  useful  and  appropriate  the  
outputs
 
are.
  
Q7.  Discuss  metrics  like  BLEU,  METEOR,  and  human  evaluation  for  
coherence
 
and
 
relevance,
 
particularly
 
in
 
conversational
 
AI
 
or
 
creative
 
text
 
generation.
 
Metrics
 
for
 
Conversational
 
AI
 
and
 
Creative
 
Text
 
Generation BLEU  (Bilingual  Evaluation  Understudy)  ➢

---

ation BLEU  (Bilingual  Evaluation  Understudy)  ➢  Measures  overlap  between  generated  text  and  reference  text  using  n-grams.  ➢  Strengths:  Fast,  objective,  and  works  well  for  structured  tasks  like  translation.  ➢  Limitations:  Penalizes  creative  or  diverse  outputs;  insensitive  to  semantics  or  
context
 
relevance.
  METEOR  (Metric  for  Evaluation  of  Translation  with  Explicit  ORdering)  ➢  Considers  precision,  recall,  and  word  alignment,  including  syno

---

,  recall,  and  word  alignment,  including  synonyms  and  stemming.  ➢  Strengths:  Better  semantic  understanding  than  BLEU.  ➢  Limitations:  Still  focuses  on  reference  matching,  limiting  adaptability  for  creative  
tasks.
  Human  Evaluation  ➢  Assesses  coherence,  relevance,  creativity,  and  ﬂuency  directly  through  human  
judgments.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   168 
 
➢  Strengt

---

tive  AI  Interview  Questions   168 
 
➢  Strengths:  Captures  nuanced  aspects  of  language  like  tone  and  style.  ➢  Limitations:  Expensive,  subjective,  and  time-consuming.   Relevance  to  Conversational  AI  and  Creative  Text  ❖  BLEU  and  METEOR  are  suboptimal  for  evaluating  open-ended,  diverse  
conversations.
 ❖  Human  evaluation  remains  the  gold  standard  but  is  complemented  by  task-speciﬁc  
metrics
 
like:
 ➢  Coherence:  Logical  ﬂow  between  conversation

---

Coherence:  Logical  ﬂow  between  conversation  turns.  ➢  Relevance:  Alignment  with  context  and  user  intent.  ➢  Engagement:  Creativity  and  ability  to  sustain  user  interest  (best  judged  by  
humans
 
or
 
learning-based
 
models).
 
 
Q8.  What  methods  can  be  used  to  assess  the  diversity  of  generated  text?  
Methods
 
to
 
Assess
 
Text
 
Diversity: ❖  Lexical  Diversity:  ➢  Type-Token  Ratio  (TTR),  MATTR,  Yule’s  K.  ❖  N-Gram  Diversity:  ➢  Unique  n-grams,

---

s  K.  ❖  N-Gram  Diversity:  ➢  Unique  n-grams,  repetition  rates.  ❖  Embedding-Based:  ➢  Cosine  similarity,  pairwise  distances.  ❖  Entropy:  ➢  Shannon  entropy,  perplexity  variance.  ❖  Novelty:  ➢  Out-of-training  vocabulary  rate,  paraphrase  detection.  ❖  Coverage:  ➢  Semantic  concepts,  topic  modeling.  ❖  Inter/Intra-Sample:  ➢  Self-BLEU,  Jaccard  similarity.  ❖  Human  Evaluation:  ➢  Subjective  assessments.  
 
Q9.  What  role  does  prompt  engineering  play  in  ev

---

hat  role  does  prompt  engineering  play  in  evaluation,  especially  for  
models
 
like
 
GPT? 
Prompt
 
engineering
 
is
 
crucial
 
for
 
evaluating
 
models
 
like
 
GPT
 
as
 
it: ➢  Controls  Outputs:  Determines  the  quality,  style,  and  speciﬁcity  of  generated  text.  ➢  Uncovers  Biases:  Reveals  model  limitations  or  biases  by  varying  prompts.  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   169

---

Generative  AI  Interview  Questions   169 
 
➢  Stress-Tests  Capabilities:  Evaluates  robustness  across  scenarios  (e.g.,  ambiguous  or  
adversarial
 
prompts).
 ➢  Enhances  Metrics:  Helps  align  outputs  with  evaluation  criteria  like  relevance,  
coherence,
 
and
 
diversity.
  
Q10.  What  are  ROUGE  scores,  and  why  are  they  commonly  used  for  
summarization? ROUGE  (Recall-Oriented  Understudy  for  Gisting  Evaluation)  scores  are  metrics  used  to  
evaluate
 
th

---

n)  scores  are  metrics  used  to  
evaluate
 
the
 
quality
 
of
 
generated
 
text,
 
particularly
 
for
 
summarization
 
tasks.
  
Key
 
Metrics
 
in
 
ROUGE:
 ➢  ROUGE-N:  Measures  overlap  of  n-grams  (e.g.,  unigrams,  bigrams)  between  the  
generated
 
and
 
reference
 
summaries.
 ➢  ROUGE-L:  Evaluates  the  longest  common  subsequence  (LCS)  between  the  
generated
 
and
 
reference
 
text,
 
emphasizing
 
sequence
 
preservation.
 ➢  ROUGE-S:  Measures  skip-bigram  overlap ,

---

on.
 ➢  ROUGE-S:  Measures  skip-bigram  overlap ,  capturing  non-consecutive  word  pairs.   
Why
 
Commonly
 
Used
 
for
 
Summarization? ➢  Focus  on  Content  Matching:  Reﬂects  how  much  key  information  from  the  reference  
summary
 
is
 
retained.
 ➢  Simple  and  Effective:  Easy  to  compute  and  correlates  reasonably  with  human  
judgments
 
for
 
informativeness.
 ➢  Versatile:  Works  well  across  extractive  and  abstractive  summarization  methods.   However,  ROUGE  has

---

ve  summarization  methods.   However,  ROUGE  has  limitations,  like  ignoring  semantic  equivalence  or  context,  making  
complementary
 
m
etrics
 
or
 
human
 
evaluation
 
necessary.
  
Q11.  Explain  the  ROUGE  metric  and  its  variants  (ROUGE-N,  ROUGE-L)  as  
measures
 
of
 
overlap
 
between
 
model-generated
 
summaries
 
and
 
reference
 
summaries.
 ROUGE  (Recall-Oriented  Understudy  for  Gisting  Evaluation)  measures  the  overlap  
between
 
model-generated
 
and
 
refer

---

overlap  
between
 
model-generated
 
and
 
reference
 
summaries
 
based
 
on
 
n-grams,
 
sequences,
 
and
 
word
 
matches.
  
  
Variants:
   ROUGE-N:  Measures  n-gram  overlap  (e.g.,  unigrams  for  word  match,  bigrams  for  phrase  
match).
  
 Formula:  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   170 
 
 .     ROUGE-L:  Measures  the  Longest  Common  Subsequence  (LCS),  focusing  on  sentence  
structure

---

ence  (LCS),  focusing  on  sentence  
structure
 
and
 
order.
  
 ➢  Consider  both  precision  and  recall  for  LCS  matches.     These  metrics  emphasize  recall,  showing  how  much  of  the  reference  summary  is  captured  
by
 
the
 
model's
 
output.
 
 
Q12.  How  would  you  assess  the  informativeness  and  conciseness  of  a  
summarization
 
model? 
To
 
assess
 
informativeness
 
and
 
conciseness
 
in
 
summarization:  Informativeness:  ➢  ROUGE:  Measures  overlap  of  key

---

tiveness:  ➢  ROUGE:  Measures  overlap  of  key  phrases/words  with  the  reference.  ➢  Precision  vs.  Recall:  High  recall  ensures  coverage  of  important  details.  ➢  Human  Evaluation:  Rate  coverage  of  critical  content.  Conciseness:  ➢  Length  Ratio:  Compare  summary  length  to  input  length.  ➢  Compression  Rate:  Evaluate  how  effectively  the  summary  condenses  content.  ➢  Human  Evaluation:  Assess  for  redundancy  and  unnecessary  details.   
Q13.  How  do  you

---

and  unnecessary  details.   
Q13.  How  do  you  evaluate  retrieval  quality  in  RAG  models,  and  why  is  it  
important?
 
Evaluating
 
Retrieval
 
Quality
 
in
 
RAG
 
Models:
 Metrics:  ➢  Recall@k:  Proportion  of  relevant  documents  in  the  top-k  retrieved.  ➢  Precision@k:  Proportion  of  retrieved  documents  that  are  relevant.  ➢  Mean  Reciprocal  Rank  (MRR):  Average  rank  of  the  ﬁrst  relevant  document.  ➢  Normalized  Discounted  Cumulative  Gain  (nDCG):  Rewards

---

Discounted  Cumulative  Gain  (nDCG):  Rewards  ranking  relevant  
documents
 
higher.
 ➢  Human  Evaluation:  Assess  document  relevance  to  the  query.   Importance:  ➢  Ensures  accurate,  relevant  context  for  generation.  ➢  Directly  impacts  response  coherence,  factuality,  and  usefulness.  ➢  Mitigates  propagation  of  errors  from  irrelevant  or  low-quality  retrieval.   

MD  ARBAAZ  KHAN                                                                Generative  AI  Interv

---

Generative  AI  Interview  Questions   171 
 
Q14.  What  strategies  do  you  use  to  reduce  hallucination  in  RAG  models? 
To
 
reduce
 
hallucination
 
in
 
RAG
 
models: ❖  Improve  Retrieval:  ➢  Use  high-quality,  domain-speciﬁc  documents.  ➢  Optimize  retriever  models  (e.g.,  BM25,  dense  embeddings).   ❖  Model  Alignment:  ➢  Fine-tune  on  retrieval-grounded  datasets.  ➢  Penalize  hallucinated  outputs  during  training  (contrastive  learning).

---

tputs  during  training  (contrastive  learning).   ❖  Response  Constraints:  ➢  Encourage  outputs  strictly  grounded  in  retrieved  context.  ➢  Use  techniques  like  response  validation  or  fact-checking  against  retrieved  
sources.
  ❖  Enhanced  Retrieval-Augmentation:  ➢  Combine  multiple  retrievals  or  rank  sources  by  relevance.  ➢  Apply  ensemble  methods  to  improve  accuracy.   ❖  Dynamic  Prompting:  ➢  Explicitly  instruct  models  to  rely  only  on  provided  eviden

---

ruct  models  to  rely  only  on  provided  evidence.   
Q15.  How  do  you  determine  if  ﬁne-tuning  has  improved  a  model’s  
performance
 
on
 
a
 
speciﬁc
 
task?
 
To
 
determine
 
if
 
ﬁne-tuning
 
has
 
improved
 
a
 
model's
 
performance
 
on
 
a
 
speciﬁc
 
task: ➢  Evaluate  Task  Metrics:  Compare  pre-  and  post-ﬁne-tuning  scores  on  task-speciﬁc  
metrics
 
(e.g.,
 
accuracy,
 
F1-score,
 
BLEU).
 ➢  Validation  Set  Performance:  Check  improvement  on  a  held-out  validat

---

nce:  Check  improvement  on  a  held-out  validation  set  
relevant
 
to
 
the
 
task.
 ➢  Generalization:  Test  on  unseen  data  or  a  test  set  to  ensure  consistent  performance.  ➢  Error  Analysis:  Analyze  reductions  in  errors,  biases,  or  undesired  behaviors.  ➢  Human  Evaluation:  Use  human  judgments  to  assess  qualitative  improvements  (e.g.,  
relevance,
 
coherence).
 ➢  A/B  Testing:  Compare  outputs  of  ﬁne-tuned  and  baseline  models  in  real-world  
scenario

---

and  baseline  models  in  real-world  
scenarios.
  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   172 
 
Q16.  Discuss  comparing  baseline  metrics  with  ﬁne-tuned  metrics,  
tracking
 
loss
 
curves,
 
and
 
using
 
task-speciﬁc
 
metrics
 
to
 
measure
 
improvement. 
Comparing
 
Baseline
 
Metrics
 
with
 
Fine-Tuned
 
Metrics: Purpose:  Establish  a  performance  reference  point  (baseline)  for  comparison  wi

---

reference  point  (baseline)  for  comparison  with  
ﬁne-tuned
 
models.
 Steps:  ➢  Baseline  Metrics:  Evaluate  pre-trained  models  on  standard  metrics  (e.g.,  accuracy,  
BLEU,
 
F1).
 ➢  Fine-Tuned  Metrics:  Compare  post-ﬁne-tuning  metrics  against  the  baseline.  Key  Points:  ➢  Improvement  validates  task-speciﬁc  ﬁne-tuning.  ➢  Highlight  trade-offs  (e.g.,  slight  loss  in  generalization  for  task  specialization).  
 
Tracking
 
Loss
 
Curves:
 Purpose:  Monitor  model

---

acking
 
Loss
 
Curves:
 Purpose:  Monitor  model  convergence  and  detect  issues  (e.g.,  under/overﬁtting).  Strategies:  ➢  Plot  training  loss  vs.  validation  loss  over  epochs.  ➢  Ensure  a  smooth  decrease  in  training  loss  and  stabilization  of  validation  loss.  Insights:  ➢  Diverging  curves  indicate  overﬁtting.  ➢  Consistently  high  loss  suggests  underﬁtting  or  poor  learning  rate.   
Using
 
Task-Speciﬁc
 
Metrics:
 Purpose:  Evaluate  improvement  on  domain-sp

---

s:
 Purpose:  Evaluate  improvement  on  domain-speciﬁc  goals.  Examples:  ➢  Classiﬁcation:  Precision,  recall,  F1.  ➢  Generation:  BLEU,  ROUGE,  METEOR.  ➢  Retrieval-Augmented  Tasks:  Precision@K,  Exact  Match  (EM).  Key  Points:  ➢  Tailor  metrics  to  task  objectives.  ➢  Use  multiple  metrics  for  holistic  evaluation  (e.g.,  ﬂuency  +  factual  accuracy  for  RAG  
models).
  
Q17.  What  challenges  arise  when  ﬁne-tuning  large  language  models,  and  
how
 
do
 
you
 
mi

---

arge  language  models,  and  
how
 
do
 
you
 
mitigate
 
them? 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   173 
 
Fine-tuning
 
large
 
language
 
models
 
(LLMs)
 
involves
 
several
 
challenges: ➢  Computational  Costs:  Large  models  require  signiﬁcant  computational  resources.  ○  Mitigation:  Use  parameter-efﬁcient  techniques  like  LoRA,  adapters,  or  
prompt
 
tuning.
 ➢  Overﬁtting:  Overﬁtting  to  th

---

ompt
 
tuning.
 ➢  Overﬁtting:  Overﬁtting  to  the  ﬁne-tuning  dataset  can  degrade  generalization.  ○  Mitigation:  Use  regularization,  early  stopping,  or  a  diverse  ﬁne-tuning  
dataset.
 ➢  Catastrophic  Forgetting:  The  model  might  lose  knowledge  from  pretraining.  ○  Mitigation:  Use  techniques  like  continual  learning  or  regularization  (e.g.,  
Elastic
 
Weight
 
Consolidation).
 ➢  Data  Quality  Issues:  Fine-tuning  on  biased  or  low-quality  data  can  lead  to

---

on  biased  or  low-quality  data  can  lead  to  poor  
model
 
performance.
 ○  Mitigation:  Carefully  curate  datasets  and  apply  bias  detection/removal  
techniques.
 ➢  Hyperparameter  Sensitivity:  Fine-tuning  can  be  sensitive  to  learning  rate  and  
batch
 
size.
 ○  Mitigation:  Conduct  systematic  hyperparameter  tuning.  ➢  Scalability:  Fine-tuning  large  models  for  multiple  tasks  can  be  impractical.  ○  Mitigation:  Opt  for  multi-task  or  instruction-based  ﬁne-

---

Opt  for  multi-task  or  instruction-based  ﬁne-tuning  to  generalize  
across
 
tasks.
 ➢  Evaluation  Challenges:  Measuring  ﬁne-tuning  success  is  difﬁcult  without  
task-speciﬁc
 
metrics.
 ○  Mitigation:  Deﬁne  clear  evaluation  criteria  aligned  with  end-user  needs.   
Q18.  Talk  about  overﬁtting,  the  need  for  robust  validation  datasets,  and  
regularization
 
techniques
 
that
 
ensure
 
generalizability
 
in
 
ﬁne-tuned
 
models. 
Overﬁtting occurs  when  a  model  l

---

ed
 
models. 
Overﬁtting occurs  when  a  model  learns  patterns  speciﬁc  to  the  training  dataset,  
including
 
noise,
 
rather
 
than
 
generalizable
 
patterns.
 
This
 
results
 
in
 
excellent
 
performance
 
on
 
the
 
training
 
data
 
but
 
poor
 
performance
 
on
 
unseen
 
data.
  
Need
 
for
 
Robust
 
Validation
 
Datasets Robust  validation  datasets  are  critical  to  evaluate  a  model's  ability  to  generalize.  A  diverse  
and
 
representative
 
validation
 
set
 
helps:

---

and
 
representative
 
validation
 
set
 
helps:
 ➢  Detect  overﬁtting  early.  ➢  Ensure  performance  metrics  reﬂect  real-world  applicability.  ➢  Guide  hyperparameter  tuning  by  acting  as  a  reliable  proxy  for  unseen  data.   

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   174 
 
Regularization
 
Techniques
 
to
 
Ensure
 
Generalizability ❖  L1/L2  Regularization:  ➢  Penalize  large  weights  to  encourag

---

zation:  ➢  Penalize  large  weights  to  encourage  simpler  models  (L1  promotes  sparsity;  L2  
prevents
 
large
 
weights).
 ❖  Dropout:  ➢  Randomly  deactivate  a  fraction  of  neurons  during  training  to  reduce  
dependency
 
on
 
speciﬁc
 
neurons.
 ❖  Data  Augmentation:  ➢  Introduce  variability  in  the  training  data  (e.g.,  image  rotation,  ﬂipping)  to  
make
 
models
 
robust.
 ❖  Early  Stopping:  ➢  Monitor  validation  performance  and  halt  training  when  it  stops

---

performance  and  halt  training  when  it  stops  improving  to  
avoid
 
overﬁtting.
 ❖  Batch  Normalization:  ➢  Normalize  intermediate  activations  to  stabilize  and  regularize  training.  ❖  Cross-Validation:  ➢  Use  k-fold  cross-validation  to  validate  the  model  on  multiple  data  splits  for  
reliable
 
performance
 
assessment.
 ❖  Pruning:  ➢  Reduce  model  complexity  by  eliminating  redundant  parameters  or  neurons.   
Q19.  How  do  you  assess  the  quality  of  ge

---

Q19.  How  do  you  assess  the  quality  of  generated  samples  from  a  
generative
 
model?
 Assessing  the  quality  of  samples  from  a  generative  model  involves  several  evaluation  
techniques,
 
categorized
 
into
 
quantitative,
 
qualitative,
 
and
 
task-speciﬁc
 
measures:
  1.  Quantitative  Evaluation  ➢  Inception  Score  (IS):  Measures  both  diversity  and  quality  by  evaluating  the  output  of  
a
 
pre-trained
 
classiﬁer.
 
High
 
scores
 
indicate
 
sharp,
 
dive

---

ssiﬁer.
 
High
 
scores
 
indicate
 
sharp,
 
diverse
 
samples.
 ➢  Fréchet  Inception  Distance  (FID):  Compares  the  feature  distribution  of  generated  
samples
 
to
 
real
 
ones
 
using
 
the
 
mean
 
and
 
covariance
 
of
 
embeddings.
 
Lower
 
FID
 
indicates
 
closer
 
alignment
 
to
 
real
 
data.
 ➢  Perceptual  Path  Length  (PPL):  Assesses  smoothness  of  the  latent  space  by  
measuring
 
changes
 
in
 
generated
 
images
 
as
 
latent
 
vectors
 
are
 
interpolated.
 
Low

---

as
 
latent
 
vectors
 
are
 
interpolated.
 
Lower
 
PPL
 
suggests
 
better
 
structure.
 ➢  Kernel  Inception  Distance  (KID):  Similar  to  FID  but  uses  a  kernel-based  approach  
for
 
more
 
reliable
 
small-sample
 
estimates.
 ➢  Precision  and  Recall  for  Generative  Models:  Measures  the  ﬁdelity  (precision)  and  
diversity
 
(recall)
 
of
 
generated
 
samples.
 2.  Qualitative  Evaluation  

MD  ARBAAZ  KHAN                                                                Ge

---

Generative  AI  Interview  Questions   175 
 
➢  Visual  Inspection:  Experts  or  general  users  examine  samples  for  realism  and  
diversity.
 ➢  Human  Evaluation:  Asking  users  to  rate  or  select  the  better  samples  in  A/B  tests.  ➢  Latent  Space  Traversal:  Examining  interpolation  and  variation  in  latent  space  for  
smoothness
 
and
 
coherence.
 3.  Task-Speciﬁc  Evaluation  ➢  Downstream  Task  Performance:  Using  gene

---

on  ➢  Downstream  Task  Performance:  Using  generated  data  to  train  or  augment  models  
for
 
tasks
 
like
 
classiﬁcation,
 
and
 
comparing
 
results.
 ➢  Consistency  Metrics:  For  domain-speciﬁc  tasks,  such  as  comparing  synthesized  and  
real
 
data
 
consistency
 
(e.g.,
 
in
 
speech,
 
comparing
 
spectrograms).
 4.  Adversarial  and  Robustness  Testing  ➢  Testing  how  the  model  responds  to  challenging  or  extreme  inputs,  ensuring  
generalization
 
and
 
robustne

---

nputs,  ensuring  
generalization
 
and
 
robustness.
 5.  Novelty  and  Coverage  ➢  Uniqueness  Metrics:  Ensuring  generated  samples  aren't  memorized  from  training  
data
 
(e.g.,
 
using
 
nearest
 
neighbor
 
search
 
or
 
log-likelihood
 
estimation).
 ➢  Mode  Coverage:  Evaluating  whether  the  model  captures  all  modes  of  the  data  
distribution
 
(avoiding
 
mode
 
collapse).
  
Q20.  How  would  you  set  up  an  A/B  test  to  evaluate  two  NLP  models? 
To
 
summarize

---

to  evaluate  two  NLP  models? 
To
 
summarize
 
the
 
process: ➢  Data  Splitting:  Divide  incoming  user  data  into  two  groups  (e.g.,  50%  for  each  
model).
 ➢  Model  Evaluation:  Route  one  group  through  Model  A  and  the  other  through  Model  
B.
 ➢  Metrics  Collection:  Compare  results  using  predeﬁned  evaluation  metrics  like  
accuracy,
 
F1
 
score,
 
or
 
user
 
satisfaction.
 ➢  Analysis:  Analyze  which  model  performs  better  overall  or  in  speciﬁc  contexts.

---

forms  better  overall  or  in  speciﬁc  contexts.   
Q21.  Describe  the  importance  of  testing  with  a  live  audience,  creating  
control/experimental
 
groups,
 
and
 
using
 
click-through
 
rates
 
or
 
engagement
 
metrics
 
in
 
addition
 
to
 
core
 
NLP
 
metrics. Testing  with  a  live  audience  allows  you  to  evaluate  how  your  NLP  
model
 
performs
 
in
 
real-world
 
scenarios,
 
uncovering
 
issues
 
that
 
may
 
not
 
surface
 
in
 
isolated
 
environments.
  Using  con

---

rface
 
in
 
isolated
 
environments.
  Using  control/experimental  groups  helps  isolate  the  impact  of  
changes,
 
ensuring
 
measurable
 
results
 
by
 
comparing
 
the
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   176 
 
performance  of  the  new  model  (experimental)  against  a  baseline  (control).   Click-through  rates  (CTR)  and  engagement  metrics  provide  insight  into  user  behavior  and  
satisfac

---

ide  insight  into  user  behavior  and  
satisfaction,
 
offering
 
a
 
broader
 
evaluation
 
of
 
effectiveness
 
beyond
 
core
 
NLP
 
metrics
 
like
 
accuracy
 
or
 
F1
 
score,
 
which
 
may
 
not
 
fully
 
capture
 
the
 
model's
 
practical
 
value.
 
Together,
 
these
 
approaches
 
ensure
 
a
 
data-driven
 
reﬁnement
 
process
 
aligned
 
with
 
user
 
expectations.
  
Q22.  How  do  latency  and  efﬁciency  factor  into  evaluating  NLP  models,  
especially
 
in
 
production
 
sett

---

NLP  models,  
especially
 
in
 
production
 
settings?
 
Latency:
 
 Directly  affects  user  experience.  Lower  latency  ensures  faster  responses,  crucial  for  
real-time
 
applications
 
like
 
chatbots
 
or
 
search
 
engines.
  
Eﬃciency:  Determines  resource  usage  (CPU,  GPU,  memory)  and  scalability.  Efﬁcient  models  reduce  
operational
 
costs,
 
support
 
higher
 
throughput,
 
and
 
are
 
essential
 
for
 
deployment
 
on
 
edge
 
devices
 
or
 
in
 
resource-constrained

---

edge
 
devices
 
or
 
in
 
resource-constrained
 
environments.
  
Q23.  What’s  the  role  of  explainability  in  NLP  evaluation,  especially  for  
high-stakes
 
applications? Explainability  in  NLP  evaluation  is  crucial  for  high-stakes  applications  like  healthcare,  
ﬁnance,
 
or
 
law
 
because
 
it
 
ensures
 
transparency,
 
trust,
 
and
 
accountability.
 
It
 
helps
 
stakeholders
 
understand
 
how
 
decisions
 
are
 
made,
 
identify
 
biases
 
or
 
errors,
 
and
 
validate

---

identify
 
biases
 
or
 
errors,
 
and
 
validate
 
that
 
models
 
align
 
with
 
ethical
 
and
 
regulatory
 
standards.
 
This
 
is
 
critical
 
for
 
fostering
 
user
 
trust
 
and
 
minimizing
 
risks
 
associated
 
with
 
incorrect
 
or
 
opaque
 
model
 
outputs.
  
Q24.  How  do  you  measure  user  satisfaction  with  an  NLP  model  deployed  
in
 
a
 
real-world
 
application? User  satisfaction  with  an  NLP  model  in  a  real-world  application  can  be  measured  using:  ➢  User

---

d  application  can  be  measured  using:  ➢  User  Feedback:  Collect  ratings,  reviews,  or  direct  feedback  from  users.  ➢  Engagement  Metrics:  Track  interaction  rates,  session  durations,  and  return  usage.  ➢  Task  Success  Rate:  Measure  how  often  users  achieve  their  goals  with  the  model.  ➢  Error  Rates:  Monitor  issues  like  incorrect  responses,  misunderstanding,  or  
misclassiﬁcation.
 ➢  Customer  Support:  Analyze  complaints,  tickets,  or  escalation  tren

---

alyze  complaints,  tickets,  or  escalation  trends.  ➢  Surveys:  Deploy  targeted  surveys  (e.g.,  CSAT  or  NPS)  to  assess  satisfaction.  ➢  A/B  Testing:  Compare  performance  against  alternatives  or  updates.  ➢  Retention  Rates:  Evaluate  if  users  continue  using  the  application  over  time.  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   177 
 
Q25.  What  is  domain  adaptation,  and  how  do  you  ev

---

at  is  domain  adaptation,  and  how  do  you  evaluate  it  after  
ﬁne-tuning
 
a
 
model
 
on
 
domain-speciﬁc
 
data? Domain  adaptation  refers  to  the  process  of  adapting  a  machine  learning  model  trained  on  
one
 
domain
 
(source)
 
to
 
perform
 
well
 
on
 
a
 
different,
 
but
 
related
 
domain
 
(target).
 
It
 
helps
 
in
 
addressing
 
domain
 
shift,
 
where
 
data
 
distributions
 
differ
 
between
 
source
 
and
 
target
 
domains.
  
Evaluation
 
After
 
Fine-Tuning

---

get
 
domains.
  
Evaluation
 
After
 
Fine-Tuning: In-Domain  Evaluation  ➢  Test  the  model  on  a  labeled  dataset  speciﬁc  to  the  target  domain.  ➢  Metrics:  Use  task-relevant  metrics  like  accuracy,  F1-score,  BLEU,  etc.  Comparison  with  Baselines  ➢  Compare  the  ﬁne-tuned  model's  performance  with:  ○  Pre-trained  model  (without  ﬁne-tuning).  ○  Other  domain-speciﬁc  models  or  state-of-the-art  results.  Out-of-Domain  Generalization  ➢  Evaluate  how  well  the  mo

---

n  Generalization  ➢  Evaluate  how  well  the  model  retains  its  performance  on  the  source  domain.  Robustness  Checks  ➢  Test  the  model  on  unseen  domain  data  to  ensure  generalization.   
Q26.  How  would  you  evaluate  the  robustness  of  an  NLP  model  to  
adversarial
 
attacks?
 Adversarial  Example  Testing:  Generate  adversarial  examples  using  techniques  like  
synonym
 
replacement,
 
paraphrasing,
 
or
 
character-level
 
noise,
 
and
 
evaluate
 
model
 
perfor

---

r-level
 
noise,
 
and
 
evaluate
 
model
 
performance.
 Attack  Benchmarks:  Use  tools  like  TextAttack  or  OpenAttack  to  simulate  attacks  
(e.g.,
 
word
 
swaps,
 
sentence
 
paraphrases).
 Robustness  Metrics:  Measure  accuracy  drop,  attack  success  rate,  or  perturbation  
size
 
to
 
evaluate
 
resilience.
 Generalization  Tests:  Test  on  diverse  and  unseen  datasets  to  check  performance  
under
 
distributional
 
shifts.
 Interpretability  Analysis:  Analyze  model  att

---

Interpretability  Analysis:  Analyze  model  attention  or  gradients  to  understand  
vulnerability
 
patterns.
 Human  Evaluation:  Validate  adversarial  outputs  to  ensure  they  maintain  semantic  
and
 
syntactic
 
coherence.
    

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   178 
 
                   
Miscellaneous  Questions                   

MD  ARBAAZ  KHAN

---

Generative  AI  Interview  Questions   179 
 
Q1.  What  ethical  considerations  are  crucial  when  deploying  generative  
models,
 
and
 
how
 
do
 
you
 
address
 
them? 
Key
 
Ethical
 
Considerations
 
in
 
Deploying
 
Generative
 
Models Bias  and  Fairness  ➢  Generative  models  can  amplify  existing  biases  in  training  data.  ➢  Address:  Use  diverse  datasets,  audit  outputs,  and  ﬁne-tune  with  fairness  metrics

---

outputs,  and  ﬁne-tune  with  fairness  metrics.  Misinformation  ➢  Models  can  generate  realistic  but  false  information.  ➢  Address:  Implement  watermarking,  fact-checking  systems,  and  disclaimers.  Privacy  ➢  Models  may  inadvertently  reveal  sensitive  information.  ➢  Address:  Remove  private  data  during  training  and  conduct  differential  privacy  
audits.
 Misuse  ➢  Potential  for  harmful  applications  (e.g.,  deep  fakes,  fraud).  ➢  Address:  Use  controlled

---

p  fakes,  fraud).  ➢  Address:  Use  controlled  API  access,  enforce  usage  policies,  and  monitor  misuse.  Environmental  Impact  ➢  Training  and  inference  have  signiﬁcant  energy  demands.  ➢  Address:  Optimize  model  efﬁciency,  use  green  energy,  and  encourage  smaller  model  
deployment.
 Accountability  ➢  Ambiguity  in  responsibility  for  generated  content.  
➢
 
Address:
 
Clearly
 
d
eﬁne  accountability  frameworks  and  establish  user  
guidelines.
  
Q2.  Can  you

---

nd  establish  user  
guidelines.
  
Q2.  Can  you  describe  a  challenging  project  involving  generative  models  
that
 
you've
 
tackled I  recently  worked  on  a  challenging  project  that  integrated  a  RAG-based  chatbot  using  
LangChain,
 
Cohere,
 
and
 
Ollama
 
to
 
answer
 
natural
 
language
 
questions
 
with
 
SQL-based
 
responses.
 
The
 
chatbot
 
dynamically
 
retrieved
 
relevant
 
data
 
from
 
a
 
database,
 
ensured
 
user-speciﬁc
 
context,
 
and
 
handled
 
unrela

---

user-speciﬁc
 
context,
 
and
 
handled
 
unrelated
 
queries
 
with
 
humorous
 
responses.
 
Key
 
challenges
 
included
 
generating
 
accurate
 
SQL
 
queries
 
from
 
user
 
questions,
 
managing
 
query
 
errors,
 
and
 
ensuring
 
human-readable
 
outputs.
 
To
 
address
 
this,
 
I
 
designed
 
a
 
state
 
machine
 
using
 
StateGraph
 
to
 
streamline
 
the
 
workﬂow
 
and
 
implemented
 
retries
 
for
 
error
 
correction.
 
Additionally,
 
I
 
created
 
an
 
interactive
 
Gradio
 
i

---

lly,
 
I
 
created
 
an
 
interactive
 
Gradio
 
interface
 
to
 
improve
 
usability,
 
completing
 
the
 
end-to-end
 
system
 
for
 
real-time
 
conversational
 
assistance.
  
Q3.  Can  you  explain  the  concept  of  latent  space  in  generative  models?  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   180 
 
Latent  space  in  generative  models  
refers
 
to
 
a
 
compressed,
 
abstract
 
representation
 
of
 
the

---

mpressed,
 
abstract
 
representation
 
of
 
the
 
data,
 
where
 
high-dimensional
 
inputs
 
(like
 
images
 
or
 
text)
 
are
 
mapped
 
into
 
a
 
lower-dimensional
 
space.
 
 In  this  space,  meaningful  features  and  
patterns
 
of
 
the
 
data
 
are
 
encoded,
 
enabling
 
the
 
model
 
to
 
interpolate,
 
manipulate,
 
and
 
generate
 
new
 
data
 
by
 
sampling
 
and
 
decoding
 
from
 
this
 
space.
 
It
 
is
 
a
 
foundational
 
concept
 
in
 
models
 
like
 
VAEs
 
and
 
GANs.

---

cept
 
in
 
models
 
like
 
VAEs
 
and
 
GANs.
  
Q4.  Have  you  implemented  conditional  generative  models?  If  so,  what  
techniques
 
did
 
you
 
use
 
for
 
conditioning? Yes,  I've  worked  with  conditional  generative  models,  where  the  conditioning  typically  
involves
 
providing
 
additional
 
context
 
or
 
constraints
 
to
 
guide
 
the
 
model's
 
output.
 
In
 
your
 
project,
 
the
 
key
 
conditioning
 
techniques
 
are:
 ➢  User  Information:  The  model  generates  res

---

➢  User  Information:  The  model  generates  responses  conditioned  on  the  user's  ID  (to  
retrieve
 
the
 
current
 
user's
 
details
 
from
 
the
 
database),
 
ensuring
 
personalized
 
answers.
 ➢  Question  Relevance:  The  system  ﬁrst  determines  if  the  user's  query  is  relevant  to  
the
 
database
 
schema.
 
If
 
the
 
query
 
is
 
relevant,
 
it
 
generates
 
SQL
 
queries;
 
otherwise,
 
it
 
offers
 
a
 
fun
 
response.
 
This
 
is
 
an
 
example
 
of
 
conditioning
 
th

---

This
 
is
 
an
 
example
 
of
 
conditioning
 
the
 
model's
 
output
 
based
 
on
 
the
 
relevance
 
of
 
the
 
question.
 ➢  SQL  Query  Generation:  The  model  is  conditioned  on  the  provided  database  schema  
and
 
the
 
speciﬁc
 
user’s
 
request,
 
ensuring
 
that
 
the
 
generated
 
SQL
 
query
 
is
 
aligned
 
with
 
both
 
the
 
user’s
 
query
 
and
 
the
 
schema.
 ➢  Error  Handling  and  Query  Reﬁnement:  If  the  SQL  query  is  invalid  or  the  user’s  
query
 
needs
 
fu

---

is  invalid  or  the  user’s  
query
 
needs
 
further
 
clariﬁcation,
 
the
 
system
 
reruns
 
the
 
process
 
with
 
adjusted
 
questions
 
or
 
retries,
 
a
 
form
 
of
 
dynamic
 
conditioning
 
based
 
on
 
previous
 
outputs.
  
Q5.  Discuss  the  trade-offs  between  different  generative  models,  such  as  
GANs
 
vs.
 
VAEs.  Aspect  Generative  Adversarial  Networks  (GANs)   
Variational  Autoencoders  (VAEs)  
Output  Quality  Generate  highly  realistic  samples  with  sharp  det

---

rate  highly  realistic  samples  with  sharp  details,  
Produce  more  diverse  outputs  but  often  with  less  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   181 
 
making  them  ideal  for  tasks  like  image  synthesis.  However,  they  may  suffer  from  mode  collapse  (limited  diversity  in  outputs).  
sharpness  due  to  inherent  randomness  and  approximations  in  the  decoder.  
Training  Stability  Trainin

---

in  the  decoder.  
Training  Stability  Training  can  be  unstable  and  sensitive  to  hyperparameters  due  to  the  adversarial  nature  of  generator-discriminator  dynamics.  
Easier  and  more  stable  to  train,  as  they  rely  on  a  single  network  optimization  process  (reconstruction  and  regularization).  
Latent  Space  Representation  
Do  not  explicitly  learn  a  structured  latent  space,  making  interpolation  and  representation  tasks  less  intuitive.  
Learn  a  s

---

resentation  tasks  less  intuitive.  
Learn  a  smooth,  interpretable  latent  space,  useful  for  tasks  like  semantic  manipulation  and  anomaly  detection.  
Computational  Complexity  Require  two  networks  (generator  and  discriminator),  often  leading  to  higher  computational  costs.  
Typically  involve  a  single  encoder-decoder  pair,  generally  requiring  fewer  resources.  
Applications  Best  suited  for  tasks  where  output  quality  is  critical,  e.g.,  image  or  vid

---

put  quality  is  critical,  e.g.,  image  or  video  generation.  
Preferred  for  applications  needing  latent  space  utility,  such  as  data  compression  or  representation  learning.   Use  GANs  for  high-quality,  realistic  generation  tasks  but  expect  potential  training  
difﬁculties.
 Use  VAEs  for  stable  training  and  applications  requiring  interpretable  latent  spaces,  
accepting
 
lower
 
ﬁdelity
 
in
 
outputs.
  
Q6.  What  are  the  primary  differences  between  H

---

What  are  the  primary  differences  between  Hugging  Face  
Transformers,
 
Datasets,
 
and
 
Tokenizers
 
libraries,
 
and
 
how
 
do
 
they
 
integrate
 
to
 
streamline
 
NLP
 
workﬂows? Transformers:  ➢  Provides  state-of-the-art  pretrained  models  for  tasks  like  text  classiﬁcation,  
translation,
 
summarization,
 
and
 
more.
 ➢  Model  loading,  ﬁne-tuning,  and  inference  with  models  like  BERT,  GPT,  etc.  ➢  Central  for  applying  and  training  transformer-based  mode

---

applying  and  training  transformer-based  models.  Datasets:  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   182 
 
 ➢  Simpliﬁes  the  process  of  loading,  processing,  and  sharing  large-scale  datasets.  ➢  Efﬁcient  handling  of  datasets  (lazy  loading,  memory-mapped),  built-in  
preprocessing,
 
and
 
access
 
to
 
hundreds
 
of
 
datasets
 
via
 
Dataset
 
Hub.
 ➢  Efﬁcient  data  preparation  for  NLP  ta

---

Hub.
 ➢  Efﬁcient  data  preparation  for  NLP  tasks.  Tokenizers:  ➢  Handles  tokenization,  which  converts  text  into  tokens  for  model  input.  ➢  Fast,  optimized  tokenization  using  Rust,  and  support  for  custom  and  pre-trained  
tokenizers.
 ➢  Prepares  model-ready  inputs.   Workﬂow:  ➢  Use  Datasets  to  load  and  preprocess  data.  ➢  Use  Tokenizers  to  tokenize  and  preprocess  text  for  model  consumption.  ➢  Use  Transformers  to  ﬁne-tune  or  apply  pretrained

---

Transformers  to  ﬁne-tune  or  apply  pretrained  models  to  tokenized  data.   
Q7.  Describe  how  to  use  Hugging  Face  Pipelines  for  end-to-end  
inference.
 
What
 
types
 
of
 
NLP
 
tasks
 
can
 
pipelines
 
handle,
 
and
 
what
 
are
 
the
 
main
 
advantages
 
of
 
using
 
them?
 Hugging  Face  Pipelines  provide  an  easy-to-use  interface  for  performing  
end-to-end
 
inference
 
on
 
various
 
NLP
 
tasks.
 
 
Here's
 
how
 
to
 
use
 
them:  from  transformers  import  pipel

---

o
 
use
 
them:  from  transformers  import  pipeline   nlp_pipeline  =  pipeline(task="<task_name>",  model="<model_name>")  result  =  nlp_pipeline(input_data)   print(result)   
Supported
 
NLP
 
Tasks 1.  Text  Classiﬁcation:  Sentiment  analysis,  spam  detection,  etc.  2.  Named  Entity  Recognition  (NER):  Extracting  entities  like  names,  dates,  or  
organizations.
 3.  Question  Answering:  Answering  questions  based  on  a  context  passage.  4.  Summarization:  Producing  concis

---

t  passage.  4.  Summarization:  Producing  concise  summaries  of  longer  texts.  5.  Translation:  Translating  text  between  languages.  6.  Text  Generation:  Generating  text  (e.g.,  GPT-like  completion).  7.  Fill-Mask:  Predicting  masked  words  in  a  sentence.  8.  Token  Classiﬁcation:  Tasks  like  POS  tagging.  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   183 
 
9.  Zero-shot  Classiﬁcation:  Classifyin

---

183 
 
9.  Zero-shot  Classiﬁcation:  Classifying  text  without  task-speciﬁc  training.  10.  Conversational  AI:  Chatbots  and  dialogue  generation.  11.  Speech  and  Vision  Tasks:  Speech-to-text,  image  captioning,  etc.  (in  
advanced
 
cases).
  
Advantages
 
of
 
Using
 
Pipelines
 ➢  Ease  of  Use:  Simple  API  abstracts  away  complexities  of  model  loading,  
preprocessing,
 
and
 
tokenization.
 ➢  Task  Versatility:  Support  for  a  wide  range  of  tasks  with  pretrain

---

rt  for  a  wide  range  of  tasks  with  pretrained  models.  ➢  Customizable:  Allows  specifying  custom  models  or  datasets  for  ﬁne-tuning.  ➢  Production-Ready:  Handles  preprocessing  and  inference  seamlessly,  
enabling
 
fast
 
deployment.
 ➢  Pretrained  Models:  Access  to  a  vast  library  of  state-of-the-art  pretrained  
models.
  
Q8.  How  does  Hugging  Face's  Accelerate  library  improve  model  training,  
and
 
what
 
challenges
 
does
 
it
 
address
 
in
 
scaling

---

challenges
 
does
 
it
 
address
 
in
 
scaling
 
NLP
 
models
 
across
 
different
 
hardware
 
setups? Hugging  Face's  Accelerate  library  simpliﬁes  and  optimizes  the  process  of  scaling  
NLP
 
models
 
for
 
training
 
across
 
diverse
 
hardware
 
setups
 
(e.g.,
 
CPUs,
 
GPUs,
 
TPUs).
 
 
 
Here's
 
how
 
it
 
improves
 
model
 
training
 
and
 
addresses
 
challenges: Improvements:  1.  Hardware  Abstraction:  Automates  device  placement,  enabling  seamless  use  
of
 
CPUs,

---

placement,  enabling  seamless  use  
of
 
CPUs,
 
GPUs,
 
and
 
TPUs
 
without
 
requiring
 
deep
 
knowledge
 
of
 
each
 
architecture.
 2.  Multi-Device  Scaling:  Simpliﬁes  distributed  training  on  multiple  GPUs  or  
nodes
 
with
 
minimal
 
code
 
changes.
 3.  Optimized  Data  Loading:  Enhances  input  pipeline  performance,  crucial  for  
high-throughput
 
models.
 4.  Flexibility:  Supports  a  variety  of  training  setups,  from  single-device  training  
to
 
large-scale
 
dis

---

single-device  training  
to
 
large-scale
 
distributed
 
environments.
 5.  Ease  of  Use:  Reduces  boilerplate  code  through  intuitive  APIs,  making  it  
accessible
 
for
 
beginners
 
and
 
efﬁcient
 
for
 
experts.
  

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   184 
 
Challenges  Addressed:  1.  Hardware  Heterogeneity:  Streamlines  deployment  on  different  hardware  
conﬁgurations
 
(e.g.,
 
mixed
 
CPU/G

---

hardware  
conﬁgurations
 
(e.g.,
 
mixed
 
CPU/GPU
 
clusters).
 2.  Complexity  of  Distributed  Training:  Abstracts  intricate  tasks  like  gradient  
synchronization,
 
optimizer
 
scaling,
 
and
 
fault
 
tolerance.
 3.  Performance  Bottlenecks:  Optimizes  memory  usage  and  computation,  
addressing
 
inefﬁciencies
 
common
 
in
 
large-scale
 
NLP
 
models.
 4.  Scalability:  Ensures  that  training  scales  efﬁciently  as  more  resources  are  
added.
  
Q9.  How  does  Hugging  F

---

ources  are  
added.
  
Q9.  How  does  Hugging  Face's  transformers  library  facilitate  transfer  
learning,
 
and
 
what
 
are
 
the
 
typical
 
steps
 
for
 
ﬁne-tuning
 
a
 
pre-trained
 
model
 
on
 
a
 
custom
 
dataset?
 
Here's
 
a
 
concise
 
outline
 
of
 
the
 
process:
 1.  Choose  a  model  from  the  library  based  on  your  task,  such  as  text  classiﬁcation,  
summarization,
 
or
 
QA.
  
2.
 
Load
 
the
 
model
 
and
 
tokenizer
 
suited
 
to
 
your
 
task:
 from  transfor

---

nizer
 
suited
 
to
 
your
 
task:
 from  transformers  import  AutoTokenizer,  AutoModelForSequenceClassiﬁcation    tokenizer  =  AutoTokenizer.from_pretrained("bert-base-uncased")   model  =  AutoModelForSequenceClassiﬁcation.from_pretrained("bert-base-uncased",  num_labels=2)  
 
3.
 Use  Hugging  Face's  datasets  library  or  preprocess  your  dataset.  from  datasets  import  load_dataset   dataset  =  load_dataset("your_dataset")   def  tokenize_function(examples):      return  tokenizer(

---

kenize_function(examples):      return  tokenizer(examples["text"],  truncation=True,  padding="max_length")   tokenized_datasets  =  dataset.map(tokenize_function,  batched=True)   4.  Convert  tokenized  datasets  into  DataLoader  objects  for  efﬁcient  batching:  from  torch.utils.data  import  DataLoader   train_loader  =  DataLoader(tokenized_datasets["train"],  batch_size=16,  shufﬂe=True)  

MD  ARBAAZ  KHAN                                                                Generative  AI

---

Generative  AI  Interview  Questions   185 
 
5.  Set  up  the  training  loop  with  an  optimizer  and  learning  rate  scheduler,  or  use  
Hugging
 
Face's
 
Trainer
 
for
 
simplicity:
 from  transformers  import  Trainer,  TrainingArguments   training_args  =  TrainingArguments(      output_dir="./results",      evaluation_strategy="epoch",      per_device_train_batch_size=16,      num_train_epochs=3,      logging_dir="./logs",  )   trainer  =  Trainer(

---

logging_dir="./logs",  )   trainer  =  Trainer(      model=model,      args=training_args,      train_dataset=tokenized_datasets["train"],      eval_dataset=tokenized_datasets["validation"],  )   trainer.train()   6.  Evaluate  on  a  test  set  and  save  the  ﬁne-tuned  model:  model.save_pretrained("./ﬁne_tuned_model")  tokenizer.save_pretrained("./ﬁne_tuned_model")   
Q10.  What  role  does  multi-modality  play  in  the  latest  LLMs,  and  how  
does
 
it
 
enhance
 
their
 
functionality

---

how  
does
 
it
 
enhance
 
their
 
functionality? Multi-modality  in  the  latest  large  language  models  (LLMs)  allows  them  to  process  and  
integrate
 
multiple
 
types
 
of
 
data,
 
such
 
as
 
text,
 
images,
 
audio,
 
and
 
video.
 
This
 
capability
 
signiﬁcantly
 
enhances
 
their
 
functionality
 
by
 
enabling
 
them
 
to:
  ➢  Understand  Context  Across  Modalities:  They  can  interpret  and  relate  information  
from
 
different
 
formats,
 
such
 
as
 
generating
 
tex

---

ifferent
 
formats,
 
such
 
as
 
generating
 
text-based
 
insights
 
from
 
images
 
or
 
describing
 
visual
 
scenes.
 ➢  Improve  Interaction  Quality:  Multi-modal  LLMs  can  handle  more  natural  and  
diverse
 
user
 
inputs,
 
such
 
as
 
combining
 
spoken
 
questions
 
with
 
image
 
uploads.
 ➢  Expand  Use  Cases:  They  support  tasks  like  visual  question  answering,  image  
captioning,
 
and
 
cross-modal
 
translations,
 
broadening
 
their
 
applicability
 
in
 
domains

---

oadening
 
their
 
applicability
 
in
 
domains
 
like
 
healthcare,
 
education,
 
and
 
content
 
creation.
 ➢  Enhance  Learning  and  Generalization:  Multi-modal  training  encourages  richer  
representations
 
of
 
concepts,
 
improving
 
generalization
 
across
 
tasks
 
and
 
reducing
 
dependence
 
on
 
modality-speciﬁc
 
data.
 

MD  ARBAAZ  KHAN                                                                Generative  AI  Interview  Questions   186 
 
Q11.  What  are  the  implicati

---

Questions   186 
 
Q11.  What  are  the  implications  of  the  rapid  advancement  of  LLMs  on  
industries
 
such
 
as
 
healthcare,
 
education,
 
and
 
content
 
creation? ●  Healthcare :  LLMs  enable  faster  diagnostics,  personalized  treatment  plans,  and  
improved
 
patient
 
interactions
 
through
 
virtual
 
assistants,
 
but
 
raise
 
concerns
 
about
 
accuracy,
 
privacy,
 
and
 
bias.
 ●  Education :  They  enhance  personalized  learning,  provide  instant  tutoring,  and  
s

---

d  learning,  provide  instant  tutoring,  and  
streamline
 
content
 
creation
 
for
 
educators,
 
yet
 
risk
 
dependency
 
and
 
misinformation.
 ●  Content  Creation :  LLMs  automate  writing,  editing,  and  creative  ideation,  boosting  
productivity,
 
but
 
challenge
 
originality
 
and
 
ethical
 
boundaries
 
in
 
intellectual
 
property.